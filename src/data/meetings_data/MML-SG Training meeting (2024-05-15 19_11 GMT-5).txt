Estoy ejecutando el el scrappy normal que tenemos aquí está la base de datos hongo dd. Y pues de momento momento no me han bloqueado ya después de cierto rato es cuando se estalla ahí ya no funciona mejor Pero mientras eso pasa, pues voy a compartirles el código que tenemos hoy, ya estamos muy muy cerca de hacer la preparación de datos, pero Hay un, pero y ya nos nos acompañaba listo, entonces va a comenzar desde arriba otra vez librerías aquí otra vez cogemos información, vemos la información. Removemos todo lo que no necesitamos Y tenemos un shift ahorita lo volvemos a ejecutar y Ahorita les voy a decir porque no lo puedo ejecutar en este momento, pero tiene una explicación.
Que tendríamos 91.000 registros. Luego va a ser la el borrado duplicados. Tenemos 44,000, que si se dan cuenta, vamos subiendo. Creo que las pasadas te comenté en 35.000, pero vamos, subimos como otros 10 y entonces ahí misma idea ahí, entonces La idea. Es que cuando ustedes vayan a crear un pyme el PayPal es como los pasos que tiene un entrenamiento Esto es lo que sigue el primer paso es recolección de la información segundo paso preparación de los datos tercer paso entrenamiento del modelo. Eso te va a entregar a ti como unas métricas y un modelo que tú puedes utilizar para testearlo con un Data sed de prueba y si las métricas son lo suficientemente buenas, lo despliegas y lo despliegas es que lo metes en un en Point para que lo consuma cualquier persona y así es, básicamente es el PayPal Entonces el ciclo de reentrenamiento que cuando es más información o necesitas hacer un ajuste pues
Cómo de hecho como el muchacho dijo el de rappi en la conferencia que tenía que ejecutar cada cierto tiempo Ellos tendrán un país en automatizado que hace todo esto entonces ejecuta todas estas funciones que hemos visto entonces acá, eh? Si no está mal estuviera el precio sugerido por el manufacturador, eh? Antes no hay muchas diferencias en realidad este si no se ha comportado ya bastante igual porque todavía tenemos como esta anomalía sus layer es muy raro ese precio, ya miraremos más adelante, cómo se va a borrar Cómo se va a manejar Tenemos aquí el histograma también del año para ver cómo se ve, eh? Habíamos habíamos ya visto esto la sesión anterior Los únicos cambios que hay.
Eh, fueron, bueno, quizás el Hash para este ya entendemos por qué en este se hace Hash en el otro se hace como World to vector que es mapear en un vector utilizando Este modelo pre entrenado entonces hacíamos transformando una a una diferencias que se hicieron la primera fue en en exterior el número de dimensiones. Al cual vamos a mapear esas palabras es cinco Porque si se acuerdan en tres por aquí tengo la imagen si se acuerdan en tres pues quedan muy aglomerados, De hecho, también le pasa al a a a a a ambos exterior e interior, pues quedan muy aglomerados y quizás el modelo no puede encontrar patrones tan fácilmente acá, pero en cinco dimensiones si es posible que los pueda encontrar más fácilmente igual Estos son hiper parámetros si no funciona así lo bajamos, lo subimos hasta que demos con el con el que mejor se ajusta, eh, eh Ese fue un cambio que se hizo acá por Over
Ya no se puede plotear entonces toda esta parte está comentada ya no se puede despegar, eh? Mostrar en un en un 13, cómo funciona porque ya son cinco dimensiones, no, no se puede, eh? Lo mismo con el color interior como les comentaba, pues estoy cambiando el número de dimensiones, acá son 5 horas misma idea nos puede bloquear, eh? Por eso está conectado em luego está la parte del precio, eh? Mismo tenemos un techo Sin novedad También acá, se ha manejado bastante igual aquí también hay como un valor a mano que ya veremos Cómo se puede resolver, eh? Hoy de hecho hoy lo vamos a ver em tenemos el, eh? Eso no importa. Entonces aquí igual volvíamos a agrupar estas características del del Drive train, eh? Por aquí, creo que se agregó una que otra, Eh pues la idea ya es hacer que esto
Robusto Como por ejemplo esta y está son muy parecidas entonces lo que yo puedo hacer es lower Case mandarlas todas a minúscula y borrarle los caracteres especiales como este y ya quedaría básicamente estas dos unificados eso lo puedo hacer un poco aquí arriba Entonces uno empieza a buscar patrones para simplificarse como la vida y digamos que también funciona para para algunas de estas y es simplemente hacerlo más robux y por acá Bueno se ha aplicado One For in Core después de eso, eh? Recuerden que One Punk coins asignó una nueva columna que está dedicada a una de las categorías que está en esa en ese feature que está en esa columna Entonces el ejemplo de los colores tengo amarillo azul y rojo Entonces ahora tengo una columna que es amarillo azul y rojo y uso binarios 0 y 1 para poder mapear A cuál corresponde cada uno
Estos eran los que teníamos dudas Sí sí, recuerdan el el make la sesión anterior fue como bueno que hacemos con este make en lo que se puede llegar a hacer con este médico que quizás Word to vector no era la mejor opción porque puede que como aquí no se usan palabras, sino sólo una palabra Puede que no las agrupe como nosotros desearíamos, eh, Y al final simplemente son marcas diferentes, pero no es como si estuvieran relacionadas entre ellas como los colores, los colores, tú puedes decidir el rojo, se parece al o sea lo que hizo lo que hizo metalizado Entonces eso los puedes meter en un conjunto, pero aquí vas diferente todas van diferentes Entonces pues por esa parte después de analizarlo hacer el cloro otra vez Se cogieron más datos aparecieron una que otra marca nueva la decisión que tomé De hecho, si alguien tiene alguna idea que lo haya trabajado se interrumpe.
Una vamos a ir viendo las diferentes opciones pero por ahora la decisión que se tomó fue se necesita una mínima cantidad de datos por cada categoría para que el modelo pueda identificar un patrón Entonces se tomó el la máxima frecuencia que se tiene alrededor de si se dan cuenta 40,000 y pico y se tomó al menos el 10% o cerca al 10% entonces o en un 300 o 400 estaría como un treasure que todo lo que esté por debajo de ese 300 y 400 son marcas que no voy a considerar es decir, ni siquiera las voy a agrupar en otros sino Simplemente no tengo suficiente información para poder encontrar un patrón o que el modelo pueda encontrar un patrón Entonces se elimina y se coloca una nota en el modelo diciendo el modelo solo funcionó para estas marcas porque porque no es suficientes datos para las demás y agruparlas no tiene mucho sentido porque hay algunas marcas.
Es como Bugatti y el precio de Bugatti podría ser muy diferente a un Fiat entonces puede que no sea tan buena idea de agruparlos, eh? Como en otra categoría como por defecto no, no tendría tanto sentido, entonces simplemente se elimina debajo del techo, Qué es lo que se hace acá se elimina todas las todas las que estén por debajo de hecho por aquí aparece de 300 ese puente de hecho que se escogió en frecuencia y pues Estas son las que quedaron vivos prácticamente, eh? Uno dice tienen suficiente información y de El desbalanceo que el desbalanceo básicamente significa que una categoría está muy sobrecargada, respecto a otra se reduce Aquí también coloqué unas notas, Eh Pues porque ustedes saben que esto es tangible Si llegan a a a a a hacer el ejercicio de interesarse en ver el detalle, Pues aquí también está hay algunas técnicas en las que por ejemplo, aquí nos podemos dar el lujo de poder eliminar decir una función pero en algunos casos en los que no es posible eliminar porque
Tenga un desbalanceo grande, que por ejemplo fue lo que dijo el chico de rappi como que hay muy pocos fraudes, pero siempre son muy grandes, entonces lo que es fraudulento es muy pequeño y lo que no es fraudulento es muy grande y hay un desbalanceo Para eso hay otras tácticas que que se utilizan que es por ejemplo, lo que él decía over sampling in the sample, Qué significa Under sampling. Escojo la que tenga mayor frecuencia y la disminuyo es decir, le le recorto, le recorto la cantidad la frecuencia, quito algunas filas que sean por ejemplo, eh, No fraudulentos y a las que son fraudulentas, yo las puedo o duplicar o entonces a ustedes todas de de a dos, las duplicó para que quede más balanceado o también hay unas más avanzadas unas un método más avanzado, que por ejemplo se llama Smoke que estadísticamente encuentra Cuál es la que está más.
Más desbalanceada se llama así Smoke porque quizás no la coge bien y lo que hace es encontrar esos casos que están muy poco frecuentes y le inyecta información sintética es decir. Busca esos pequeños, eh? Buscar como un vecino que esté cerca a algo fraudulento, entonces mapeo más o menos, cómo se ve uno fraudulento y colocó otro postulen, pero le colocó un poquitico de ruido al lado para que no sea el mismo dato, sino que tenga una pequeña variación entonces en esas prácticas que Les acabo de comenzar los métodos hay que tener cuidado porque si yo abuso de eso ya le meto información sintética que no es real, entonces hay cierto límite para meter esa información sintética lo suficiente como para que el modelo pueda encontrar un patrón puede encontrar una línea pero no tanta para que crea que
Ya es el camino real, sino sino ya le estaría metiendo yo un un sesgo que no, que no existe que no estaría en el en en los datos reales, entonces hay una, hay un hay un límite en el que se puede usar eso o también se puede utilizar otras, eh? Prácticas de regularización. Eso lo vamos a ver más que todo es en redes neuronales para que se entienda, eh? Que regularización básicamente es evitar que el modelo se sobre con un dato o con un sesgo, sino forzar que el algoritmo cuando se está entrenando tenga dificultades al momento Cómo explotar en el aprendizaje, sino que el aprendizaje podría ser un poco más lento, pero no es tan agresivo Y de esa manera, puedo aprender de lo que está menos balanceado también o incluso hay otras prácticas que es darle más peso a los datos que tienen a los que tenemos pocos datos y darle menos peso a los que tenemos más datos colocando El ejemplo del carro es como que yo cojo el carro.
Más que conducir y a ese ya voy relajado, no le paró tanta bolas cuando estoy entrenando, pero al carro que yo sé que es bien raro y que tiene su manita a veces Estoy full enfocado en entrenar O sea me doy cuenta de cada detalle Cómo se mueve es la misma idea, básicamente le estamos diciendo algoritmo a estos que son muy comunes no le parece tantas bolas lo suficiente y a estos que son bien raros Necesito que necesito que se enfoque bien en en en saber cómo funciona es básicamente la misma idea extendida a un algoritmo. Entonces acá simplemente se aplicó, pues este no aplicamos ninguna de las anteriores, porque ya aplicando Electric Pues balanceamos un poco más, eh? Y después aplicamos banco it, eh? Nos entregaba un montón de columnas Que de momento Bueno hay que tener cuidado porque ya empezamos a tener más más y más y más, eh? Pero ya en el entrenamiento. Veremos si son demasiadas o Tendremos que hacer algún otro ajuste, pero esa fue la solución que se le dio en este caso.
Make al piso que se llama señor una una duda ahí con ahorita que estaba nombrando es como que no alcancé pues como a preguntarte, pero es que he leído acerca de eso y en varias partes como que castigan un poquito como esas librerías de métodos que que generan datos sintéticos porque precisamente pues como que me están digamos que con datos ficticios gente del modelo, puede aprender de ellos, eh? A veces puede quedar incluso para que haya over peeling por ejemplo y y nombraste el otro de las redes, si si uno no utiliza una red neuronal, si no, otro algoritmo uno también podría decirle al al algoritmo al modelo tenga cuidado con que cuando se vaya a equivocar prediciendo, por ejemplo, la clase más desbalanceada, pues como póngale más atención a esas depende del algoritmo.
No todos, pero depende del algoritmo, eh? Te permiten hacer esa acción o no De hecho por aquí? Yo tengo un libro, eh? A ver si encuentro el capítulo rápido en el cual explican también, por ejemplo, el ejercicio de la regularización se llama así de hecho, es este libro, se los vamos a Por qué es es de hecho este es el mejor libro que yo he encontrado para Machine learning, este brutal brutal es es muy denso, pero de hecho mucho de lo que hemos hablado, acá lo tiene, lo tiene este libro Y por acá, hay algo llamado a ver si lo podemos pensar fácilmente, se llama l1 y L2 Esos son los términos técnicos que se usa para hacer una un ejercicio de normalización y para colocarlo en términos sencillos Es un término que se le agrega a el error que se calcula Cuando hacemos si se acuerdan en las primeras sesiones, la forma en la que esto se entrena es Yo calculo miro la diferencia con la que tengo con respecto al resultado Real y
Los parámetros del modelo vuelvo a hacer el ciclo cálculo otra vez me entregó un resultado diferencia ajusto justo Entonces eso va forzando que la justicia más, eh? Más cercano a los datos reales porque yo minimizo el error es la idea ahí lo que yo hago es agregarle un término adicional a este error para que se vea forzado a hacer el ajuste, pero no hacer un ajuste agresivo, sino hacer un ajuste un poco más medido Sí ese tipo de de prácticas se puede utilizar, eh? En los en los modelos, eh? Desconozco Sí para absolutamente todos se puede pero por ejemplo para el lineal se puede para soportar Machine se puede para redes neuronales se puede de hecho se usa mucho, eh? Pero espera a ver si encontramos los movilización Yo sé que está como los primeros capítulos
Yo sé que esto es al principio a ver si lo encontramos rápido, sino De hecho, yo sé la imagen de la que aparece exactamente donde empiezan a hablar de eso Aquí vamos a clasificación es por acá, me duele que es por acá Aquí están aplicando, no no, no, no siento que es por ahí está es acá lasso recreation se llama Beats recreation y la última es elastic net que es básicamente la suma de las anteriores, qué pasa?
El el read recreation si mi memoria no falla es un factor que tú le puedes agregar es por ejemplo, lo explica bastante esta imagen lo explica bien de cómo funciona para que no haya ese, eh? Que hice ahora sí sí, entonces ahí le está diciendo tú le puedes agregar un factor a ese a ese como factor de castigo, en el ajuste, puedes decir castigarlo mucho castigarlo poco eso también es un buen parámetro de de que le que le metes a la normalización y se ve mejor. Yo lo aprendí mejor fue con la polinómica. La lineal no se entiende tanto, Qué pasa, por ejemplo, cuando hay un Dodge liar en la lineal, la línea tiende a torcerse demasiado porque hay unos Live porque hay un un factor un valor muy raro y las los planos o las líneas son muy sensibles a los layers lo que hace.
Factores que él le dice ajuste pero no ajuste tan duro y no me importa que se esté muy equivocado por ahí por allá arriba pero no me ajuste tanto Entonces ahí es donde se hace un un tipo de normalización para evitar ese huir pero acá se entiende mejor Entonces por ejemplo, la que mejor representa el modelo vendría siendo la verde te das cuenta la verde, pues tiene como una cercanía más más próxima a a los puntos la azul es la que está muy sobre ajustada y la roja correcto entonces para evitar esa sensibilidad y ahí yo colocó este caso, eh? De normalización que también se podría aplicar para el desbalanceo Es que para esos casos raros tuve que presenta también un ajuste a que uno lo ve en 2D como para que sea más sencillo entender Pero esto es muchas dimensiones podría presentar una sensibilidad muy grande en el ajuste entonces para evitar que no que que que que no haya operé o que se sobre con un caso específico yo le aplicó este tipo de
Esa es la la la táctica que se llama se llama normalización, entonces hay varios tipos hay una que se llama ricks, que básicamente minimiza Recuerden que los los modelos tienen una cosa llamada parámetros que es lo que yo voy a ajustar. Básicamente en la línea en la línea. Yo tendría lo que venía haciendo los parámetros el Slow que es la pendiente y el otro el el factor B que es donde se choca con con el eje y la fórmula de la de la línea Esos son mis dos hiper parámetros lo que yo fuerzo a que pase cuando le aplicó, eh? Este tipo de normalizaciones en el Bridge si me demora No falla yo busco que estos hiper parámetros sean lo más pequeños posibles porque yo podría tener aquí en el b. O el de tu 200 y de pronto el Slow 500 Pero eso podría ser equivalente a tres y a cinco está en la misma proporción y casi que hago la misma línea.
Entonces ese tipo de de ajustes es lo que haría el de él de Rich y el otro que es el el lazo sí memorando falla lo que hace es eliminar la menos relevante va a eliminar las que son menos relevantes no las minimiza las elimina literalmente, o sea él él se da cuenta que en el ejemplo de carros se da cuenta que de pronto el color interior es totalmente inútil no sirve de nada entonces en la medida que se va entrenando va descartando le va quitando peso a ese parámetro va siendo esto no no interesa mucho si me tiras cualquier color interior no va a afectar el precio entonces, eh? Una hace una loba chiquita. El otro lo remueve básicamente Y sí el lazo sí que lo remueve porque esto es la fórmula y me acuerdo bien y aquí pues muestran un poco las diferencias de las penalizaciones Cómo se ven las penalizaciones en el ejemplo y en las technet es básicamente la suma de las dos, eh? En dónde le doy más peso a uno
O le di más peso a la otra y es un hiper parámetro también, entonces eso te ayuda a evitar que tenga también ese opening cuando hay desbalance esa es otra táctica que se puede usar y lo que tú dices del S Mode is the hecho is Mode significa sintético, no sé qué algo así porque es malo de tener escrito exactamente tiene posts muy buenos precios, no lo tengo escrito Jajaja Pero significa sintético sintético, eh? My Nuri over Soup in Tech que es el algoritmo mismo identifica Cuáles son los que están más chiquitos, o sea que no son muy frecuentes y empieza a ser oversize a rellenarlos, un poquitico más no puedo usar de eso porque como comentabas generó sesgo.
Y puedo aplicar Uber eating, es básicamente como yo inventarme carros marca de carros que no están sí o cómo puedes, por ejemplo, empezar a decir un Bugatti puede valer, no sé 40 millones de pesos, no ahí no no tiene sentido ese tipo O sea hay que ser cuidados, a que a que en qué momento lo metemos Porque si ya le meto mucho, puedo llegar a esas conclusiones, puede que los primeros puntos sean cerca del Bugatti y no sé un millón de dólares. Ta ta ta cerca al bugat, pero si le meto mucho ya estoy diciendo que el Bugatti puede que valga muy poco porque haya un Bugatti color verde fluorescente o algo así ese tipo de cosas son las que pueden pasar en ese caso ahí sí te si te si te declara Ahí cualquier cosa interrumpe muchachos, me comentes Bueno mira que parece que no ha fallado Yo pensaba que iba a fallar Bueno me hizo.
Se los juro que me han bloqueado, se los juro me han sacado, no, no podía acceder a la página, eh, listo. Entonces luego aplicamos más columnas de momento corramos el riesgo a ver cómo nos va, eh, En total. Ya diríamos 100 columnas y tendríamos 42,000 registros que con el tiempo no seguir creciendo. Entonces las por otras tampoco es que sea tan mal el body Style eh, este ya lo teníamos también es un One For eh Juanjo cori. Sí se aplicaba, se agregan también al otro que teníamos dudas. Era este Cat la estrategia que vemos de pronto sugerido la vez pasada era separó por por rayas al piso que es como la forma en la que ellos tienen de dividir las palabras y, eh, De momento sin, o sea, con total desconocimiento de si vaya a funcionar o no.
Pues se le aplicó Bueno una transformación de limpieza aquí se hace la división para que queden en palabras segmentadas como ven acá en palabras pulsa es ruta luxuri, pasó ayer y luego se le aplicó el el work otra vez que por aquí está entonces aquí si se dan cuenta no son tantas categorías, pero ya pues están medianamente ordenadas y lo que se hace Pues es lo mismo en este caso. Si se dan cuenta, quizás agregar una dimensión más no es tan necesario, se alcanzan a ver suficientemente distribuidos y y queda bien en 3D Entonces es así la dejamos en con tres dimensiones y miramos, Qué pasó la agregamos, eh? Uff Esto está trabajando un poco la computadora, eh? Lo agregamos y luego hacemos ya lo siguiente que era con el el tipo de
O el tipo de combustible que usa esto ya lo hemos manejado aplicamos banco incoin también aquí de hecho estamos abusando mucho como lenguaje, pero al parecer es el que tiene más sentido dado las características que tenemos, vamos, a cómo nos resulta la la vuelta, a ver si funciona o no funciona aquí me acabo de dar cuenta que tengo un error, no me he dado cuenta de eso sí y ya sé dónde acá Tenemos 41,000 aquí 44,000. Esto está mal. Le toca arreglarlo deberían ser todavía 41,000 es por el Index el Index se estalló Ay de hecho puede ser por eso que estoy ya les muestro porque entonces estoy sufriendo. Esto puede ser lo que me está haciendo sufrir seas lo mismo con este type Esto sí es simplemente un booleano, es verdadero o falso sencillo pero Entonces hasta ahí, ya tendríamos, eh? Unos resultados que acá recuerden serán 41,000 y
114 columnas bastantes pero ya todas son numéricas con diferentes características y la idea es trabajar con esto a ver si nos nos da suficiente información para entrenar el modelo acá hice una exportación simplemente para poder tenerle un csp y luego hacer como un tera, profile para otra vez ver cómo se vería creo que el de ira profile no lo ejecuté porque está enfocado en lo en en la siguiente paso cuando Ustedes ya tienen los datos transformados suele pasar que hay información perdida nosotros vimos que había uno que otro campo perdido De hecho acá. Esto es un montón de nulos que ven acá fue error mío, Sí alguien está hablando. No Qué pena se me ha prendido Sí mira acá fue completamente dormido Sí ya ya me di cuenta porque estoy sufriendo tanto ya les muestro el sufrimiento hay algunos datos.
Están vacíos Sí qué pasa los algoritmos no, no entienden un dato vacío, qué significaría nulo no es ni siquiera que tenga un dato como desconocido, sino o sea, como una columna que lo representa sino Simplemente no hay dato ni cero ni nada, ni texto ni nada, entonces en esos casos uno tiene que aplicar métodos de imputación o métodos de remoción, Sí sí, ese dato es inválido, Por ejemplo, si no tengo el precio ese dato es válido porque si no tengo precio no tengo Target lo elimino, no tiene sentido, pero por ejemplo, si no sé quizás el año en el que lo fabricaron yo Podría quizás sacar un promedio o calcular, cuál podría ser ese año de alguna manera y no directamente eliminarlo, eh? Sucede con todas las características que tenemos en el dato set entonces uno puede empezar a jugar y a a a a a a a a a aplicar, diferentes metodologías Como por ejemplo, eh? Reemplazo los vacíos por la mediana por la media.
Eh, Por la moda que es como la más la más común, eh? Por una constante que eso también podría ser por ejemplo, en un proyecto que estamos haciendo acá. Sí sí, el campo que es que viene es vacío significa que se reemplaza por cero, porque ese campo representa una cantidad un valor de indemnización Entonces es como si si no lo tienes registradas porque no tienen indemnización entonces por defecto c es el valor que se usa por cada una se hace un análisis, pero también hay otros métodos un poco más sofisticados para hacer imputación uno de ellos se llama imputation que es un método que utiliza.
Yo tengo perdido la la velocidad máxima Max Speed yo la tengo perdida, No sé cuánto es Pero entonces dado Dónde están los demás datos la clusterización que tiene Yo podría más o menos suponer dado la velocidad debe el el promedio de velocidad que hay acá. Yo podría más o menos suponer Cuál es cuál es el dato que está perdido, eh, Por ejemplo O sea si yo digo acá, que el promedio de velocidad está por encima por cerca los 100 km por hora, eh? Yo podría asumir que da la clasificación el valor de velocidad máxima podría estar en este Rango en este Rango acá, que serían 130 125 porque está cerca a datos que ya conozco. Sí es básicamente Cómo funciona el la imputación Por quién porque en en que el cluster encuentra esos patrones y él te dice Dame la información que más o menos tienes acerca de de
Distancia que está perdida y yo te digo más o menos, cuál podría ser el valor, Sí eso usa algoritmos de clusterización funciona mejor en algunos casos que otros hay otro que de hecho es mi favorito es el que estamos usando y creo que por eso es que la estoy. Estoy embarrando acá que se llama iterate imputation, Qué significa interactivo intention. Bueno algo más que se me olvidó decir el el los algoritmos de clusterización o los que usan Eh sí clusterización se rigen por distancias Y qué pasa con esos algoritmos que son muy sensibles a las Diferentes escalas de las distancias, por ejemplo sabemos que el precio está en millones o miles de dólares y quizás el número de llantas colocando que hay otra está en unidades máximo tres de pronto dos o una moto triciclo y carro y quizás hasta más si contamos un camión pero esas escalas hacen que el espacio entre entre
Las instancias se pueda distorsionar y al momento de hacer la crucificación falla porque él toma una distancia con respecto a los demás puntos que están cerca de él y sí creo que me fui por un momento cierto, creo que ahora sí ya durmieron acceso van a ver los va a compartir pantalla eso fue porque ya no me no me permitieron tener acceso atrás, me volvieron a bloquear, me imagino yo.
Que pueden ver cierto ahí, ven cierto, Sí o sea, ya estamos viendo la pantalla Ay no todavía tengo acceso. Es que se los juro que me lo quitan le quitan el acceso, pero creo que se estalló está disco por memoria ahora ya cargado mucho si se habrá estallado por memoria Pero bueno Afortunadamente no quitaron acceso bien, eh? Me hizo quedar mal porque esta tarde sí me quitaron acceso, no tenía acceso, pero bueno, entonces les está explicando, eh, En estos algoritmos de clusterización la distancia si es importante la escala de la distancia Entonces a veces uno tiene que aplicar algo que se llama escalar los datos que de pronto es meterlos entre un Rango de 0 a 1 - 1.1 o utilizar la desviación estándar para poder que todos los datos a pesar de que tengan Diferentes escalas, pues sus dimensiones quedan en un en un Rango ajustado y
De esa manera, el algoritmo de posterizacion, Sí podría encontrar esos patrones más fácilmente y mejorar la imputación de los datos en este caso es que es este algoritmo que vemos acá y el siguiente que es el que estamos usando que se llama iterate imputation este código que lo leí me parece una chip Este me parece una chimba porque yo lo asocia mucho como un sudoku, no sé si ustedes han jugado su que es como que vas llenando las palabras, Eh si son palabras o no me acuerdo como respuestas, pero cada vez que vas, señor que vaya okay, Y cada vez que vas llenando llenando más datos, tienes más información y es más fácil llenar los siguientes datos. Esto funciona bien de una forma pareciera, qué pasa con la literatura imputation la literatura imputation primer Ordena los datos y se ordena menús de el que está más completo Al que está más vacío, entonces puede que algunas algunos carros.
El caso algunos registros de carros tengan tres datos perdidos, cuatro datos perdidos cinco datos perdidos Entonces es lo que haces ordénalo de arriba hacia abajo, entonces comienzo desde lo desde los que están más llenos y hoy empezando a llenar los que están más vacíos entonces Empiezo con los que solo tienen un dato perdido, Qué hago yo con todos los datos que sí están completos. Yo hago un modelo de regresión y utilizo ese dato perdido como un Target entonces con todos los demás datos yo predigo, cuánto debería valer s y hago una regresión para hacer una imputación Entonces es cuando yo les decía que me parece Genial porque la misma el mismo Machine learning tiene como esa posibilidad de solucionar problemas mismos, o sea, estamos haciendo un modelo de regresión que queremos que predica, eh? El precio pero a la vez tenemos datos perdidos Pero podemos utilizar un modelo de regresión para poder encontrar esos datos perdidos y ahí me parece genial Genial que tenga como esa.
Esa habilidad tiene alguna restricción de tipo de dato que va a imputar numéricos, tienen que ser datos numéricos, ahí hay algunos Bueno sí de los que entrenan y de los que van a decir sí, correcto es como por ejemplo, te colocó un ejemplo acá ponle tú que el precio no lo metemos acá, eh? De hecho por acá. Yo elimino el precio porque el precio es nuestro Target el precio no Debería ser utilizado para hacer imputación porque si ya que a ella le estaríamos metiendo, eh? Como pistas de cuál sería el precio que debería tener entonces el precio se elimina, pero yo por ejemplo, Pon tú que el año lo tengo vacío, no sé cuál es el año en este en este registro acá Entonces yo uso este de acá, que es el precio sugerido Busco más o menos Cuántas son las millas y utilizo todos esos datos históricos y todas las demás columnas para predecir no, el precio sino en este caso el año yo predigo, ahora es el año y hago lo mismo con todos los demás.
Pero ese que está perdido con los demás con las demás características que sé que tengo ese que está perdido con las demás características las demás características de hecho Esta es una manera de meter, eh? Datos sintéticos. Eso es una forma de meter, no es un dato real es un aproximado entonces también hay que tener esa precaución de que pues puedes hacer imputación, pero si ya tienes más de la mitad perdidos Puede que no sea una buena idea usar ese feature en primer lugar, o sea, sí, ya hay una columna que tiene muchos datos perdidos. Quizás ya que aplicar otra táctica o algo porque si le metes imputación directa pasa a meter datos falsos, Sí entonces ahí tener cuidado que en su medida no puede ser tampoco Ahí está y siniestro, Qué pasa, se demora resto Rest tanto, así que lleva 486 minutos que yo ni siquiera sé cuánto es en obras sociales.
No terminó y yo creo que no ha terminado y yo que 60 minutos 8 horas desgracias de 8 horas, llegue lo corre y se lo juro. Yo llegué apenas. Llegué loco, Qué pasa Yo creo que esto se está demorando mucho es porque aquí la embarré, está intentando presidir todos estos datos que no son reales primer error mío, no me di cuenta error. Me iba a ir, eh? Porque aquí son 44 en realidad son 41. Entonces que la luz está sobre esforzando y segundo, eh? Como yo estoy haciendo así no tuviera tantos perdidos, como yo estoy haciendo tantas predicciones, tiene sentido que se demore en algunos casos Aquí es donde también ya se pone a mirar, eh, Me compensa, usar Este modelo, da la duración de tiempo que tienen en en terminar O sea compensa, que yo usé esta vaina tan pesada.
Para la cantidad de datos o de pronto tengo que hacer una modificación, además de hecho encontré algo, que yo no sabía, eh? Yo dije, pues sí, tengo la posibilidad de al algoritmo de decirle. Venga, use toda mi cpu, o sea, usted me cpu, Porque si yo abro como el administrador de tareas, em está consumiendo más cpu mi explorador y incluso menos memoria. Es O sea no está sacando Ah mientras este está acá visual Studio está acá, no está. Sacándole todo el provecho a mi cpu Entonces al parecer se puede hacer algo llamado como paralización, pero no es nativo no es nativo del algoritmo, es decir, esta librería no tiene esa función nativa toca hacer como una paralelizacion y decirle entrenarte en todos los cobres que yo tengo en mi computador y es un poco más complejo, Eh pues se necesita más más esfuerzo quizás ahí uno ya tenga que explorar otra librería o otro método de imputación o cosas así.
Pero me parecía también muy interesante ver esto que esto puede pasar puede que hayan algoritmos que funcionen muy bien, pero si tienes que decirle al cliente esto se le va a demorar una semana en entrenarse de pronto. El cliente te dice Uff no necesito que pase por ahí 3 días máximo porque sé yo, eh? No necesito para mi cliente o para que lo usé más rápido o lo que sea, entonces acá uno se pone a evaluar eso, pero yo creo que aquí fue porque yo la embarré Hasta ahorita. Me di cuenta de 8 horas después pero no pasó nada, entonces una vez que ya se hace la imputación, hay algo que que también quiero mostrarles Este es otro libro, que yo tengo que lo recomiendo mucho para la preparación de los datos se llama tera, preparation, eh? Les voy a mostrar la portada porque con la portavoz no se queda más más grabado el el libro es ésta de la preparation For My learning. Ese también lo van a encontrar en en recursos que están en el chat.
Son si notas Ahí está, eh? Pero entonces aquí pueden ver como las buenas prácticas que se usan para preparar los datos para poder entrenar el modelo y entonces aquí también sugieren que antes de eliminar los sublimes que es otra cosa, otro paso que uno debe seguir De hecho aquí, lo explica muy bien, que es Cómo limpiar los datos. Aquí te dice como que un duplicados e identificación y remoción de los sublimes de hecho esto debería estar un poco más abajo, porque primero debería ir la imputación de los datos, la mayoría de algoritmos que se usa para identificar los layers, eh, Son sensibles a la información pérdida es decir, si no tengo datos no puedo aplicar los algoritmos para encontrar los sublimes Entonces en ese caso primero aplicar imputación. Luego se aplicar detection de playas que el Player sería como por ejemplo el precio de 3 millones de dólares que puede que sea un precio real.
Pero es un precio que se sale del Rango de lo que queremos que aprenda el modelo el modelo quizás no queremos que aprenda eso quizás en ese caso, si queremos modelos que valgan eso, pues usamos un Data set de solo carros que tengan esos precios, pero buscamos carros más normalitos, digamos entonces, eh, acá lastimosamente no se los podemos Mostrar eh? Porque porque se está ejecutando, pero el siguiente paso es la identificación y remoción de los sublimes de nuevo, hay diferentes métodos para hacerlo cada uno se ajusta a una necesidad diferente, eh? Para explicar unos que otros, eh? Está la detección de slayers por la desviación estándar de hecho la desviación estándar es relativamente sencilla de entender, que lo abro acá, Qué es una forma de
Encontrar al rarito utilizando, eh, la media y utilizando la distancia con la que tienes respecto a la desviación entonces en una en una distribución tienes una media que es donde se encuentra el centro digamos la distribución dado los valores que tiene y tienes una desviación estándar que se podría llamar De hecho esto al cuadrado este mal se llama varianza es básicamente el rango en el que me muevo que puede ser ponle tú en el precio puede que podamos decir nos manejamos entre un Rango del 25% de 10 mil dólares y Vamos hasta 70,000, ahí se encuentra la gran mayoría de la información y esa es la variación que yo tengo y eso es la desviación estándar que yo alcanzo a tener lo que pasa es que él, eh? Un un método es usar esa desviación para poder decir todo lo que esté por encima de dos veces la desviación es un slider Entonces sería Cómo duplicar esto O sea duplicó.
Acá lo aplico, o sea, todo lo que esté por encima de dos, en este caso. Chao Por qué Porque ya son muy raros. Son datos que no tengo, eh? Que no tengo presente, No no tengo suficiente información para entrenar entre dar bien, Qué pasa con este enfoque funciona por lo general para una para un solo fichó es decir para una sola característica y ya vamos a ver a qué me refiero más adelante en el cuartil, eh? Intercontinental 75 90 95 Y eso le puedo decir todo lo que esté por encima del 95 Ciao eh, por encima del 90 Ciao eso puedo también hacerlo, eh? De hecho estos son parecidos. Luego hay otros como por ejemplo, local online online factor, que aquí voy a mostrar primero que significa lokaler.
Creo que eso también lo hemos visto la vez pasada local and Global outliers, en la imagen que lo presenta muy bien cuando uno Busca una Player uno no solamente se puede basar en una característica a no ser de que haya una regla de negocio que te diga usa esa característica y no puede ser, por ejemplo no hay un valor que te digo área cuadrada y si el valor es negativo, es inválido. Eso es layer, no me sirve Entonces simplemente lo lo borro o lo reemplazó por nulo, cómo funciona mejor, pero eso es una sola característica. Yo no puedo simplemente descartar una instancia porque una característica es rap, Porque si tiene muchas características, puede que todas las demás características. Sí, estén en un Rango aceptable, pero preciso solo hay una que es rara, solo hay solo tiene un dato que es raro el resto tienen sentido entonces para colocarles. Un ejemplo, este es el truco que yo uso para
Para poder encontrar los las anomalías y encontrar Cuáles son las buenas oportunidades de negocio, básicamente si esa idea si yo simplemente eliminara porque el precio es raro. Yo podría perderme la oportunidad de encontrar ese carro que es re Bueno pero preciso tiene el precio barato, si yo solamente cojo, eh, Y corto por un por una sola característica, entonces uno a veces tiene que usar algoritmos que contemplen todas las características y a partir de eso definir Cuáles son los outliers y cuáles no? Entonces hay unos términos que son local outlier y global Liner loca, los layer es como este punto rojo que aparece aquí en la parte de abajo, Qué significa el el local los layer que dentro de un grupo local, si se dan cuenta que hay un un una agrupación, tú eres el rarito. Estás ligeramente fuera del Rango O sea tienes una característica ese es un local un global es en todos los aspectos.
O en la mayoría de los aspectos que te representan eres raro. Estás alejado de todos los grupos no hay casi nadie parecido a ti en ningún aspecto. Entonces uno puede usar algoritmos que borren los globales y que borren los locales, el que el que Les acabo de mencionar que es local los Live factor. Él se enfoca en agrupar primero de hecho. Él usa algoritmo de clusterización los agrupó primero y encuentro Cuál es el rarito, cuál es el que está un poquitico lejos de ese grupo del centroide, que eso también lo lo vamos a ver cuando estemos viendo supervisado, entonces encontramos Cuál es el que esté lejitos y lo detectó y lo elimino el otro algoritmo que se llama selecion Forest esto de hecho, eh? Aquí en los dónde empieza a ver cómo también, cómo los mismos algoritmos se ayudan a a resolver el problema isolation Forest está basado en Random Forest es el algoritmo que vamos visto de ramas Entonces sí.
En el algoritmo el encontrar Cuál era el valor como equilibrio, no, pero el identificar cuál era? Esos esas características que eran más importantes y las ordenaba con base a esa importancia Entonces como si fuera un árbol, el tronco contiene las características más importantes y las ramas principales son las divisiones de las características más importantes Pero a medida, que voy yendo a las ramas ramas ramas. Tengo que tomar muchas decisiones para encontrar ese punto significa que estás siendo raro Sí para colocar. Un ejemplo acá, eh? Acá se necesitan digamos Solo dos condiciones para encontrar este grupo, pero para encontrar este pequeño punto Acá tengo que meter una condición acá y luego otra acá y luego otra acá otra caca hasta poder encontrar este punto me tocó hacer muchas ramas lo que significa que es un Live lo puedo borrar Entonces es otra forma en la que yo puedo identificar Cómo funcionan los sliders Y eliminarlos en este caso hay un algoritmo, eh? Todos estos son hiper.
Metros que es como en estimators son básicamente Cuántos esto usa Random Forest entonces utilizamos, eh? Árboles pero creamos muchos árboles como si fuera un bosque un Forest un bosque le decimos utilizo utilicé 200. Hágalo con 200 para que se reduzca el error que pueda tener en las variaciones y poder estar seguro que el que el que hay que eliminar es es el que hay que eliminar y yo puedo definir un grado un grado de contaminación es quiero que elimine el 10% más rápido y los demás me los dejé quietos el 10% que sea más rarito me lo va eliminando y me deja pues este dato haciendo un poco más limpio con los datos que que me sirven a mí este algoritmo si tiende a ejecutarse muy rápido no es tan pesado pero para que vayan viendo cómo se puede encontrar esas esos datos raros esas anomalías y acá simplemente se elimina solo que pues este de acá arriba está ejecutando todavía.
Creo que ahorita lo va a parar apenas terminemos porque ya ya que eh Pero esto ya me entregaría un dataset un poco más pequeño con datos más consistentes y que ahora sí encerraría en el mundo, el el mundo que quiero modelar hay otras áreas que se llama move feature selection, eh? Skate era y dimensional y reduction, eh? Quien también las pueden encontrar en el libro de hecho aquí podemos hacer una exploración de Qué significa cada uno el fixture selection es una una práctica usada para encontrar Cuáles son los datos más relevantes para presidir tarde sí, entonces ahí diferentes correlaciones que no pueden encontrar tanto lineales como no lineales estos son los algoritmos y te dicen, básicamente correlación, Sí sí él.
Se dio por el por el manufacturador sube y si el precio Target sube Entonces al parecer una relación porque se mueven se mueven a la par sí, pero también podría pasar al contrario que si sube el precio de manufacturador por alguna razón, el precio del del carro bajo que eso sería una relación inversa pero también hay relaciones, eh? Que no son lineales, si se dan cuenta está lineal, pero podría pasar que en algún momento sube y luego cuando supera cierto límite vuelve a bajar y luego cuando superar otro límite vuelve a subir la relación podría ser muy dinámica pero para eso son estos algoritmos, el que más me gusta se llama mutual information, pero es básicamente te ayuda a ti a decir qué podría ser importante que no en este caso vamos a dejarlas todas no vamos a seleccionar Si no vamos a dejarlas todo así a ver qué pasa para para tener como un Punto de partida, eh? También lo de escalar los datos creo que yo les comentaba en algunos algoritmos se necesita escalar en la misma dimensión.
En otros no es necesario, por ejemplo en el algoritmo que vamos a utilizar, inicialmente para presidir el precio, No necesitamos escalar el algoritmo puede manejar ese tipo de dimensiones diferentes o escalas diferentes entre las características pero por ejemplo, para redes neuronales pyt, ellas solo funcionan en rangos de -1 y 1 por lo general o cero y uno son los rangos que utilizan Eh entonces hay que hacer una escalabilidad que hay diferentes métodos de escalar y lo veremos cuando veamos redes neuronales Y por último, eh? Tenemos también reducción de dimensionalidad, qué pasa con la reducción dimensionalidad, eh? Cuando tú tienes muchas características que por ejemplo, puede ser en nuestro caso cuando tienes muchas características a veces al modelo, le puede ser muy difícil encontrar un patrón, eh? Porque le estoy metiendo mucho ruido, le estoy metiendo mucha información que él no necesita Entonces en ese caso hay algunas, eh? Hay algunos métodos de hecho justo ese de que estaba lloviendo.
Con mi compañero porque le estaba explicando cómo funcionaba hay uno que se llama pca, que es básicamente reducir las dimensiones, ya van a ver cómo funciona hecho bastante sencillo, entonces acá él coloca como el ejemplo dice a medida que voy agregando más información, pues se van incrementando las dimensiones que vendría siendo línea acá tenemos Pues un cuadrado. Un cubo esto creo que es Eh Lo eh, Avengers esta cosa no me acuerdo cómo se llama Pero bueno, cuatro dimensiones, entonces una una de los algoritmos que se usan para poder reducir la dimensionalidad se llama pca traduce si no estoy mal Espero que por ahí está pesca principal componente análisis, lo que hace principal componer análisis Es que reemplaza este esta imagen lo demuestra bastante.
Simplifica la información en una en un en una dimensión inferior o muchas dimensiones inferiores para colocar el ejemplo Estos son tres dimensiones si se dan cuenta los puntos están ploteados en tres dimensiones, eh? Parece ser como una luna o algo así y lo que hace pca es yo voy a colocar un plano que represente mejor la varianza de esos datos, si se dan cuenta si yo colocó el plano Así es mejor que colocarlo así porque así representó mejor la distribución de los puntos que colocarlo así la idea cuando aplicas pca es no perder información lo menos posible entonces si yo lo meto Así cuando yo los proyecto aquí acá me van a quedar solapados y no estoy entregando la mejor la mejor información pero si yo los meto aquí así dada la distribución que tienen, eh? Van a quedar proyectados pero ahora van a quedar en un plano 2D y ahora se vería pasamos de un 3D Cómo se ve acá a
Un 2D como se ve acá Entonces si se dan cuenta son los mismos datos Pero proyectados y que hicimos redujimos una dimensión ya no necesito tres dimensiones para representar Ese Conjunto de datos, sino Sólo dos. Eso es algoritmos y otros algoritmos se utilizan para poder reducir la dimensionalidad que suele ayudar al modelo a entrenarse más rápido o encontrar los patrones más importantes y demás De hecho, hay algunos algoritmos que ya son más más completos porque la limitante de este psa es que el psa solamente funciona con planos O sea no puede si está distribución tuviera una curva, eh? Sería como lo que vemos acá si yo sí yo lo intento meter un plano a esto si yo lo intento meter un plano aquí así y proyecto sobre todo sobre ese plano, eh? Para bajar la dimensión el problema que va a tener Es que voy a tener datos solapados Cómo se ve acá acá se ve solo apagó esta parte se ve toda negra en la mitad porque como yo metí el plano acá y eso era como un embudo así y proyecté.
Se están chocando por lo que estoy entregando una representación errónea de del problema, pero hay otras tácticas que se pueden utilizar para desenvolver. Digamos esa torta y tener una representación un poco más amable con un con un truco que se llama kernel, pero de momento no, no lo vamos a usar esto Les comento es como más informativo entonces con diferentes.
De lo que intentamos hacer y cómo lo proyectan si se dan cuenta en un plano 2D porque hacen esa transformación del kernel del espacio para usar el mismo plano, pero ahora lo represente de una forma más amable de hecho este no está tan mal porque si nos damos cuenta aquí el amarillo está por acá todos los rojos están por acá negros por aquí arriba de hecho no está mal esas no está interpretando y así hay muchos algoritmos entonces, eh? Estos son los pasos Esto es lo más largo que hay que hacer al momento de entrenar un modelo la preparación de los datos ver cómo hago para que sea algo que el modelo entienda Para que encuentre patrones y demás, eh? Pero ya lo tenemos a una pizca lastimosamente, pues como se dieron cuenta por todo por lo que sea, pues este no se termina de ejecutar la próxima sesión es, pero ya tenerlo bien para que podamos verlo a detalle Cómo se vería el imputado Cómo se vería el el selection Forest para entender cómo
Y de momento no usamos las demás para dejarlas ahí y el paso siguiente es una vez que tienes esos datos, pues ya pasamos a los algoritmos que son los algoritmos que utilizaríamos para para hacer la regresión, eh? Vamos a utilizar tres el primero va a ser, eh? Uno que se llama cactus que es como la competencia es como la superé el Random Forest se llama catboost lo que es con doble así este algoritmo es como un es cómo se llama esto un Random Forest súper avanzado que ya personas que ya le metieron mucha cabeza a esto, pues usaron este Random Forest con las tácticas que yo les decía que era como ponle tú estilo Naruto que te partes en muchos y aprendes muy rápido.
Eh, Porque los entrenos de forma individual luego concentra todo el entrenamiento y ya tienes, o sea es como si a practicarás algo muchísimas veces en paralelo y ya después un esto el conocimiento y ya tienes un modelo muy fino que funciona para la tarea que necesitas de forma muy muy fina muy específica, eh? Vamos a usar este vamos a usar paycare, que es el de él de auto en él, que básicamente este va a ser todo por nosotros, simplemente le tiramos esto los datos que él necesita, eh? Y él va a empezar a testear con un montón de algoritmos de regresión papá, papá. Hasta que encuentre Cuál es el que mejor le funciona para la regresión y nos va a decir cuál es el que mejor funciona, pero este también nos va a servir para encontrar la calidad de los datos para ver las métricas entender Qué es un buen modelo y qué métricas usamos y demás y el último es ya meterse a red neuronal literalmente entender cómo funciona una red neuronal para hacer una regresión.
Es importante porque con eso vamos a construir el auto encoder que se llama así el autoencoder para poder encontrar las anomalías Este es el truco que hay detrás de de encontrar el inmueble más barato y encontrar en este caso, el carro más barato este estás arquitectura que estamos usando para encontrar anomalías y ya veremos cómo funciona, pero si se encuentra son redes neuronales Solo que la arquitectura es como en v, va pegada y ya les explicaré Por qué funciona así Qué beneficios tiene cómo lo vamos a utilizar para encontrar la anomalía Sí pero importante primero entender que es una red neuronal, es si se dan cuenta Esta es una red neuronal normal, no tenemos en ningún momento como depresiones o uniones, si no es una red neuronal normal con diferentes capas que se utiliza para entrenar modelos, eh?
Sí, espero que esto Esto es lo que ha disparado más que todo la Inteligencia artificial ahorita así funciona chachi, así funciona que es el que genera imágenes así funciona los detectores como más avanzados así funcionan muchos muchos algoritmos que van a ver, eh? Con redes neuronales Entonces vamos para allá vamos para allá, vamos para allá, eh, Y nada, eso sería todo por hoy, lastimosamente no podemos pensar. Dice que la imputación no había funcionado, es por lo del índice. Sí te explico. Qué pasó ahí, eh? Lo que pasó fue que aquí Arribita justo me di cuenta porque revista si hacemos arriba acá, si te das cuenta en este paso que hay Acá hay 41.000 filas cierto y tenemos cuatro columnas, eso viene de
Que le aplicamos al al full time Sí qué queremos hacer nosotros con esto. Nosotros queremos coger este dataset que ahora representa el full time. Queremos concatenar con los carros con el datacell original para que reemplace el full time ya no va a ser el el texto, sino ahora van a ser estas cuatro columnas que me representan ese dato aquí yo la embarré en el sentido de que aquí yo debí haberle hecho aquí ya habiendo es una tontería cuando se hace un concad que aquí se hace una concatenación que básicamente es yo cojo una tabla cojo dos tablas y las pegó Sí para que yo tenga más columnas tengo las mismas filas, el mismo número de filas, pero las pega es lo que queremos hacer aquí la forma en la que trabajaba pandas. Es que cada una de las pilas tiene un índice Entonces tienes 0 1 2 3 4 5, esos índices pueden variar Y si estás tablas no tienen.
Exactamente los mismos índices lo que va a pasar Es que él va a intentar combinar los que sí tienen índices y los que no hacen match con el índice genera una nueva fila Y eso es lo que pasó acá. Entonces me generó esta nueva fila, está completamente vacía porque en la tabla original no había ninguna quisiera match con el índice, entonces me generó esas como si no, venga, no, no las encontré las dejé vacías entonces hizo que me generará más filas que son Ahora son 44,000, me generó 3000 + Sí ahí fue el error ese error, cómo me impacta a lo que estoy haciendo ahorita, eh? Pero espera, déjame antes de que se me olvide arreglo arreglo esto cómo me impacta lo que estoy haciendo ahorita como el paso siguiente justamente con mi mala suerte, que el paso siguiente es hacer una imputación, así se arregla Entonces él va a encontrar que hay un montón de datos perdidos y claro entre más datos perdidos.
Más información tengo que llenar y más me voy a demorar, es esto acá, Sí por eso es que está demorando mucho porque él dice me entregaste una fila completa con 100 Campos vacíos Entonces él tiene que empezar a hacer la regresión por cada campo entregaste, 3,000 filas correcto Esa es la cuenta Ajá Y ahí. Ahí fue la embarradita mía de un clic el típico punto y coma en programación que te de toda la vida punto y coma que nunca falta digo punto y coma porque antes en los lenguajes de programación, tú tenías que cerrar la sentencia con punto y coma si no le pondrías la punto y coma el programa ejecutaba, pero fallaba y tú no sabías Por qué Entonces tenemos ese dicho los programadores como ya de antaño más viejitos el punto y coma que es ese carácter Tonto estúpido, que tú dices cómo se me pasó Así pasa en programación una cosa tonta. Si te das cuenta fue una línea de código, pero pues bueno eso.
Que perdieron hay que pensar que estaba bien lastimosamente, entonces esa es la razón está listo, no muy claro muy claro, no chicos, ya no sería más, no sería más por ahora, eh? Si Dios quiere, eh? Cuando empecemos reto, yo quizás me extiendo un poco más a empezar a hacer el auto en Core para encontrar las anomalías, Porque si podemos exponerlo en una próxima sesión en estos grupos sería genial. Parece que ese grupo que vimos era una chimba y sería participar entonces, pues nada, eh? Chicos, dudas preguntas lo que quieran ahí estoy, eh? También como les comentaba si están interesados Yo sé que aquí en los Te la están contratando, creo que todavía no están contratando en Machine learning, De hecho no están contratando The Machine Lego porque yo sé que maneja la herida pero
Eh, bakkan Y de cosas así como datos como bases de datos y demás sí sí sé que hay vacantes entonces requisitos que manejen más o menos el inglés, eso es importante y pues que sepan hay programación igual Si tiene alguna duda sobre algún concepto de programación yo capaz, que también tengo el conocimiento. He estado en muchas áreas y y casi que me conozco bastante cosas, entonces podría darles una mano y lo último, eh? Libros Eh Pues no sé cómo se les da la lectura, pero aquí en esta parte de del discord, en el canal, que se llamaba notes resource aquí, eh? Se han compartido algunos canales de YouTube pero también se han compartido algunos libros. Esto es una imagen, pero el PDF como tal que creo que es el PDF creo que está acá Fox sí.
Lo descargó y lo abrimos aquí rápidamente tienes los links para que sea más fácil Cómo acceder a cada libro Entonces por ejemplo Estos son como los básicos para entrar en materia muy sencillos, eh? Este es el que yo recomiendo muchísimo de hecho aparece varias veces porque tienen muchas áreas Entonces si lo pican ahí te direcciona de una vez como Amazon es cuánto podría valer y hemos Ahí está entonces en diferentes, ahí lo intenté dividir como en diferentes áreas preparación de los datos, no supervisado, eh? Sea el supervise que es como un poco degenerativo también, eh? Triformes learning que se si Dios quiere, ya sería mucho más adelante patrones de diseño matemáticas Esto sí es como más de relajo Sí ya quieren entrar mucha profundidad de matemáticas estos de acá, pero son como los básicos los básicos así para para entrar en materia.
Un poco ya ya no tendría nada más para usted por ahora la unas noticias que me desbloquearon de esto bien, ya tenemos datos otra vez se los juro que me han bloqueado listo chicos, no bacano que me está acompañando, eh? Voy a intentar a ver si hacemos más engagement para que venga más gente a veces se interesa, creo que a veces les da un poco de perecita, porque ya cuando uno coge vuelo pues retomar todo lo que ya se hizo toma tiempo no, pero vamos, va a pensar ideas para que sea más llamativo creo que cuando empecemos a tirar resultados va a estar bacán porque los empezamos a publicar y decir vea este carro entonces ahí la gente Cómo llegaste hasta allá así igual También nosotros nos podemos ayudar con ella de hecho estas dos son dos que están acá es puro sabor Entonces eso ayuda total.
No, pero vamos a hacer, vamos a hacerle que la perseverancia digamos que yo les comentaba también al principio de todo esto que lo que a mí me llevó acá o como yo concibo el grupo es yo platino. Hago patinaje de velocidad de alto rendimiento y y es es disciplina, es estar todos los días ahí es apoyarte a tus compañeros, es es estar ahí, pero es vacando cuando son comunidad piensa que hablar de eso, Cuáles ruedas, me sirven más estás patinando así agáchate más, eso es eso y la perseverancia creo que eso es la diferencia para mí, si me lo pregunto el que te guste y el que está ahí el estar ahí intentando desafiándote cosas nuevas todo el tiempo y por ejemplo, lo que ven en pantalla son resultados de lo de finca raíz. También les queda compartir, que es básicamente esto algo parecido a lo que lo que lo que terminaríamos viendo que sería como por ejemplo acá.
Eh, según se hacen dos validaciones, entonces según las validaciones estos, por ejemplo está entre un Rango de 8% a 11% por debajo del precio 35% 9% Hay unos que son demasiado buenos 4416 Esto es lo que siento así señor, este sí Y este es real. Yo lo revisé y si es real, sí, O sea no está barato. Está en cuestión y mira acá te dice, por ejemplo, este es el precio real y este es el rango de precios que debería que se podría vender 170 millones podría venderse 286 y en 265 en este Rango es el que se podría vender Entonces esto es encontrar la anomalía que encontrar el la aguja en el pajar, pero pues que si tuviéramos suficiente dinero o si alguien creo que la venta no se me va muy bien, pero estoy intentandolo Eh Pues sería muy útil para encontrar esas oportunidades.
Negocio entonces comprobar a tu vendo caro, pues ni siquiera caro vendo al precio del mercado, pero ya sé que lo compré barato esto lo que vamos a hacer, vale? Con los carros Esa es la idea esto chicos ya por ahora no tengo más es un es otro enfoque de la anomalías muy guapa. Las gracias, Sí señor. Sí, ya Chicos antes nos vemos próxima sesión. Cualquier duda cuentas abiertos y nos veremos, vale? Muchas gracias, gracias.