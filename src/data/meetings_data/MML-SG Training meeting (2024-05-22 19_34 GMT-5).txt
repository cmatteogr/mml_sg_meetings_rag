Eh entonces acá por ejemplo, eh? También se hacía una pequeña limpieza que era que, eh? Que todo lo que Ah mentiras, Perdón perdón, era una limpieza Esto es lo que se hace aquí es separar nuestro Target que es el precio de El Data set Por qué Porque cuando ya estamos haciendo imputación, No queremos que si hacemos alguna imputación en este caso, por ejemplo a través de de que es un algoritmo de clusterización o de interactive imputation no queremos que el precio transfiera algún tipo de información a los valores que tenemos perdidos porque indirectamente estaríamos sugiriéndole a sus valores perdidos Cuál es el mejor valor que puedes tener para que esté sea mi precio entonces separar. Eso es importante, eh? Lo separamos y luego aplicamos este algoritmo que era.
Que se llama iteratec imputation que básicamente lo que hace es primero ordena la tabla entre los que están más llenos y los que están más vacíos, entonces todos los que estén más llenos están al principio y todos los que tengan Campos vacíos, están al final es una forma de entenderlo fácilmente y lo que hace es con los datos que están llenos entreno modelos regresivos que me predican el campo que está vacío, entonces básicamente utilizo las demás columnas para predecir el campo que está perdido, por ejemplo, utilizo, el color utilizo color interior exterior, eh? La atracción y demás para más o menos sugerir cuál podría ser el modelo o la marca Entonces eso era posible hacerlo con este algoritmo que es terapia intention. Él va encontrando a través de modelos regresivos Cuál es el posible valor que va a tener ese que está perdido, eh? El ejercicio anterior que hicimos Bueno nos dimos cuenta que era un error, pero al final ya sé teste.
Y se demoró 12 horas ejecutándose por lo que en lo que ven en pantalla es el código que que implemente para decirle si quiero que se ejecute de nuevo o si quiero que utilice el que ya tenía guardado Sí el modelo, que yo ya tenía guardado, eh, En este caso, pues no lo voy a correr Aunque hice una mejora que yo ahorita se las muestro, pero aquí se hacía un un trato imputation, aunque tengo la sospecha de que va a fallar porque posiblemente haya más marcas, vamos a ver Ahí está leyendo de hecho él él solo documento de este modelo pesa 6 GB o sea, es un burrazo enorme aquí nos aquí nos estamos saltando el entrenamiento del modelo y aquí, simplemente cargamos el modelo y aplicamos la transformación que es utilizar el modelo de nuevo aquí. Yo creo que va a fallar es que porque posiblemente.
Hayan Eh sí hayan unas categorías nuevas pérdidas, por ejemplo está está perdida De hecho, no sé porque está está perdido Está interesante, Déjenme hago una trampilla Okay creo que ya la Amparo del modelo con el modelo de regresión con el que va a hacer la imputación o lo O él o él solo elige, cuál puede ser el mejor buena pregunta el modelo de imputación viene con Random Forest por defecto si no estoy mal porque tú puedes utilizar, eh? Eso se llama estimador que es, eh, el método que utilizas para poder encontrar el valor mayor sugerido o incluso la regresión que yo les comentaba Recuerden que Random Forest es otro algoritmo que se usa para regresión y de hecho lo podemos ver Acá viendo la documentación.
Eh, Él te dice como el estimador Ah mentiras de hecho él el que es por defecto es eh? Fallecen Reach que esto es a partir del algoritmo de vallas. Si no estoy mal que es probabilidad de hecho por aquí estaba leyendo porque les quería Mostrar eh, que también se utiliza para encontrar los hiper parámetros, pero básicamente él encuentra, eh? Qué tan probable? Es que suceda o que sea este valor dependiendo de las probabilidades de las demás de las demás variables, entonces utiliza Esa esa forma es algoritmo de de probabilidad para encontrar Cuál es el mejor valor en este caso. Nosotros si mi memoria no me falla, estamos usando Random Forest porque es menos sensible a los slimes que como Ahorita tenemos algunos ítems que tienen una cola muy larga. Sí, creo que esa fue la razón por la que estamos utilizando eso, pero claro Mira acá, puedes utilizar un montón de
De hiper parámetros, eh? Unos ya los va aplicando entendiendo, eh? Entendiendo el algoritmo como función, pero si todo esto son hiper parámetros de literatura que tú puedes utilizar por ahora para mantenerlo simple le estamos aplicando, eh? Este este regreso en vez del del Valles y este es un esto es un parámetro no diría que es un hiper parámetro es un parámetro que simplemente te imprime El Progreso para que tú vayas viendo las líneas citas cuando sean los vuelos como que vas viendo el progreso en en la consola y es simplemente informativo, tú le puedes el número uno significa qué nivel de detalle quieres que tenga entre más nivel, por ejemplo, el tres te va imprimiendo cada cosita que va haciendo mientras el uno es como cada que hagas algo grande Me avisas y vas colocando una línea allí en la parte inferior, pero para que funcione voy a
Y aquí en teoría debería ya entrenarse que tomaría tiempo Aunque déjame y aplicamos un último cambio que fue que yo encontré y ya para que no se demore no se demore 12 horas, qué fue lo que se demoró y me imagino que entre más datos más se demora hay una librería que se llaman x o algo, así que optimiza todo lo que hace esquila Y en este caso optimizada, básicamente todos los algoritmos que hemos hecho, Déjame lo colocó al principio por si las moscas porque no es sabido, Qué pasa si lo ejecutó después y volvemos a tirar todo, esto se va a demorar ahí un ratico mientras hace el print el print de El plot ds3, dimensiones fue el que más es uno de los que más se demora, pero ya después de que pasemos acá y bajemos va a ser muy sencillo y Yo calculo que se puede demorar por ahí una.
Una hora y pico, pero digamos que podemos seguir la idea aún así viendo el código y la próxima sesión ya revisando los resultados, eh? Esperemos que llegue Todas se está ejecutando por allá arriba Entonces entonces entonces entonces entonces por ejemplo, este este truco de guardar el modelo que se guarda en un archivo pkl también hay otros formatos, pero pkl sé que yo he usado digamos ese mismo algoritmo o este mismo modelo es el que te puede servir a ti ya cuando estés en el despliegue porque en el despliegue porque podían haber casos en los que yo ya coloqué esto en un encuentro hay alguien que lo va a usar y esa persona a mí me tira información perdida O sea me dice. Mira, este es, eh? Este es el precio sugerido Este es el tipo de de tren ta ta ta pero me dice, pero yo no me sé el modelo no me sé el modelo no lo tengo tata, yo qué hago?
Hago el mismo truco como ya tengo un modelo que fue pre entrenado con la información que sí tengo lo que hago es aplicar o solo usar ese modelo para poder más o menos predecir cuál sería el valor, cuál sería ese modelo en realidad, cuál sería ese modelo que que tienes entonces Eh me sirve para el entrenamiento, pero también eventualmente me podría servir para el despliegue para yo ya colocarlo como en línea, eh? Esperemos a ver Esto puede que se demore un rato. Mira, ahí se va imprimiendo y me va diciendo más o menos Cuánto es, pero sé que se demora como 1 hora larguita pasar de 12 horas a una hora, pues es muy genial, la gran diferencia que van a ver acá es que si Yo abro el administrador de tareas Aquí sí voy a ver mi cpu un poco, eh? Pasando la mano con un 100% si se dan cuenta está bien estallada por lo mismo porque
Procesador que le acabamos de enviar coge el algoritmo y le dice pues ahora sí coja todo lo que tenga en en recursos porque la prioridad es la velocidad y pues hágale eso es lo que le estamos diciendo ahí. Básicamente me va a reducir tiempo, pero va a gastar más energía y pues más recursos O sea no voy a tener la libertad de abrir más cosas con tanta facilidad de momento ahí todavía me escuchan bien cierto, no se ha pausado ni nada porque el performance podría afectar, sí Bien listo. Entonces dejémoslo corriendo dejémoslo ahí, eh? Mientras vamos hablando del resto. Entonces una vez que ya tengo todos los datos, eh? Llenos lo siguiente que yo quiero hacer es encontrar esas anomalías esos raritos, que no quiero que estén ahí porque no me sirven porque me pueden generar conflicto, me pueden generar problemas Al momento de predecir entonces, eh? Y pilas acá los raritos que son los Dodgers es diferente anomalías.
Anomalías es algo que está fuera del Rango normal, pero no significa que sea un dato erróneo los outlier si es como que estás fuera de todos los rangos O sea no hay lugar por donde yo te pueda tomar y tomarte como un dato válido ponle tú el carro, por ejemplo que hemos de 4 millones y pico, o sea ese carro no entra en un Rango bueno para poder entender para poder entrenar nuestro modelo y que sea que sea útil en en nuestro modelo. Sí, Ya quisiéramos tener predicción de carros de alta gama, pues mejor hago un modelo que se dedique a entrenar, eh? Con carros con características de carros muy caros y demás y ahí sí tendría sentido para en este caso son carros normales carros comerciales, no quiero meterle mucho más en este caso. Usamos otro algoritmo, que se llama selección Forest Aunque Aquí hay varios que también la vemos evaluado en la sesión anterior.
Se puede utilizar los cuartiles hay otro tipo de hay otros que no los agregan acá, pero en este caso estamos utilizando la selección Forest que lo que hace es borrarme los slides globales, eh? Y quitamos como un porcentaje del dataset para tener un Data 0 Más estable en este caso, el porcentaje de datos set que estamos quitando es el 10%, Este es un hiper parámetro y Este es otro que le decimos más o menos Cuántos árboles quiere que quiere que tenga porque esto funciona con Random Forest Entonces estamos haciéndole usted tantas tantos árboles para hacer esa estimación y demás hiper parámetros básicamente que también podemos configurar, eh, En este caso misma idea que se podría hacer con el de imputation. Yo le podría decir a él, guárdese y cuando yo reciba una solicitud Yo puedo identificar si esa solicitud entran del Rango aceptable que podría.
Bueno, de esa manera puedo decirle a la persona lo que me estás pidiendo es que te predica un Bugatti con un montón de millones de dólares con un montón de cosas que no están en mi Rango común. Quizás no te lo puedo decir al menos con precisión Entonces eso también podría funcionar por ese lado, eh, Y recuerda que estamos quitando los globales Con qué me refiero otra vez a los globales, eh? Solo para recapitular, se puede se tienen loca los layers. Sí tienen globos en algunos casos. Sí es conveniente quitar los outliers de una característica específica que básicamente los sublíder de una característica Déjenme lo lo colocó acá cuando tengo una distribución los la s en una sola característica, básicamente son los que
Están muy lejos en una cola que sería por ejemplo en este caso 700 800, o sea, son datos muy raros en los que tengo poca frecuencia no tengo tantos datos como para encontrar un patrón ese sería el problema principal que encontraría el modelo a veces es conveniente mochar en el en en en el Future es decir en la característica específica pero en otros casos nosotros tenemos que considerar es el todo de pronto porque tengas un precio sugerido muy raro, pero el resto de cosas las tienes normales, no significa que te tenga que borrar Simplemente tienes un dato raro y eso está bien, no hay problema Entonces uno uno tiene que también considerar si quiere borrar un local outlier porque eres rarito en una característica o si quiere borrar un globo a los layer que es eres raro en muchas cosas y en ese caso, si no te puedo utilizar para mi modelo en este caso estamos utilizando ese enfoque Quítame los que son globalmente raros los que salgan de de del margen, pero a nivel general, eh?
Dando todos los features Entonces vamos, volvemos acá y miramos Cómo va Cómo va esta ejecución se toma su tiempo usualmente iría, ya va a aparecer dos o tres Déjenme les muestro Cómo se vería esto ya guardado, Dónde está esto deira, exploration output, Ah Es esta de acá, Mira tanto así es tan pesado, que no lo pude subir al repo es el único es el único documento que ignoro porque es muy pesado Mira 6 GB 3, no Give have me dice lo siento, no te puedo subir o me tienes que pagar entonces, eh, este tipo de algoritmos a veces podrían ser muy pesados, porque recuerden estoy haciendo como mucha regresiones de los diferentes Campos vacíos Y en este caso tengan en cuenta que tenemos 117 columnas, posiblemente.
Varios vacíos Entonces ahí es donde se complica la cosa un poco luego de borrar los los raritos podría hacerse otro tipo de cosas que es, eh? Escoger Los ficheros que sean más relevantes em aplicar escalabilidad que en este momento no lo vamos a ver, pero cuando en la próxima sesión si si Dios quiere y todo sale bien ya entraríamos con redes neuronales Y si Lo tendríamos que utilizar y eh? También podríamos hacer una reducción de dimensionalidad que es lo que les comentaba la sesión anterior sobre el psa, que es Cómo hacer una representación de ese Data set que yo tengo en una dimensión más pequeña esto es diferente a pictures selection porque en pictures selection yo estoy diciendo básicamente esta columna sí está columna no está columna Sí esta columna no dependiendo de qué tanto influencia tenga con el target yo puedo predecir eso, eh? Pero este es ya tengo mi Data set definido con el que sé que funciona pero aún así hay
Columnas quiero reducir Esa esa información en menos dimensiones en este caso sería pasar ponle tú de 117 dimensiones. Ah quiero que sea la mitad 50 dimensiones porque de pronto el modelo se me demora mucho tiempo en entrenar O podría tener diferentes razones técnicas en este momento lo vamos a dejar así vamos a ver cómo se comporta con el modelo si se demora mucho tiempo y quizás tengamos que aplicar dimensionar y reducción pero en este caso lo vamos a dejar así finalmente exportamos resultados listo que esto es ya Ah bueno ahorita no lo puedo hacer porque está ejecutándose por allá arriba Eh pero ya quedaría el básicamente el ejercicio completado cuando ya tengo esto que es media taza ya preparado lo siguiente que yo puedo hacer es ahora sí aplicar un profile que es el mismo ejercicio que hicimos al principio al principio que era coge los datos que tengo en este momento y analízalos hay una
Que se llamaba profiling que te hace eso súper sencillo en vez de tu tener que crear histograma revisión de eh, pasillos ta ta ta ta pues esto te lo hace completamente aquí, eh? Voy a abrirlo en esta parte para que lo podamos ver porque no quiero estallar más la maquinita apenas está logrando hacer la tarea lo voy a vivir en Google Para que ya le echemos un ojo de cómo se vería entonces Mira por ejemplo primer detalle se incrementaron las columnas Ah bueno, eh? Alejandro que de pronto no has visto esto Este es el reporte que genera el la librería que usamos, eh? Que se llama Gera profile, es básicamente un un html interactivo muy fácil de entender que te resume todo está todo esta información, eh?
De una forma digamos, amable. Ay, creo que sí se rompió. Creo que le hace una sola Ram ya bien, parece que todo está ahí dice que todo está ahí bien listo, listo, entonces acá, por ejemplo que puedes ver número de columnas número de variables que se usan número de observaciones, que es la parte de acá, si se dieron cuenta ahorita decía 117 columnas, quizás ahora más modelos entraron en el rango del treasure. Eso podría ser Ahora hay más modelos que tienen suficiente información para poder entrenarse, eh? O en algún warhol o algún cambio entonces a veces uno tiene que mirar ese tipo de detalles Al momento de reentrenar un modelo que podría pasar más columnas más procesamiento para poder hacer la predicción, eso sería, por ejemplo, un problema con dimensional y reducción Yo podría acotar eso a 50 dimensiones Ni más, ni menos no me metas más 50 dimensiones fin de momento lo vamos a dejar así. Sí, ya vemos la necesidad lo
Lo lo implementamos número de observaciones número de columnas de filas, Perdón filas en la tabla Entonces tenemos 37606, eh? Ahorita teníamos 50 y pico mil 57.000, pero recuerden que este es de El dataset anterior que teníamos entonces, posiblemente esto también cambiaría aquí te dice el número de de celdas perdidas número de duplicados de hecho este Está interesante Ah esto quería revisarlo. Yo no diría, no, a ver, hay un 10% duplicado. Eso no lo he visto, sí, 10% B Ahí está bueno tocaría mirar si es que son carros que son muy parecidos y lo están vendiendo al mismo precio, podría pasar podría pasar que lo que no sería un error, simplemente sería parte del dataset O podría ser algo que pasamos por alto, eso también podría ser entonces acá, por ejemplo, te dice la distribución de este msrp es el
Precio sugerido por el manufacturador por el que creó el carro entonces por ejemplo te dice la distribución destino te dice cuántos valores distintos hay mínimo máximo te lo plotea aquí Incluso le puedes colocar mor y te dice más información cuartiles, o sea todo esto te lo simplifica que a mí me parece una maravilla porque a veces hacer estos de las partes más mamonas de hacer no hay valores negativos bien o luce súper bien en años en años Sí hay un, eh? Un escudo que es una una cola muy larga incluso después de haber aplicado después de haber aplicado el el la insulation Forest aún así hay una cola muy larga ahí tocaría mirarse sí sí tenemos que mochar también en cierta manera para que la cola no sea tan larga si es que esto llega a afectar el modelo en su en su predicción que ya lo veremos la edición que
O sea, él él le hizo la edición que dijiste ahora va a eliminar, es global outlier cierto de los que son los que si finalmente hay una instancia un registro que es muy raro en todos los bichos que tienen correcto, puede pasar a cada vez que puede ser algún carro viejito, pero en realidad es un carro viejito que todavía tiene características muy parecidas a pero están dentro del dentro del Rango en las otras características en otras dimensiones, lo tienes perfectamente Sí señor, porque solamente un ejemplo que se me colocará aquí Puede que sea un carro de colección y por más que sea viejo no tiene millas, no tiene tantas millas. Entonces entran este Rango de normal, podría estar ubicado acá Mientras hay un carro que lo estrenar en el año pasado, pero ese más leído chancleta y por eso es que está por aquí ya muy alto, lo que hace es, eh, el el
la isolation Forest es borrar los globales eres muy raro en muchos aspectos, eres muy raro Entonces no no te puedo utilizar básicamente César qué tanto de pronto crees también que puede afectar el tema del de la de la magnitud de los números porque Mira por ejemplo el mal hecho está en orden de 10 a las 6 y años está al 1000 después Alejandro sin más los precios también para responderte la razón por la que si te si te diste cuenta acá tampoco aplicamos escalabilidad pero yo sí comenté posiblemente si lo apliquemos Al momento de de utilizar redes neuronales es porque el algoritmo de res neuronales sí es sensible a la escala, el si es sensible a la escala el funcionamiento de rangos más o menos menos uno y uno o cero y uno pero el algoritmo que vamos a usar ahorita es de el campus o el Random Forest
No son sensibles a la escala o no tan sensibles no hay tanto problema, que tengas Diferentes escalas, Sí eso por aquí lo veíamos en este librito Uf yo creo que va a tocar parar ese muchacho es porque ahora si el computador Lo siento súper lento, pero miren ya avanzó ya me imprimió la primera línea bien Creo que imprime como unas 10 por lo que te va a faltaría y pasaron ya 15 minutos seguro como una hora, dos horas más o menos Entonces con su permiso lo va a parar como para que pueda mover bien la la máquina incluso parando es es una demora, pero pero ya va a parar Sí reinícialo y liberé la eso solo Necesito que me libre y se dan cuenta ahí ya está bajando mi mi procesamiento ya el computador si tú estás respirando está respirando, pero entonces, eh? Este libro que es el que yo.
Recomendado, porque es arranque era que es el de voy a colocar aquí este hands on Machine learning with skiller y Cat Us y tensorflow la belleza, este Explica cómo funcionan los árboles de sesión y también, eh? Viéndolo algoritmo uno entiende que no es sensible a los diferentes valores que tengan los fichos no es tan sensible da la naturaleza del algoritmo Entonces en este caso no es tan problemático que tenga esa ese comportamiento Sí sí, resuelva y dudo Alejandro entonces acá continuamos con la con la revisión del documento stock type binario bastante bien. Entonces, por ejemplo Aquí es donde empieza a ver, eh? Que ya la interpretación Se va perdiendo que esa es una de las cosas que a veces también van a ver en la documentación que dice que cuando a veces intentas explicarle a alguien de negocio.
Cómo hiciste la transformación a veces es difícil porque no tienen el contexto completo de Por qué se hizo una transformación u otra cuando se pierde esa traza del negocio, o sea, yo puedo entender el año el año lo entiendo Hay carros muy viejos Hay carros nuevos sencillos las millas también el stock Time también es nuevo es viejo fin Pero ya cuando le meto el modelo que el modelo la aplicamos Hash el hashtag acá me dice si hay un
Este documento que para mí creo que tiene sentido eso creo que podríamos tener Hash más pegaditos y no crear tantas dimensiones porque es justo, eso es lo que está pasando porque me las están creando todas estas dimensiones Uf ya vamos 22 23 24. Yo creo que sí, 30 y pico Uff sí, 40 es por los modelos, pero ah, van hasta lo último 40 50 51 aquí yo diría quizás quiero pegarlos más, pero ya también vamos a tirarlo Así vamos a ver cómo se comporta el modelo entonces acá con el color Recuerden que el color lo convertimos en cinco dimensiones misma idea ya no es un color es un vector el vector se compone de el valor que está acá el valor que está acá que está acá, que está acá y el que está acá acá.
Sí señor, correcto en esto usamos World to vector color interior misma idea muy parecido que ya la interpretación Se va perdiendo ya no es fácil verlo, eh? Pero ahí vamos teniendo la idea, entonces este es el one qualing que sería, por ejemplo all Drive ahí se ve bastante bien es la columna que pasé de categórico. Cuan eh One incore. Tengo una nueva columna. Entonces acá pasa lo mismo Front Aquí tengo el Front Aquí tengo el río y ya solamente son tres categorías. Si no estoy mal, sí, Al parecer, superbien, make, Ah okay. Esta es la marca Esta es la marca está también la aplicamos One Horizon Core Entonces en este caso este este sí va a tocar ver cómo se maneja Chevrolet tiende a ser mucho bastante lleno esperado, pero esper.
Que y si se dan cuenta esto hasta te dice como que alarma, No qué alarma tienes de cada uno acá te dice que está desbalanceado cuando ya haces un guajon que te dice que está desbalanceado no es tan problemático cuando tienes la categoría pura ahí sí, porque porque ya te hay indicios de que hay muy pocos datos, pero esta es la transformación entonces puede que estemos todavía en un en un en un Rango seguro que lo diga no significa que sea un problema porque se le aplicó One For en corin. Se espera que se disperse más el dato a través de todas las columnas, entonces acá tenemos Mazda Subaru Toyota Volkswagen etcétera, etcétera, Ah pero aquí está por insta a él por ahí está el tema la aplica más bajo en corin, eh? Ah no sí, Doris Ah sí sí sí, ahorita le aplicamos. Sí, sí sí, sí, señor, incluso Acá hay uno que es, eh?
Indefinido, que lo habíamos dejado así también un voice tal que no sé qué tipo es sencillo el cat otra vez vectores en este caso solo son tres bien. Full type, Electric o ajo y kori. Y hasta ahí. Hasta ahí vamos Sí esta librería también te permite a ti encontrar relaciones entre diferentes tipos de datos. Voy a colocarlo en unos que son más sencillos de entender, por ejemplo, el año va a colocarlo año versus el precio sugerido sí Entonces si se dan cuenta año en la parte inferior precio sugerido en esta parte por aquí nos pinta 1960 porque por aquí seguro está ese año rarito que que vimos en el en la distribución original por aquí están la mayoría de carros que son prácticamente nuevos desde el 2010 nuevos por aquí casi no hay y aquí el precio sugerido Pues en esta parte.
Acá, qué te dice? Te dice dos cosas Aunque aquí no se ve tan bien. Aquí sí sería genial quitar estos layer, Porque podríamos ver, quizás una tendencia de que a mayor año a más años Puede que haya mayor precio sugerido que es algo esperado a través de los años Pues el precio de los carros va aumentando debería haber una línea acá ascendente Y estos colores, el color oscuro o claro te dice dónde se encuentra la mayoría de casos es decir dónde se encuentra la mayoría de casos de de estos precios y este año la mayoría se encuentra ahí coloquen como si esto fuera si yo lo plotear en tres dimensiones a más a más azul más alto Entonces ya tendría aquí dos Torres bastante grandes, pero me dicen que ahí se encuentra la la mayoría de mis datos Entonces cómo se dan cuenta. Esto es muy informativo esto a mí me quita, me me ayuda a encontrar patrones, me ayuda a ver ese tipo de cosas de hecho, a ver si pintamos años versus millas.
Esperaría que fuéramos chévere, pero no es este comillas versus precio sugerido venimos a ver qué tal Entonces le metí Cat no es este versus millas, pero si los online no nos permiten ver tan chévere, esto triste tocaría quitarlos porque se llama bacano, pero por ahora Dejémoslo así la próxima sesión si algo los revolvemos por ahí se pueden ver las diferentes relaciones, hay, hay unos que se ven muy bacán, se los aseguro acá también te dice correlaciones, por ejemplo, cuáles tienen más relaciones con otros por aquí entonces entre más entre más rojo más correlación inversa que es mientras uno sube el otro baja. Eso es correlación inversa a más azul correlación, eh? No sé directa si sube uno sube el otro Entonces yo esperaría que por ejemplo, uno muy azul.
Yo esperaría que fuera, ponle, no sé el año versus el precio sugerido porque si uno sube Pues el otro sube es lo que no esperaría ver, pero en este caso hay muchas no se alcanza a ver tan chévere, eh? Este tipo de algoritmos es el que se usa para poder encontrar, eh, Cuáles son los features más importantes que era lo que también veíamos acá la selección de los fichos, aunque no todos los no todas las variables tienen ese tipo de comportamiento aquí lo que hacen es para encontrar el feature más relevante es coges el precio del Target el precio y lo evalúas con cada uno de los fichos Y si yo cambio el precio y la variable cambia hay una correlación. Sí dejó todas las demás quietas, pero si hago ese cambio hay una correlación, pero no siempre las correlaciones son lineales, no significa que cuando sube precio la otra sube y cuando baja el precio de la otra sube algo si no.
Sino que a veces puede pasar que sean después de cierto Rango del precio, si sube, pero después de cierto Rango baja, pero después de ciertos rangos sube Podría tener un comportamiento no lineal y en ese caso Hay unos algoritmos que se dedican a encontrar esa relación que te dicen a ti, eh? Hay uno que me gusta mucho que se llama mucho al information, eh? Básicamente evalúa, Qué información comparte Cómo muta la information Y a partir de eso evaluó el cambio que puede tener una respecto a la otra que no siempre es lineal Aquí también se miran valores perdidos aquí se ve horrible porque tenemos muchas de tirar los duplicados De hecho. Aquí están chévere verlos, pero entonces ya ahí terminaríamos como la interpretación con estos datos, tomamos decisiones miramos y tenemos que hacer algún cambio en el pre-proceso, si de momento estamos conformes con eso lo tiramos así y el siguiente paso sería muy sencillo, ojo todo este Script yo lo hice de esta manera, porque así me
Espacio hay una cosa aquí en visual Studio code que es control shift p que te Abre pues El Comando arriba y acá tú puedes exportar este Notebook como un Script de pyt Entonces él me transformaste este código que yo tengo acá en este código y yo lo hago con control shift p escribo export, eh, current current python, Ah no al revés es al respecto. Tengo que saltar acá en el Notebook y otra vez export y acá export to python Script Aquí está me genera este archivo y aquí lo que básicamente se hace es, pues borró todo lo que no sea lo que no sea ya un Script puro que sería las transformaciones a mí ya no me interesa ver por ejemplo el shape, ya no me interesa. Eso lo elimino el duplicate si lo mantengo porque eso sí me interesa las funciones también las mantengo Porque si me interesan todo lo que sea como ya informativo como todo esto pues lo voy borrando, no me sirve.
No me interesa Y como resultado yo tendría ya un primer Script que sería el pre-proceso que se vería algo, así que sería un Script muy sencillo, que tiene, eh. Las funciones todas las funciones que se han aplicado Ese es el, eh, eh? Yo suelo colocar todas las funciones en la parte superior todas las funciones de todos los tipos que ya las conocemos y hecho aquí se me demora No falla, posiblemente aquí tengamos que hacer agregar. Está categoría nueva ahí la agregamos de una vez porque si no falla estoy seguro Aquí está tu última categoría que agregamos acá tocaría agregarla Aquí también la próxima sesión si algo lo limpiamos un poco para que funcione mejor tengo todo el Script Ahí vamos uno a y está el pre-proceso el preprocess siguiendo, eh?
Este es uno de los principios más fundamentales en programación que se llamaba solid principles creo que también ya lo hemos hablado que son como unas reglas básicas que se tiene para programar y que te sugieren más o menos, qué debería tener una función o una clase o un Script para que te para que siga unas buenas reglas y no tengas problemas técnicos en el futuro problemas de performance diferentes tipos de cosas, entonces son es un acrónimo, son cinco reglas bastante sencillas cinco responsabilidades que es el Script que hace sobre la función que haces, debe tener solo una responsabilidad, por eso es que nosotros estamos dividiendo aquí creamos esta función, pero creamos funciones independientes para cada una de las tareas específicas que tenemos que hacer open open Close principal, que es Open two expectation closed modification, que es, básicamente Cómo vas a utilizar esa función.
Y Cómo permitir que esa función extienda su su operación, o sea, se extiende en funcionamiento, pero no necesariamente cambia el funcionamiento que ya tenía, sí, en este caso es como por ejemplo, yo les colocó. El ejemplo acá se vería perfecto uno de los ejemplos sería Ah no es que ahora Quiero agregar dimensionar y reduction en la tarea de preprocess Quiero agregar dimensional y reduce entonces dimensionar y reducción me me pidió un parámetro de número de dimensiones Entonces yo Qué hago yo lo puedo agregar como un algoritmo como un como un parámetro de entrada y le puede decir por defecto, quiero que sea 5 pero puedo meterle, pues Cuántas yo quiera y por aquí abajo, yo llego y meto el Script que antes de exportar los demás meto el Script que va a hacer esa función que es va a reducir la dimensionalidad pero si se dan cuenta todo el comportamiento anterior no lo modifiqué lo extendí, no, no fue.
Yo rompiera y cambiará preprocess y le y le quitará parámetros ya existentes o modificar algo muy fuerte, sino algo que ya existía extendí su funcionamiento y no va a haber ningún problema, entonces aquí hay más interface agrégation. Eso lo vamos a ver también después, pero por tiempo digamos que voy a saltar porque sí me gustaría que hiciéramos ya como pensar qué tipo de algoritmo podríamos utilizar para la la regresión en todos los argumentos que venden trabajo, eh, Son los que usaría el preprocess que sería el el documento original, el pad, el PayPal se lo enviaría, se leería aquí adentro y empezaría como, eh? La tarea, eh? El tamaño del Data set de entrenamiento que eso no lo hemos tocado en la exploración, pero ahorita sí lo va a tocar y lo va a explicar acá, por ejemplo, también coloqué el el price 3 sol, que es el el precio mínimo que creemos que es válido que son 1500 dólares.
La frecuencia que necesitamos mínima para para saber si una marca la vamos a utilizar o no, que son 300, que ya la hemos visto también y aquí está el el algoritmo para el Hash Aquí está para las dimensiones del color las las dimensiones de color interior exterior las dimensiones del Cat todo esto aquí está Incluso el que definimos si queremos que entré en el imputado o no queremos que lo entrene Aquí está Qué tipo de de contaminación, quiero que se borre en el isolation Forest para que borre los anomalías todo esto lo puedo lo puedo ingresar como un argumento en el pre-proceso y lo utilizo, entonces acá descripción de cada uno de estos, qué me retorna a mí que es la es es el resultado vendría siendo el dataset de training el target de training. Ya vamos a ver por qué funciona así el dataset de eh, de testing y el
De Test porque recuerden Se entrena con un dato a set, pero se evalúa con otro ataque que el modelo nunca haya visto eso es importante porque si yo entreno y testeo con el mismo tren Podría tener un un resultado sesgado Porque si funciona muy bien en el train el ejemplo que yo les colocaba en el carro Cómo va a aprender a manejar un carro si aprende a manejar una marca específica y te evalúan sobre esa misma marca, pues lo vas a hacer perfecto, pero si te tiran otra marca nueva la embarraste Entonces es bueno que el modelo sea capaz de generalizar y para evitar que eso suceda se separa se separan en dos, eh? Aquí también me va a entregar el modelo impulso y el modelo de detección de anomalías esto por lo que les comentaba de que posiblemente en el futuro, si yo ya lo despliego quiero poder se necesita rellenar los campos vacíos que me entreguen y encontrar si el el el la predicción que me están pidiendo es rarita o no.
Y una vez Cómo colocar esa alarma Entonces paso sencillo así como diferencia respecto a la exploración que se va acá, pues se lee el archivo aquí en esta primera parte se hace todo lo que sea limpieza aquí se limpian todas las columnas que sean relevantes aquí se creó esta este Script que tiene solo una lista de las de las columnas que son relevantes en vez de borrarlas irrelevantes, vamos a solo filtrar la relevantes así si hay columnas nuevas, pues no me interesa, eh? Removemos duplicados Por aquí hacemos el filtro del precio por aquí, eh? Que si está vacío por aquí el filtro del precio con el tres show, eh? Aquí miramos si hay algún Drive train que esté vacío también que habíamos definido, que eso también era inválido Aquí también algún tipo de combustible. Está vacío. Lo borramos que son muy pocos, pero hay eh, Por aquí se hace otro filtro Ah la categoría. Esto es a partir de la categoría de
Marca perdón, el tres son de la marca, se hace toda esa limpieza, eh? Luego acá acá es la una de las partes importantes que es acá separó el precio que es mi Target Lo separó de El Data sent con la misma razón, que les comentaba no queremos que interfiera al momento de ser imputación y demás y se separa en dos en dos conjuntos diferentes uno que es de T de training. Otro que es de Test y que también me entrega el target de uno y el target del otro entonces tengo cuatro objetos. Sí, básicamente que son estas una lista Esta es una lista que es toda la lista de precios, Eh $50,000 $10,000, entonces ese primer $5,000 corresponde a marca Chevrolet con ta ta ta ta ta ta ta Sí pero entonces simplemente lo separe lo separe. Eso es lo que dice aquí todo esto aplicó todas las transformaciones que hicimos el Fisher el feature hashtag, que aparece acá el works vector, pero ya hay una forma mucho más ordenada. Si se dan cuenta ya se ve limpio One horico.
Ta ta ta ta ta le aplicó todas las transformaciones, eh? Hago todas las concatenaciones y demás si se dan cuenta acá algo importante, eh? Acá no se ve tanto Sí aquí, se ve aquí hay un ejemplo, por ejemplo, cuando yo estoy utilizando un modelo, eh, Y estoy haciendo Esta preparación de datos. Yo preparo los datos con training Pero aplico todas las transformaciones a testing también, pero no entreno contestó Entonces en este caso. Si se dan cuenta, yo estoy entrenando el one home con training, que es el que aparece acá si se dan cuenta este modelo de One corrientes, se está entrenando con tren, es el que está acá acá se ve lo aplico a joder De hecho. Aquí eran barrer, porque aquí Debería ser transformó más, Sí aquí, mala mía de hecho. Lo voy a dejar así original por si llega a fallar, pero aquí solo yo debería aplicar la transformación transform.
Sí entreno aquí transformó y hago lo mismo con el test, pero solamente transformó no, no entreno, vale? Esto porque hay algo que se podría llamar, eh? Como fuga de dato que si yo entreno también Contesten por ejemplo, un emputa el imputado Podría tener un sesgo con la información que ya tuvo en en testing Y eso podría afectar al momento de hacer la evaluación Entonces eso podría ser un inconveniente entreno con Trini aplico a training y a test, vale ahí para que quede claro se ve más se ve más claro, eso que les comento al final aquí en el imputado, que es Mira acá. Estoy entrenando con tren y está acá aplico Ah bueno, aquí estoy cargando perdón, aplico trending y a test, Sí pero nunca entreno contestó, simplemente siempre con tren, vale Para que no haya fuga de datos.
Un lado del otro Eso se llama tataliken con testing aplico con aplico para testing y para, eh, Y para tren para ambos, pero entonces siempre Se entrena con Trini y aplico a los demás y si se dan cuenta ya me queda limpio el básicamente el Script me queda limpio Aquí estoy en en retornando el imputado el modelo porque Quizás lo quiero guardar quizás quiero hacer algo después utilizarlo, eh? De momento lo estamos retornando también de la selección Forest lo estamos retornando, eh? Y esa sería la primera parte de nuestro pipeline que ya un pipeline sería los pasos que acabamos de mencionar al principio preprocess colecta información la preparo siguiente paso entreno siguiente paso pruebo y siguiente paso despliego en este caso, El Paso más largo, por fin después de muchas sesiones, ya nos quedó terminado.
Que se vería más o menos así acá simplemente que estoy haciendo en este Payne que todavía es muy rústico, pero aquí se entiende un poco es este es el argumento de entrada, que lo voy a enviar, que quiero que sea, eh? El pad Dónde se encuentra la información de los carros el tamaño que quiera que creo que tenga el el test size, en este caso lo modifiqué para hacer unas pruebas y si quiero o no que se entrene el imputado en este caso también le diré que quiero que se entrene entonces si yo ya lo tiró acá en la parte inferior veríamos Pues los logs una forma muy ordenada muy chévere, aquí ya podríamos ver, eh? Toda la información esto, por ejemplo de words y demás es porque la función de Word to vector sí encuentra que no es una palabra válida, te lo imprime y la la reemplaza por vacío, por eso es que aparece toda esta lista, pero aquí, por ejemplo, te dice mira acá aplicando otra formación de El drag. Rain aplicaciones de la transformación del meic el one for incor.
capital aplicando la transformación del Cat work to vector ahí ya vamos y acá el imputado que es el que se demora como un montón Entonces ya tendríamos los datos listos para entrenar según el Paint Cómo se tendría que ver esto pero antes de seguir todo bien chicos dudas o todavía listo Perfecto entonces esto Cómo se vería ya colocándolo en en términos del python, yo tendría una función que se llama preprocess que me retorno los valores son estos bien Ahora ya tendría una función que se llama train que ya ahorita la creamos De hecho no la he creado que recibiría la información de trading está acá y quizás esta información también y si hace falta Pues algún otro hiper parámetro dependiendo del algoritmo que yo vaya a escoger se colocan otra en otra en
Script para mantener lo que les comentaba de cinco responsability hay un Script dedicado a entrenar el modelo no vas a hacer nada más que entrenar el modelo, eh? Pero se vería algo así entonces vamos a hacer el trabajo de crear esta carpeta. Yo suelo crear carpetas para cada uno de los de los pasos, por ejemplo Acá está pre process, aquí están toda la información de preprocess me suelo crear una carpeta dedicada a train que está aquí y aquí creamos un Script que se llama train muy sencillo también el nombre nuevo y acá simplemente creamos una función después le colocamos los textos y todo pero ahorita para que sea de una forma sencilla acá le decimos usted va a recibir esto y va a recibir esto Estas son las dos cáscaras recibir para recibir las características y la lista de precios la tabla con las características y la lista de precios Ahí vamos bien entonces aquí no empieza a conseguir, qué es lo que espero que esto me me retorne a mí esperaría que yo que esto
Un modelo ya entrenado cosa que sería regresion model y quizás un json con una métrica results Jason voy a colocarlo Así que es básicamente te dice que también lo hizo en el entrenamiento dos cosas sencillas para entregar sí Y acá, pues para que ya lo lea el pib, eh? Acá vamos a importarlo lo importamos ya lo lee bien ya lo lee cómo funciona ya no nos tira error y aquí le decimos usted va a recibir estas dos cosas usted va a entregar estas dos cosas por ahora y Esperamos que usted retorne Esto entonces acá ya sería la parte de frame y acá sería la parte de pre procesos y ya sencillo, está ahí quedó quedó súper.
Ahora entrenar uno preguntaría Cuál es la cuál es el el más conveniente Aquí vamos a a utilizar dos de hecho tres, eh? Pero vamos a hacer el camino largo solamente para aplicar ya todo lo que habíamos estudiado al principio Pues de toda esta de todo este grupo de estudio que hemos, eh, su proyecto machines que vimos recién lineales logarítmicas, eh? Polinómicas que vimos también, eh? Árboles de decisión Random Forest catbus, eh? Exiguos todos esos algoritmos que hemos visto a excepción de redes neuronales, que ese es el tercer algoritmo, que vamos a utilizar entonces uno se pone aquí a definir qué Qué tipo de problema tengo un precio. Sí, tengo muchas características. Hay muchas filas también. Entonces uno se pone a pensar Quizás El lineal. Queda corto así el lineal sea con muchas dimensiones son lineales.
Posiblemente hayan relaciones curvas, no también podría pensar el polinómico Pero quizás también tenga un un problema parecido, además que el polinómico también es sensible outlier y Recuerden que dejamos suppliers por ahí suelta en algunas características. Uno podría pensar en Support verter, machines soporta machines es sensible a la escala y adicionalmente también es sensible a los likes. Entonces quizás tampoco sea la mejor opción en este caso no puede mirar árboles de decisión puede que sí, pero necesito una solución más robusta Ahí es donde pasó a Random Forest Random Forest puede ser algo que podamos utilizar ahí No ya empieza a encontrar a filtrar dependiendo las características y si me voy a pensar por ejemplo y se Y por qué no usas de pronto una red neuronal, la red neuronal, posiblemente se demore más tiempo en entrenarse Entonces en ese caso podría ganar Random Forest ahí uno va va agotando entonces como que de pronto ya la solución de red neuronal para
Tema que tenemos es demasiado, No necesitamos tanta capacidad para para resolver el problema, si ya fuera una regresión muy complicada con muchas variantes y muchas cosas uno dice de pronto de la complejidad que hay quizás una red neuronal, podría funcionar mejor o por alguna otra razón podemos utilizar, pero en este momento vamos a utilizar, eh? La que conocemos de The Random Forest de hecho, vamos a utilizar dos versiones una que es el Random Forest puro purito. Este es el el lo que ven en pantalla. Ahora es el proyecto que vamos creado sesiones anteriores para hacer la predicción de los precios en Medellín y aquí básicamente la idea era entrar en calor y entender cómo funcionaba, eh? Picar botones y demás entendíamos que era un hiper parámetro como que que algoritmo se podían hacer para encontrar los mejores hiper parámetros como los podemos implementar y además.
Y esto pues es un recordatorio de lo que habíamos visto en sanción se utilizaba para encontrar un súper parámetros, eh? Un tipo de barrido que se llama vallese Search que es ahora. Sí les puedo mostrar la imagen con más con más precisión que otra vez a través de probabilidad. Él va picando diferentes hiper parámetros del algoritmo y va identificando con probabilidad, cuál podría ser el mejor siguiente hipermetropía, puedo escoger para que me dé un mejor resultado entonces otra vez el truco de que los mismos algoritmos de Machine learning se ayudan a resolver sus mismos problemas esa idea todavía me me gusta mucho y es muy parecido a esta estos puntos verdes Entonces digamos que él empieza por acá. Él empieza aplicando hiper parámetros acá y dice estos Todavía están muy arriba en el error Todavía tengo mucho error Entonces me di cuenta que cuando Piqué acá me reduje entonces con probabilidad de pronto me di cuenta que sí, picó acá y descarto el de acá con
No me refiero que también pico acá, pero me di cuenta que aquí se embarró más por lo que este camino no es entonces me voy caminando para acá. Voy para acá. Voy para acá. Voy para acá hasta que voy encontrando la zona más ideal en la que en la que me encuentro también podría utilizar otra que es Random Hay una que se llama Random que es como tire de los en todas partes aleatorio pico acá aplico acá pico acá y me dice cuál le funciona mejor no sigan ninguna no sigan ninguna secuencia informativa Cuál es el beneficio de Random dura menos tiempo, por ejemplo este dura más tiempo porque tiene que hacer este tipo de operaciones adicionales o hay otro que es el grit, que también la hemos visto que es una malla. Póngale ahora ustedes, que todas estas líneas amarillas, eh? Amarillas blancas En dónde hay intersección es un punto en hiper parámetro que va a escoger entonces pico acá pico acá pico acá pico acá pico acá. Sí, pero si se dan cuenta, puede que acá yo nunca llegué al más cercano también, qué pasa? Se demora más que el Random pero menos que el valle seamos entonces.
Entonces ahí también uno. Mira tiempo recursos Sí sí lo puedo hacer si no lo puedo hacer de momento yo creo que sí, con el dato sé que tenemos, lo podemos hacer entonces se utiliza ese tipo de algoritmo para poder eh, saltar en hiper parámetros se utiliza Random Forest no es sensible of Legends permite Diferentes escalas, eh? Es un algoritmo que funciona bastante bien Al momento de hacer una regresión sí, a pesar de que me lo entregué por rangos, porque él no me entrega un valor continuo, si no me entrega un valor por rangos, que es lo que sea aquí en el libro, eh? Acá. Si se dan cuenta, me entregas el valor, pero por rangos no me lo entregan un valor discreto Exacto si no me entrega por rasgos desventajas, entonces uno ya empieza a mirar esta operación de desventaja, pero cuando se utiliza los embebidos que es el que vamos a utilizar con cactus pasamos de esto es horrible porque se usa puros rangos a la izquierda decisión threed pasamos a algo que está.
Fecha que ya aparece una curva mucho más fina mucho más precisa, entonces ahí está el poder de usarlos embebidos Respecto a los a los que son normalitos, que también lo hemos visto en la sesión anterior. Entonces, eh? Perdón en sesiones anteriores, entonces acá va a copiarme esto porque la pereza puede más y la va a pegar por acá y la vamos ordenando así por partes, entonces Esto me lo pegó por acá. Tengo acá bien todo esto lo muevo a la derecha para tenerlo tengo que importar Random Forest eso tiene que estar por allá arriba. Yo creo en el importe, Dónde está la importancia. Este es el primer Exacto lo cojo no importa para que no me
Error bien, Ahí vamos, listo Perfecto perfecto, entonces acá 30 set este le cambiamos simplemente el nombre acá se llama x Data set, vamos así entonces Este modelo ya entrenado, sería el de regresión acá, podemos colocar este nombre Sí así se vería entonces acá para definir Qué hace cada una de estas partes del código, aquí estoy definiendo el espacio en el que quiero que se ejecute ese, eh? Esos hiper parámetros lo que aparece acá es un diccionario, si se dan cuenta el diccionario tiene una palabra clave y tiene un valor que en este caso el valor es básicamente esto es un Rango le digo Son enteros y quiero que comiences desde el 10 hasta el 100 y vas pasando uno a uno 10 11 12 13 14 15 100 acá.
Lo mismo que haga con otro hiper parámetro que es la profundidad que que tiene, eh? Cada árbol, quiero que pases de una unidad a 50 unidades, eh? Acá Este es otro, eh? Creo que es cuando se divide entonces tengo que leerlo otra vez no me acuerdo Pero que tiene el Random Forest este de acá, Ah Pero espera yo la embarré Esto no es acá, sino acá Aquí se hace el fit Pero él el modelo se inicia acá error mío, va a dar cuenta. Entonces acá inicializo, eh? El modelo, pero si se dan cuenta, yo no le estoy mandando los hiper parámetros que aparecen acá porque en ese caso lo va a hacer, pues me algoritmo que estoy utilizando para encontrar. Eso es súper parámetros que es el valle siam Entonces se llama se le suele Llamar optimizador inicializo el optimizador. Le digo vea, este es el Ah de
Esto no debería estar así esto Debería ser así acá acá Ahora sí, capaz, que por eso no me funcionaba el de regresión cuando hicimos el de él de Medellín y eso no bajaba Quizás por eso ahí tiempo después uno se da cuenta de esas cosas, pero acá le estoy diciendo Ah este es el algoritmo que va a usar para hacer la regresión. Estos son los hiper parámetros que quiero que cambie y en estos rangos quiero que integre el número de iteraciones que creo que hagas son, eh? Son 25 que eso también leyendo cómo funciona es básicamente repites repites repites. El el ejemplo que yo les decía de pronto como es como si fueran a Naruto que Naruto se multiplica sí, Naruto aprende cada uno de los clones aprende y otra vez cuando todos los colores vuelven al mismo Naruto todos tienen la información del entrenamiento de todos misma idea es
Cómo que multiplicas todo el ejercicio y luego résumés todo y aprendes mucho más rápido porque todas las demás personas estuvieron trabajando al lado tuyo y colectaron conocimiento que tú, Posiblemente no tenías Entonces ese tipo de cosas y este Sí vi es porque hay algo llamado que, eh Foods eh, Ya les muestro A qué se refiere key Foods se utiliza cuando a veces hay algoritmos que pueden tener sesgo Perdón Data sets que pueden tener sesgo o que no tienes suficientes datos en la set entonces para entenderlo de una forma sencilla hay un tercer Data set que se llama Data set validador tenemos el training validación técnica Entonces el training es el original el que tenemos para el entrenamiento testing con el que evaluó y el validador a veces se deriva del
Training puedes utilizar Exactamente para validar es como para hacer pequeñas pruebas de evaluación en el momento que estoy entrenando para saber que también lo estoy haciendo Y si tengo que ajustar más o no o en qué dirección tengo que ir acá se puede ver un poco acá el nombre lo tiene el mal técnicamente no debería llamarse test debería llamarse, eh? Validación acá le tiene el nombre de Test pero Debería ser validación validation de hacer entonces él qué hace él coge mi dataset Entonces esta primera iteración, por ejemplo la iteración 1 el parte del dataset en cinco partes Esto es lo que está haciendo acá parte de la estación de cinco partes voy a utilizar cuatro partes para entrenar y una parte para validar entonces esa parte la dejó original no la utilizo para el entrenamiento al final cuando haya utilizado todo el algoritmo el al final que cuando se haya entrenado en esta primera iteración yo llego y evaluó con este con este último dato y aquí yo cojo información y digo lo estoy
Todavía no está haciendo mal que también lo está haciendo y así lo hace a través y si se dan cuenta Va iterando, entonces ya el último no es, sino va a ser el cuarto, va a ser el tercero y con el resto entreno, va a ser el primero y con el resto en tren calculo el error promedio que hay Y de esa manera, puedo tener un ajuste o un modelo más completo es Cómo ver el problema, es de diferentes puntos de vista. Póngale que es algo, así lo veo desde la izquierda. Lo veo desde la derecha. Me di cuenta que desde acá Hay un sesgo, pero desde otro no hay sesgo y así encuentro los valores que mejor se ajustan para poder hacer la predicción de del carro que por ejemplo en alguno en en alguno de estos datos sets ponle tú que estaba se nos cobró el Bugatti el Bugatti que valió un montón de millones de de de de dólares Entonces nos dimos cuenta que en este el error es muy grande por el Bugatti pero en los demás no Entonces nos dimos cuenta que de pronto ese es una anomalía algo que no tenemos que entender tanto en cuenta ese error Entonces vamos vamos, eh?
Cuando los parámetros más adecuados para que el el error en general se minimice Entonces ese CD ese CV Tan sencillo, que vemos ahí en esta en esta parte es Cuántas En cuántas partes quieres partir tu dato Z para hacer justamente esta evaluación. Tú puedes decir cuatro cinco, eh? Tres Eh ya ya es opcional de cómo lo lo lo lo prefieras manejar Pero entonces Eh ya una vez que lo tienes entrenas, vamos con el fit que es la entrada a partir del optimizador, si se dan cuenta el feed de su cena del optimizador, pero es porque como si yo hubiera cogido el modelo de regresión y lo hubiera envuelto le metí una cobertura que ya sabemos que hace todo eso que acabamos de comentar el último valor, que es el Random State es un hiper parámetro que se utiliza para definir qué tan Random quieres que que se comporte el el algoritmo hay algo que que en programación normal se tiene.
Y es que tú fácilmente Pues replicar un una ejecución, si quieres sí programaste el botón para que abriera una ventana, lo puedes picar 100.000 veces y muy posiblemente en muchos casos va a abrir esa ventana Sí pero en este caso, como estás usando datos aquí, básicamente lo que dices es quiero que me cojas los primeros 10 y luego los 15 los 30 y luego si voy saltando y tú puedes decirle que tan Random quieres que sea si quieres que haya una que intente tener una una réplica, eh, parecida usas un Random de State bajo, si quieres que tenga una variación muy alta usas un Random estoy muy alto por defecto suelen utilizar 42. Lo voy a dejar ahí porque sí, no hay ninguna razón, que lo que lo respalde y Pues básicamente esa sería la la creación de nuestro Modelo Si quisiéramos usar, por ejemplo kadus, que es el que es el otro algoritmo que
Lo tenemos por acá, es muy sencillo de usar catbus, tiene otro zipper parámetros aquí está un montón de capas por acá Aquí está Mira Carlos muy parecido este también tiene unos unos zipper parámetros que yo puedo cambiar el rango El rey de aprendizaje que que lo hemos visto con el descendiente del gradiente, eh? El campus tiene un comportamiento un poco mixto, entonces ahí se alcanza, a ver también está el dip que tan que tan bajo quiero que sea el número de iteraciones y demás y parámetros que yo puedo configurar Pero la idea, es la misma si se dan cuenta acá inicializo inicializo por acá hiper parámetros utilizo el optimizador, que es el que me va me va a totear todos los hiper parámetros por todas partes, eh, En este caso usando balles para que encontremos el mejor camino esperando que nuestro computador Sito tenga lo suficiente para hacerlo, eh? Aquí, por ejemplo, si se dan cuenta aplico que
Pero le digo solamente cuatro en vez de que sean cinco solamente cuatro, por favor, pero la misma idea hago el fit y ya Cómo se utiliza esto ya después para hacer una una predicción se utiliza transform, que es el método que se utiliza para para hacer la transformación muy muy muy igual a cómo lo aplicamos en el one for In corinne, A cómo lo aplicamos en el work to vector es muy parecido tiene Speed y tienes transform y tienes fid transform, que básicamente entrena y transforma el tiempo en este caso los vamos a separar Porque queremos guardar el, eh? La transformación queremos guardar el modelo una vez ya esté hecho ahora Se entrena yo como rayos le le identifico, si lo está haciendo viendo lo está haciendo mal de hecho aquí creo que hay algo perdido porque sí Déjenme aquí abrimos esto.
Hay un hiper parámetro perdido, pero creo que lo dejé así porque es por defecto. Tú tienes que darle, eh? Una métrica que él pueda medir para saber que también lo está haciendo Sí entonces creo que se llama scoring acá en este lugar a ver cómo nos va, no? A ver si en ese será o será en el fit que yo de pronto estoy alucinando Dónde estaba el método de fitz It's no no, no, este acá ven a ver si este nos da chicos, se me está perdiendo hace rato. Creo que no hacía entonces hay.
Indica que nosotros tenemos que o será esta r. Scorre se me perdió porque él va Ajustando respecto a algún error que nosotros podemos definir de forma global La idea. Es que el globalmente pueda encontrar el menor error, pero ahorita se me acaba de perder veamos en el Random por si es que lo hago acá y yo estoy alucinando acá, donde no es criterio, este. Ah, sí, sí, sí, sí, sí, hay un valor por defecto que es el el Square Entonces nosotros también en sesiones, anteriores. Habíamos visto que tienes un modelo y tienes que buscar esa métrica que Define que también lo hace o no, Sí y hay diferentes.
Métricas dependiendo de diferentes tipos de problemas en este caso si te das cuenta tiene cuatro opciones este hecho No sabría si les quedase este tampoco no sé para qué sirven sus desventajas y desventajas desmentido conozco de hecho este Está bueno para consultarlo Ah Creo que este es el combinado Creo que este es el combinado es previo Creo que este es el que combina el este está raro. Tú querías estudiarlo de pronto Quizás hasta sea una métrica propia de los Random Forest está está buena pero para atenderlo de una forma sencilla vamos a ver solamente el valor absoluto versus, eh? El valor al cuadrado básicamente creo que se puede ver bastante bien hablamos este portal a ver qué tiene de
Tú comparas o quieres saber qué tan malo estás haciendo la solución más sencilla es, pues resto básicamente el valor que predije menos el valor real y encuentra una diferencia, entonces entre más pequeñita sea Esa diferencia mucho mejor Pero yo hago eso con cada una de las predicciones que yo hice con todos los carros que estamos cogiendo cogemos el dato que predice el dato real Calcula diferencia Y a partir de ahí el modelo empieza a ajustar Él llega y dice en total la embarré malísimo que suele ser así Al principio. Él dice en total las estoy embarrando súper mal, pero si me muevo en esta dirección y cambio estos hiper parámetros y empiezo a ajustar, me di cuenta que el error sea reluciente el error sea reduciendo el error, se va reduciendo, va a haber un punto en el que el error siempre va a haber un error mínimo, si los modelos que lleguen a 100% no es real, es imposible, siempre hay un error mínimo Pero entonces, tú buscas que ese error.
Más pequeñito posible lo más pequeñito posible entonces hay una forma de verlo muy sencillo, aunque creo que esto está un poco confusa, Déjame ver si una que se llama esta que solamente muestra estas dos la hugh es la que suma las los llama acordé la hueva es la que suma la absoluta y al cuadrado Entonces por ejemplo acá tenemos al cuadrado, viene siendo línea negra absoluta viene siendo Línea Amarilla roja, que es básicamente si me si me equivoqué porque está por debajo del valor o porque está por arriba del valor, pues cálculo valor absoluto y miro O sea si me equivoqué por cinco unidades por debajo 5,000 dólares por debajo y si me equivoqué por $5,000 por arriba, al final me equivoqué por $5000 no importa 5000 es mi diferencia Sí eso es lo que hace el el valor absoluto. El lo que hace el el valor al cuadrado es cálculo la diferencia pero la
La l o al cuadrado la potencia eso porque a veces es importante porque tú vas a forzar al modelo a que ajuste más duro, si la embarró más duro es decir, es diferente si yo si yo la embarré $5,000, así la embarré si la embarré 10,000 a pesar de que sea el doble como yo le ve al cuadrado el 10,000, ahí tengo 100 mientras si yo elevo el 5000 tengo 25 Entonces el ajuste que tendría que ser el modelo es mucho más agresivo, si yo utilizo el mse, que es al cuadrado así utilizo el absoluto, entonces uno va definiendo cuál métrica funciona mejor en este caso uno empieza a mirar, Qué beneficios, tiene usar uno a la otra, por ejemplo desventajas que tiene la MS si llega un carro que la diferencia es muy grande, que podría considerarse, digamos unos layer el ajuste que yo pueda que yo hago Puede que sea muy drástico y empezó a dañar.
Más ajustes empezó a dañar los que ya funcionaban bien, Qué pasa el mse, es más sensible a los outliers Pero qué es la la ventaja que él va a tender a agresivamente a bajar al valor mínimo agresivamente eso no está, pasa un poco al contrario con esta de de del valor absoluto a pesar de que eran Barré si son 10 unidades Pues a justo, pero no ajusto tan duro por lo que si llegan heiress layers no hay tanto problema, no va a ser tan conflictivo hacer la regresión, entonces uno va definiendo también, qué métrica quiere utilizar para poder empezar a hacer esos ajustes en este caso, vamos a dejar la que está por defecto sencilla así sin tanto dilema y la tiraríamos así, tírala, con con el error, que está al cuadrado y Y pues si llega a ver la necesidad de de ajustar como un hiper parámetro, pues lo cambiamos incluso este de acá podría ser un hiper parámetro que está acá. Sí, Y al final se define pues.
Con la con la con el resultado que vayamos viendo ya para finalizar y ya no les molesta más que esta sesión se fue larga, eh? Acá el último pasó una vez que ajustas es que tú sueles sacar unas métricas adicionales hay dos importantes una que se llama r. Cuadrado y otra que se llama residuales para explicarlo como de una forma sencilla r. Cuadrado es, eh, No es un carro es es una métrica también que básicamente compara predicciones versus valores reales y hay una fórmula que básicamente te dice a ti qué tan parecidos son estos dos datos en general, Sí ya no es una medición de error global, si no te dice.
Tanta representación de la predicción se ajusta con los datos reales este valor tiende a estar desde cero mientras desde menos infinito hasta -1 el valor máximo es uno si es si es negativo el modelo está funcionando Superman o sea, no, el modelo no está representando ni siquiera de cerca lo que está lo que está pasando realmente los precios se está uno que también es imposible, pero sí está muy cercano a uno la representación de las predicciones se ajusta muy bien a a los valores reales Entonces el modelo te está funcionando bastante bien. Es una métrica importante, pero no es la que Define completamente si funciona o no, el modelo y esta de esta también se deriva algo que se llaman residuales, es muy sencillo, entender también, que básicamente es a ver esta.
Nos puede ayudar a Aunque me gustaría uno más llena esta escuela está es muy buena. Está Está chévere, Entonces los residuales, básicamente es la resta entre el valor predicho y el valor real Esa es la resta Sí pero lo que yo hago es hacer un plot de esa resta y lo ploteo O sea primero cálculo la diferencia y luego lo hago versus los los valores que se predicen entonces, eh? Este es un caso en el que se ve muy bien aquí, si se dan cuenta colocando un eje sobre la línea cero, porque ese es el ese es el valor que deberían tener idealmente todos los datos ahí, sí, todos son cero los residuales son cero, es decir, el error es cero el modelo está prediciendo perfectamente lo que sucede, pero en este caso, si se dan cuenta, está disperso el el siguiente eje que se ve acá son los valores predichos. Entonces acá, por ejemplo, tienen un valor que es -2 para los valores de -2 hay un error considerable en esta parte para los valores que están en cero O sea que el que el valor que hay que predecir es cero.
Los que intentan predecir 0 se encuentran por acá en este lado, o sea, tienen un residual muy grande, tiene un residual de -2 tiene un residual de uno, o sea se escacha por una unidad no predicen el cero, sino predicen un uno no predicen el cero sino predicen un -2 y como Yo calculo la resta pues ahí se ve por ejemplo acá para colocar otro diferente a cero deberían predecir uno pero entonces el residual es más o menos 1.70 y pico entonces puede que esté apareciendo tiene que ser uno pero predice 2.75 la resta es el residual hay encuentro el plot y tienen diferentes comportamientos Pero son muy muy eficientes de lo que puede pasar entonces en este caso en este caso es muy particular me pasan un proyecto que tengo actualmente y tiene sentido no significa que el modelo funcione mal Solo que hay de pronto falta información, Qué pasa les colocó el ejemplo que que yo estoy haciendo para que se entienda nosotros tenemos que predecir la duración de un paso en un proceso digamos.
Es que hacer hamburguesas tenemos que predecir con base al histórico Cuánto dura haciendo una hamburguesa. Eso es un paso Entonces qué pasa cuando la duración del paso es pequeño, es decir, el más se demoró 15 minutos el modelo funcionó muy bien el modelo, te predice muy cerca que viene siendo digamos estos casos Supongo que está está en escala de horas. Este está en escalas 10 horas 20 horas y bueno, para el ejercicio hamburguesa, digamos que son minutos minutos para que sea más entender minutos Entonces hacer la primera hamburguesa todo lo que está por debajo de 10 Funciona muy bien, o sea, el modelo lo predice perfecto está cerca, o sea, la desviación que tienen residuales son de Pequeños pequeños minutos, No no, no hay, no hay tanto problema a medida que va demorándose más el man cocinando hamburguesas, vamos atendiendo errores más grandes, Sí si se dan cuenta es como un cono.
Eso por qué pasa? Quizás haya falta de información o sea, quizás cuando le preguntamos al Señor venga por qué se demoró porque se demoró 50 minutos haciendo una hamburguesa y el man dice no es que la pidieron con extra carne extra queso querían que le metiera la minillas de oro, no sé qué ta ta ta pero nosotros quizás no tenemos esa información en el modelo Entonces se demora más y hay más error Pero puede ser porque hay pérdida de información en nuestros datos no tenemos si la hamburguesa la pidieron con las minillas de oro o con triple carne, sino simplemente Cuántas hamburguesas, no tenemos ese detalle justamente ese problema es el que tenemos yo le explico Así Fíjate Imagínese que le piden extra carne extra no sé qué, pero usted solamente me dice cuántas hamburguesas son, pero no me dice Qué tienen y si no me dice que tienen pues no puedo, no, el modelo no puede predecir encontrar esos patrones y encontrar esa relación entre lo que contiene la hamburguesa y porque se demora más.
El tiempo en preparar Entonces los residuales te dicen toda esa información, o sea, te alcanzan a decidir y hay diferentes patrones O sea hay algunos muy interesantes, eh? Tienen diferentes nombres los patrones hay algunos que te dicen si hay outlier Si no hay of Lights eh Y eso te ayuda Cómo ajustar el modelo te ayuda a decir. Ah, tengo que ajustar Acá tengo que mover acá. Sí, ese tipo de cosas son las que las que podría pasar todo esto para hacer un modelo regresivo para saber que hoy en día que ya para cerrar así pues existe algo llamado Otto ML que es lo que también va a estar implementado ya en la próxima sesión, eh? Es un algoritmo que lo que hace es coger todos los algoritmos que vimos e incluso unos ya potenciados catbus Xbox eh? Embebidos de no sé qué ya hay un montón y que más yo uso es este y lo que hace es ésta Ya tú tu memoria y tú y
Eu o si tienes gpu y empieza a procesar, vas a ver esto saltando al 100% pra. Pra. Pra. Pra. Pra. Pra. Pra. Pero porque qué está haciendo está testeando con todos los posibles hiper parámetros con todos los posibles algoritmos, estallando estallando estallando estallando hasta encontrar el que mejor se comporta el que mejor te va a funcionar desventajas, es muy costoso a nivel de procesamiento. Si tienes un dato a ser pequeño Todo bien, No pasa nada, pero si tienes que hacer algo ya muy grande, posiblemente cuando tengas que tirar la factura del cliente va a decir, esto no va a pasar entonces ahí te toca hacer algo más personalizado y que no gaste tanto, pero entonces todo este tipo de cosas cuesta contemplar los argumentos, entonces pycaret hace eso ya lo habíamos hecho también de la sesión de la la predicción de los precios de los bienes raíces, en este caso vamos a cruzar los dedos va a funcionar con la con los carros también y ya terminaríamos esta sesión, así no sé si
Dudas o todo bien hasta hasta el momento sí Recuerden que silencio es bien César ni siquiera sé qué, qué tengo que preguntar sobre la marcha, o sea, créeme que vamos a ver, pero pero o sea, me interesa más eso, Por ejemplo de preparación de datos y demás queda claro que ha entendido porque se hizo de esa manera se queda que ha que ha entendido sobre de pronto lo que les comentaba ahorita de regresión, cómo se hace el ajuste acá, por ejemplo, cuando habíamos visto, eh? Cuando hemos visto la, eh? La parte de todos los algoritmos de regresión, eh? Yo he explicado este que es una forma de ver el lineal, pero acá se ve más chévere porque si te das cuenta Así es como funciona más o menos un algoritmo de regresión en este caso es lineal Y funciona, Porque el dato Zelda es muy sencillo, tenemos dos dimensiones, no, no hay mucho.
Pero básicamente iteraciones las barre mucho ajusto. La embarré mucho ajusto. La embarré mucho ajusto ajusto ajusto ajusto el hiper parámetro que es acá, el hiper parámetro es el Slow que es la pendiente y el el el el vector Sí el vayas que es básicamente, eh? De la Fórmula normal de toda la vida de de la línea que es x * m + b b, es otro hiper parámetro Entonces en este caso. Él está Ajustando son esos hiper parámetros. Él ajusta tengo que mover la la curva más para acá más para acá más para acá más para acá más para acá hasta que encuentre el mínimo error posible, en ese datazo esta misma idea es lo que hace todos los algoritmos que hemos estudiado regresión incluyendo el de Random Forest Solo que su su funcionamiento es diferente, o sea, esto es lo que más me parece importante que que que se entienda, o sea, como, por qué?
Toman ciertas decisiones Por qué se hacen ciertas tácticas para resolver ciertos problemas, cómo funciona? Porque ya sé que el lineal no es Útil para el problema que tenemos Eso sí, eso quedó claro, ahí vamos bien, no palante O sea vamos por buen camino ya el resto muchachos ustedes ponerse ahí ya a estudiar y entender cómo funciona call algoritmo, Sí claro, ahí bien te va a pedir un favor. Si podrías actualizar las clases grabadas en el Drive seguro que creo que nada Está como a inicios de mayo, Sí señor me pongo la tarea de hacer eso, por favor, alguna otra duda chicos o todo bien son muy claro. Bueno todo muy claro, eh? El respuesta actualizado lo actualizo justo de terminar esta sesión porque faltaría algunos casos y
Y Y sí, yo lo actualizo, eh? Wilson también levantas a la pata, sí, César te dejo una pregunta es cómo se está usando, pues hay que leer el problema de los alquiler, ni es que tenemos los limitantes del recurso cierto, solamente podemos dar cpu cierto Y entonces estaba pensando que es que eh, sensor from no es el que trata de de suplir esa necesidad como tal de de de del recurso Cómo están son diferentes, eh? Skiller Yo diría que son librerías complementarias, pero no se suplementan tensorflow. Está dedicado a eh, algoritmos de redes neuronales, se especializan redes neuronales, es lo que utilizo aquí de pronto, o sea.
No hay como que yo sepa Mira quizás, que yo estoy diciendo cualquier cosa, no sé, no sé si Random Forest tenga capaz, que sí, Y aquí estoy diciendo una burrada, Sí yo no sabía me voy a enterar. Gracias, no tenía ni idea Sí porque es que por ejemplo, imagínate en producción del que te no sos amor tantas horas, no, papi, eso ni por tanto le va a pagar por eso Entonces sí me acuerdo porque yo me acuerdo en los inicios de de de de de de de de de de de de de de de de de de mandar Machine learning a a producción y entonces dijeron Pues bueno, pues te infló Flow es el que va a tratar de de sufrir esa necesidad del recurso cierto, porque si si toca y el problema con tensor lo es que sí, es más es una una manera más más complicado o bueno más bien complicarnos Es como un paradigma distinto.
De ejecución no de programación como de ejecución Entonces sería como interesante empezar como también a verlo, pues para Sí se ve como un ver, eh? Esas diferencias como tal de del tiempo y cosas así Mira me acabas de abrir un mundo que no ni sabía que existía, o sea, estoy anotado porque parece que sí parce no sabía o sea, no sé de hecho todavía no, No sabría confirmarte si es más eficiente no según lo que me dices Sí de hecho, creo que hasta hasta que si no estoy mal hasta tensorflow va y se mete allá a a saikil y llama, pero ya obviamente como tensorflow ya lo hace por el nivel de de la de la gpu, no Y mira que la berraquera.
Que hasta se escriben muy parecidos y lo que pasa es que debe ser de keras, porque creo que ambos serían de que eras eras es el núcleo el la librería madre la la Papa De Los Pollitos de ahí deriva tensorflow y creo que también es Killer pero no Mira yo no tenía ni la menor idea Aunque eso podría solucionar problemillas que tengo por ahí no sabía yo solamente usaba tensorflow para redes neuronales, nunca nunca lo usé para Uff Gracias tía, ni idea Está cero va a investigarlo Porque mira si se le puede meter de todo, o sea todo esto es súper parámetros no se comparan con todo lo que vimos acá en el Es que mira aquí tiene que no sé mal contados, que unos 20, o sea, yo cuento como unos 50, entonces acá me imagino que esto ya es para investigador juepucha, alguien que se sabe esto de de arriba Pao pero no se asusten.
Tiene su ciencia acá, digamos, hay para categóricos incluso bacán. Estamos bacano. Gracias Ahí aprendimos todos bacán, no chicos, alguna otra idea o comentario o duda todo bien Entonces nada les repo, eh? Lo que les digo practicar estudiar es como la forma de de salir adelante con esto da miedo, pero ahí vamos, vean ya casi vamos a entrenar nuestro primer modelo próxima sesión la idea Cuál es Vamos a aprender, eh? Redes neuronales Cómo es que este esta silla es larga Uff para qué les voy a decir mentiras, hay cositas que sí, eh? Van a tomar tiempo, pero créanme que vale la pena porque hay muchas.
prácticas hay muchas cositas que entender pero unas que se entiende es genial, cuando funciona es mucho más genial entenderlo limitantes, por ejemplo pueden creer que esto tiene una cosa es como un cerebrito pónganle y haces el cerebro se muere o a veces explota eso es posible en esta red neuronales hay un término que se llama vanishing Explorer en redes neuronales y es porque se explota literalmente se pierde información se muere el cerebro hay forma de revivirlo ahí, o sea términos así son fascinantes porque vamos a aprender el término, verdad o Explorer Explorer in explotar vanish, Sí señor Eso es así si no está mal Sí y básicamente, Qué pasa si tienes por ejemplo muchas redes neuronales, esto le pasa aquí lo explican más o menos bien Se va perdiendo como el camino Se Va Muriendo Se va muriendo por partes y hay prácticas para
Evitar que se muera Entonces es genial, cuando lo ves y lo entiendes y y demás, eh? Es brutal es brutal es bacano ver esos términos así, eh? Que no esperas en tecnología pienso yo, por ejemplo, otro que me mata la cabeza es reformas leaning. Tiene una vaina llamada sorpresa, qué nivel de sorpresa quieres que haya o de curiosidad que tengan modelos increíble, eh? Porque vamos a aprender esto porque vamos a utilizar algo llamado Otto en Core que esto ya va por el camino degenerativo, pero es neuronales generativas, pero este es el que vamos a utilizar para encontrar las oportunidades en el mercado de carros Este sí, esto es una red neuronal. Aquí no se ve tan bien, pero cada cápita está básicamente cada columnita representa esta misma, pues este mismo diagrama así con cada celda Y eso, pero vamos a utilizar esto para poder encontrar los raritos los ra.
De Dell Data set y después aplicamos otro algoritmo para encontrar ordenarlos, básicamente en Ya encontramos los raritos ahora los ordenamos para saber cuáles son los raritos en precio básicamente eso es lo que queremos saber entonces por eso vamos a aprender redes neuronales listo créanme que se ve desafiante, pero es chévere Ya cuando lo entiendes está bien Sí chicos de mi familia bacano bacano, entonces nada chicos, nos vemos dentro de ocho, está estuvo.