Ahorita les hago el cómic Es que quiero hacer cómics más más detallados porque estoy haciendo cómics con nombres muy feos y así no están chevere es para que se entiendan los cambios, vale? Contexto contexto en sesiones anteriores, pues mismo ejercicio que hemos estado haciendo poder coger esta página que se llama Cars y hacer un scrap y sacar toda la información queremos pedir el precio de los carros Ya hicimos toda la parte de la extracción por el proceso hemos hecho varios ejercicios para poder entrenar diferentes algoritmos, cómo puede ser Random Forest catbus? Hemos utilizado dommel actualmente ambos ahorita ver redes neuronales con tensorflow y redes neuronales con python en la parte en la que vamos en el pre-process pues
Hicieron ciertas transformaciones para que tuviera un sentido bastante claro los los datos para que el modelo pudiera hacer una regresión bastante buena con el auto con el que mejor nos fue tuvimos un 98.55 de r. Cuadrado que es una de las métricas que se utiliza para medir que también lo está haciendo el modelo respecto a datos reales y a tus predichos entre más cerca estés al a la unidad o al 100% si lo ves como porcentaje mejor la idea era utilizar ese mismo ejercicio para poder construir redes neuronales, eh? Que es una de las de los algoritmos más potentes que que hay varias pero que es solamente para el ejercicio porque en Casos como estos cuando no tiene un un ejercicio no tan complejo basta con utilizar un dato de miel un cactus un algoritmo bastante bueno en regresión.
Y no haría falta extenderse tampoco hasta una red neuronal, porque ya una red neuronal digamos que es el típico dicho echo es una bazuca para matar una mosca. Entonces misma idea acá, simplemente lo hacemos como para entender cómo funciona Cuáles son los principios Pero ya cuando son relaciones más complejas O tengo muchos datos O tengo, eh? Un un trabajo más elaborado es cuando empieza a valer la pena utilizar este este tipo de algoritmos y igual estos algoritmos se utilizan para lo que esto lo lo que es generativo de hecho Déjenme por acá les les muestro en los libros que ustedes saben que es una de mis guías se utiliza para análisis de secuencia para todo lo que son de visión, por ejemplo, que muestran Cómo se utilizan las redes neuronales para visión para video cuando ya son ejercicios más complejos son los patrones que hay que identificar en el evento son más complejos este tipo.
Así que son muy bienvenidos, Vale entonces aquí lenguaje natural también incluso para reforma learning De hecho hay uno esté yo lo quería proponer porque yo lo vi me pareció una locura en este libro degenerativo, hay unos tipos de códigos que se llaman, eh Espero que estén tirando hay unos tipos de algoritmos que se llaman War models modelos del mundo es una forma de hacer que la red neuronal imaginé o el modelo imaginario y eso permite que pueda entrenarse incluso con pocos datos entonces para construir esto que está acá esta sección que se va acá, por ejemplo que es bae, que es variación de los autores se utiliza una red Entonces digamos que es la base para ya construir cosas muy bacanas y por ejemplo acá lo que se hace es imaginar un poco el camino para que el carro
Estrellas Lo que se está haciendo ahí cuando no se tiene suficientes datos o quieres entrenar el modelo para que siga mejorando su comportamiento Entonces si lo ven acá es como borroso pero es porque está haciendo ese ese modelamiento imaginativo digamos que puedo imaginar este tipo de cosas es la que se puede construir con redes neuronales por eso es que la idea es verlas entenderlas y extenderlo para ver hasta dónde llegamos pero para entenderlas Pues un ejercicio sencillo como este, eh Hasta ahí bien sí no sé si Joseph estabas por ahí tienes alguna duda o hasta ahí todo claro todo bien Sí muy bien Recuerden que silencio es bien entonces continuo y Recuerden que me puede interrumpir en cualquier momento Listo súper bien entonces para empezar con eso, eh? Este fin de semana tuve la oportunidad de estar en en en la
Con h en Medellín muy chévere cosas muy muy interesantes siento que tenemos talento para ser buenos acá en en Colombia de verdad que lo pienso y tuve la oportunidad de hablar con uno de los expositores que me hizo caer en cuenta de algo que yo había leído Pero había ignorado por un buen tiempo y es que Existen varios frameworks para construir redes neuronales principalmente y tres en lo que es tenso Flow Pero creo que es pythor y otro que es jazz investigando leyendo viendo Cuál era la mejor herramienta actualmente los modelos más avanzados de las empresas que están punteando están utilizando pyt, por ejemplo, enea y cosas así porque están quizás más enfocadas a a entender desde las bases el modelo y ahorita vamos a ver las diferencias que tiene Pero se utiliza un poco más para estudiantes.
investigadores lo que suele decir y el performance tiende a ser más alto, eso es también lo que se alcanzó a leer y lo que yo alcancé a investigar también leyendo un poco se decía que que tensorflow, por ejemplo respecto a python era más para procesos productivos, pero que por ejemplo las versiones viejas de python serán horribles que todo el mundo las odiaba y digo de tensorflow eran horribles tonos las odiaba y por eso Claro python, que a veces tiene problemas de compatibilidad y cosas así entonces hay como hates y al parecer se duda de su continuidad porque también esto es de Google y el mismo Google sacó jacks, que jacks está más enfocado en hyperformance como en en que ya cojas esa cpu y todas las operaciones están súper mega optimizadas y demás entonces dicen que lo va a reemplazar dicen que no
Mi opinión personal Es que esto va a seguir existiendo hay muchas empresas que lo usan y simplemente lo van a mejorar Pero quizás hay que tener herramientas más poderosas y la mayoría de papers y la mayoría de algoritmos avanzados que están por ejemplo en cooking Face que es uno de los de los portales la mayoría de modelos que están construidos sobre esta plataforma que se utiliza mucho para para es una comunidad de Machine learning también para ver modelos preconstruidos para ver papers para ver todo código fuente ejemplos etcétera la gran mayoría están construidos sobre python, entonces yo dije Creo que vale la pena también Mostrar esa herramienta y ya pues cada uno lo Escoge de pronto dependiendo de la necesidad o si quiere aprender más a profundidad y demás y también podemos medir un poco el performance que tiene uno.
Otro y cosas así entonces Esa fue la razón por la que me pareció útil agregar Entonces eso es como el intro de de esta sesión vamos a ver los ejecutándose un poco de código para entender cómo funciona Pero antes, eh? Para ya cerrar la base de entender cómo funciona el entrenamiento en la red neuronal Cómo se diseña y demás, pues quería como recapitular lo que lo que hemos aprendido, pero Incluso en este punto ya meterle Incluso un poco de matemáticas Incluso un poco entender porque funciona de esta manera y qué es lo que veo Cuando veo la fórmula en un libro y demás entonces, Eh Pues listo, la parte los tengo aquí me lo va a aceptar y la sesión anterior también habíamos visto este video que es de triple bro que es un es un canal de YouTube muy bueno para tener como cosas así un poco.
En matemáticas o en este caso redes neuronales y aquí te explicaron un poco Cómo funcionaba ese entrenamiento en este caso lo están haciendo para clasificación que es entre una imagen que la pasan en un vector por cada Pixel un píxel es un es un es un punto de entrada. Lo ingresan a una red neuronal, densamente conectada y las salidas son los números a los cuales representaría cada imagen La idea. Es que cuando por ejemplo aquí le meten un nueve en una imagen, pues esto me entregue un 9 en la salida y se hace la regresión para que toda la todos los pesos y todas las neuronas se ajusten para que cuando aparezca una imagen parecida 9 o él o en cualquier formato que esté un poquitico hacia arriba o en diferentes formatos sea capaz de generalizar lo suficientemente bien para que solamente se alumbre digamos el bombillito de Noé y todos los demás queden como en Seco eh Eso era lo que hemos visto Así como
Con la sesión anterior a que hacen como una explicación detallada, yo recomiendo que Vean este video para ya terminar de entender cómo funciona el párrafo y demás Pero entonces para entender cómo funciona y recapitular, pues en esta sesión, eh, La idea, es eh, empezar con lo que también ya hemos aprendido desde el principio de estas sesiones que era el ajuste que se genera a partir de El error o la de las la función de de coste, que se tiene que básicamente significa yo meto unas entradas al algoritmo, cuál sea en este caso red neuronal, eso me entrega unas salidas aleatorias y lo que yo hago es comparar estas salidas con respecto a los valores reales, que yo quiero calcular de hecho por aquí creo que es una operación también. Si no estoy mal en una de las imágenes, no?
Minutos pero hacia la resta en la misma parte se pasa a ver de pronto, ya lo pasé y no me di cuenta Bueno si algo después La buscamos bien, pero el vídeo se los recomiendo creo que tiene hecho varios, eh? Hablando de redes neuronales Ah De hecho creo que es en el que hace la en el mar propagation es el que explica en este lado aquí sí, esta es la imagen que está buscando aquí. Básicamente el mete la imagen dos, por ejemplo a mano derecha se ve los resultados que se esperan que son los Reales Aquí está el 2 y a mano izquierda son los que me entregó entonces Yo calculo ese costo en la función de pérdida que hay en este caso como es un ejercicio de clasificación se usaría una métrica diferente, por ejemplo Hay una que se llama Cross entropy, eh? Esa función
Para poder manejar clasificaciones y si no estoy mal, funciona como probabilidades y demás tendría que leerlo otra vez a detalle para para recordarlo, eh? Pero entonces lo que se busca es que eh, encontrar esa Esa diferencia que hay que podríamos interpretar como the construction que puede estar en diferentes formatos, hay unos para regresión como que les como les comentaba otro para clasificación binaria o múltiples clasificación y demás, eh? Incluso Hay unos que están hechos para calcular diferencias entre probabilidades, hay hay un montón muy interesantes para diferentes aplicaciones, pero se se ha calcular ese error para que se desea calcular el error Para yo poder después aplicar un ajuste hacia atrás y encontrar Cuál es el valor óptimo de esos hiper parámetros digo, Perdón de esos parámetros que me van a entregar la solución, es decir, voy a colocar aquí.
El tip de siempre es ver un poco cómo funciona con la recta pues para que se entienda más o menos una idea de cómo funcionaría Pues con las con los pesos, eh? La diferencia es que acá de pronto sería algo así un poco más, eh? Que nos aparece si aparece un kit que nos muestra como ese ya sería con Resortes creo que aquí en esta propiedad con el video, Sí se ve que es a partir de ese error que yo calculando voy haciendo un ajuste en cada una de las capas de las redes neuronal. Yo creo que aquí se ve De hecho, hay es justo lo que les comentaba la diferencia. Es que aquí que está haciendo él. Él está calculando el error que tendría para
Cada uno de los números que se que se que se están ingresando es decir, se cojen varios imágenes se cojen varios, eh? Errores y se saca el promedio esa es una forma de hacerlo entonces a pesar de que pronto para la imagen dos está neurona este o está este peso esté funcionando un poco mal para está funcionando bien, pero para la mayoría está funcionando mejor entonces de pronto. El ajuste no es tan drástico a pesar de que es solo una de ellas. Si tengo un error muy grande, porque la idea Recuerden que es generalizar se desea que es la red neuronal, pueda encontrar este dos, pero si yo lo pinto de otra manera un poco distinta también encuentra el CO2 y lo clasifique bien. Ese tipo de cosas son las que se desean hacer entonces se usa como un conjunto Grande de de de números 2 número 5 números 4 y demás, eh, Se entrena con ese paquete completo y así se van Ajustando cada uno cada una de las capas y cada uno de los pesos que hay, eh? Dentro de esa dentro de esta red neuronal.
Para nuestro caso es un un modelo regresivo queremos predecir el precio, eh? Las métricas que podemos usar para calcular ese error puede ser bien absolute error que Recuerden que si esta línea azul que simplemente calcula la diferencia que hay entre ese número real y ese número predicho y saca el valor absoluto, sí, básicamente te dice Ah estás equivocando en dos unidades, no me interesa si son dos unidades para arriba o dos unidades para abajo, estás equivocando en dos unidades en ese caso cuando tienes un un error muy grande de algún dato que te estás equivocando, en 20 unidades, eh, Son bastantes unidades, pero tiene menos sensibilidad cuando hay errores muy grandes, pero por ejemplo la que es MC que es mi Square Air esa es una una potencia Entonces sería la embarraste en 20 unidades.
Las el hecho te saco te saco el cuadrado Entonces sería 20 * 20, pero ese 400 no sé cuánto es, pero el error tiende a ser más grande cuando cuando hay, o sea, eh potencia, el error cuando hay diferencias más grande, entonces en algunos casos es más conveniente tener uno en nuestro caso es más común de tener otro cuando ya estamos entrenando, hay que tener también en cuenta algo que es, eh, a medida que vamos entrenando, se va generando este efecto de correlación de errores que viene siendo el vallas y el batts el vayas básicamente es un error, eh? Cuando el modelo no logra entender la complejidad del evento de una forma burda decirlo es el el modelo es un poco tonto y no entiende todavía las diferentes patrones que se encuentran allí, pero cuando también ya estoy entendiendo por
Por encima de lo que debería el patrón puedo crear otro tipo de error que es el baryons que es cuando yo creo overfield Entonces cuando tengo mucho, eh? Vayas is Under free cuando tengo mucho varias es powerfield, que es el ejemplo que les colocó siempre del carro como que te enseñaron siempre manejar estos 10 tipos de carros y los maneja este súper bien y nunca te dijeron que que te tenías que cambiar Y de pronto no ser tan estricto Entonces cuando te muestran un nuevo carro donde la palanca, cambia ligeramente porque ahora está un poco más a la derecha ahí, ya no sabes manejar el carro es porque ya estás sobreentrenamiento en este tipo de carro que ya conocías entonces queremos Buscar ese punto de equilibrio en el cual no tenga tanto vayas y no tenga tanto vayas a pesar de que haya un error que es insuperable siempre va a haber un error acá, que es esta línea que está aquí debajo que representa por toma de datos O algo parecido que no no logro bajar ese error. Ya viene como y y incluido los datos es imposible bajar, eh?
eso es lo que siempre estamos buscando y también nos damos cuenta que en las redes neuronales esto se podía definir de esta manera que es a medida de que yo voy calculando el error y empiezo a hacer este background que es cálculo cálculo del error, en este batch, que es un conjunto de datos de muestras que yo uso, eh, En vez de entrenar todo el dataset, porque recuerden Yo podría entrenar con todo el dataset Pero eso puede computacionalmente ser muy caro y puede demorarse mucho Entonces yo lo que hago es coger muestras, digamos, cojo los primeros días números y cálculo el error en promedio Sí y empiezo a calcular la diferencia que tienen, eh, el real con el predicho empieza a ajustar en la primera capa la primera capa va Ajustando a la siguiente capa la siguiente capa a partir de algo que se llama el gradiente porque estamos en un landscape donde la
Es el landscape, básicamente me representa el error que tengo en mi en mi entrenamiento, por ejemplo esta imagen creo que lo que podemos ampliar le muestra bastante bien y es, eh? Para los valores de los diferentes parámetros en este caso, cada metro representa un peso dentro de la red neuronal, eh? Yo voy teniendo Pues un valor definido de error La idea, es encontrar esa configuración de esos parámetros que me va a entregar el menor error posible donde yo ya esté en el se llama globel mini en el en el que el error que es el eje que ven, eh, Lost fonction, que aparece acá si se dan cuenta donde el error para la mayoría mayoría de los casos sea lo más pequeño posible, entonces en búsqueda de hacer eso podía suceder que eh, de nuevo yo sobre entrenar y empezar.
Ha hecho a ser muy bueno en el trading con los datos del trading, pero empezar a fallar con los datos de validación, que es el mismo ejercicio que les decía ahorita como el carro, me aprendí estos 10 carros súper bien Ya sé dónde está la palanca, no tengo ni que mirar, pero apenas me tiran un carro medianamente diferente lambar no sé manejarlo porque la palanca Tenía que estar aquí a 5 cm y no la encuentro, entonces La idea. Es que se encuentre en ese punto que no sé sobre ajuste y que pueda generalizar lo suficientemente bien, vale? Aquí hay unas prácticas que se pueden utilizar para evitar ser perfil que también en esta sesión de pronto no lo vemos para la siguiente, sí que son el término, se llama normalización, dónde buscas como un poco ajustar o limitar el modelo para que evite s sobre ajuste y que el entrenamiento.
Lo más óptimo posible entonces a veces hay algunas técnicas como escalar dependiendo del algoritmo algunas técnicas también como agregar un factor adicional en el error que es el l1 y L2 que hemos entendido la vez pasada que el l1 y el L2 eh Esta es la representación matemática, pero yo creo que así es confusa esta imagen que está acá yo a veces no la terminé de entender, pero teóricamente si lo lo llegué a comprender que es básicamente el error el añadido de error, que yo le agrego es dependiente de los parámetros del modelo en este caso de los pesos, entonces si los pesos son muy grandes ese factor de terror, tiende también a ser grande, entonces al momento de hacer el ajuste de la red neuronal no solamente va a ajustar para corregir el valor que es es el valor real versus el valor predicho.
Si no adicionalmente depende de que los pesos sean pequeños entonces va a buscar que los valores de los pesos en la red neuronal también tienden a ser pequeños, eso evita que haya un lobo tan tan fuerte tradicionalmente también de estabilidad al entrenamiento y dependiendo de el factor de regulación o normalización que escojas, eh? Te va a permitir borrar o darle menos peso a los datos irrelevantes es también tiene varios factores adicionales darle menos pesos o al menos minimizarlos para que no tenga esa ese factor de Explorer que también veíamos, eh? La sesión anterior Cuando tenías una red global, te puede pasar que tienes dos caminos Sí ya la red neuronal es muy grande, compleja, que vendría siendo a ver si lo ponemos por aquí, creo que era por aquí sí era que tenías saturación en en el post function en la función de coste.
Se saturada por arriba por abajo y empezaba el se puede decir la red neuronal a morir empezó a morir porque ya a partir de que iba traspasados en las capas ya no sé, ya no se veía un valor de cambio en la función de activación, sino siempre me entregaba el mismo 0 en este caso siempre me tiré al cero el cero cero o me tira al uno uno uno uno uno Eso sería exploit uno, eh? Cero ya sería que está Bang está muriendo. Digamos Esa esa red neuronal este tipo de de prácticas Cómo aplicar, el l1, L2 evita, que haya eso, pero eh? Solamente lo informo porque en el futuro se podría utilizar en este ejercicio todavía no, no se ve la necesidad si ya usan algún modelo un poco más grande, Ojalá de verdad, lo podamos hacer y ver ese tipo de retos en el futuro sería interesante sería interesante Buscar un problema que tuviera ese tipo de de tareas algo adicional, que se me pasó aquí en el funcionamiento de la red neuronal es, eh? Un poco otra vez explicar su
Desde las bases creo que esto debería Haberlo dicho al principio, pero creo que no está de más volver a recordar cómo funcionaba era, pues le va imperfecta que lo escribo, eh? Tengo unas entradas y tengo unos pesos esas entradas, eh? Creo que colocamos el ejercicio la sesión anterior de que Imagínese que quiero decir el precio y un tic de una boleta y tengo el número de personas que hay que es x1, la distancia a la que estamos que es x2 Y de pronto, eh? El disco un Bono de descuento que nos dan es X3 entonces con con estos diferentes datos y con el precio, eh? La red neuronal se empieza antes nos da la siguiente manera el x1 se multiplica por un peso que se llama, eh, wait One wait to White Take a1 de las entradas tiene un peso diferente y se multiplica por ese peso específicamente, eh? Aquí se suman.
Asustaste este icono de sumatoria y luego se hace se hacen pasar esa suma por un una función que se llamaba función de activación Por qué se utiliza la función de alteración? Porque si yo solamente dejo las sumas como aparece aquí que por ejemplo yo podría decir. Ah, somos 10 personas y eso tiene una relevancia de 5 y o de 0.5 Y de pronto Acá tengo en este dato Estamos a 10 km eso tiene un peso de 0.2 es decir no es tan importante y el disco de pronto es un poco más importante que la distancia Entonces estamos a 5 a el disco que tenemos ahorita es del 10% Y eso pesa 0.13 y cuando yo multiplico y sumo eso eso me entregaré, me entregaría a mí un valor un valor particular Y eso podría ser la salida el problema con ese tipo de patrones. Es que si ustedes se dan cuenta haciendo la fórmula, sí, solamente estoy haciendo multiplicaciones y sumas Pero en ningún momento tengo la capacidad de
Una regresión de una curva si él si el el evento llega a ser más complejo si no hay siempre relaciones lineales, Cómo sucede la mayoría de problemas en el mundo. Él él solo multiplicar los pesos y sumarlos quedaría siendo insuficiente, por eso es que se agrega este función que se llama función de activación que pues ahorita la recordé porque justo estábamos hablando de la función de activación por la saturación tanto arriba como abajo cuando ya tienes esa idea, pues Recuerden que también había un factor adicional que era el bayas que ese término es como un comodín se llama offset en realidad y es muy parecido a Cómo funciona la recta ahorita que estamos viendo la regresión de la recta, creo que los r sí pero entonces sí sería algo así, por ejemplo acá en esta imagen.
La ecuación de la recta que conocemos de toda la vida es y = m * x + c. Si se dan cuenta m se multiplica por un valor de entrada que es x Sí pero c siempre que hay de Independiente ese no se multiplica por ninguna otra en este caso lo utilizamos como un comodín que básicamente c es el que mueve la línea en el eje vertical Sí pero es un como es muy parecida a la idea a aquí se utiliza esa ese factor de varias como un comodín que también se ajusta con un peso definido conectado con todas las neuronas para poder tener pues ese, eh? Esa libertad y y ese ajuste si hace falta así funciona luego cuando ya tengo la idea de neurona pues lo extiendo a diferentes conexiones es decir diferentes neuronas e incluso a diferentes capas que es cuando ya le meto una capa dos capas y es aquí donde empieza a funcionar el haydn, eh, eh? Los highline layers que son
Las capas ocultas Aquí voy de hecho aprendí algo que no De hecho nunca lo había visto de esta manera y son las cosas que me que me gusta de pronto también estar en el grupo de estudio porque me toca estudiarlo y y encuentro cosas muy chéveres y es yo me acuerdo que ya en la universidad en el colegio yo he estudiado esto de eh shein boole, la regla de la cadena que había una fórmula matemática que era Mira si tienes una función que está dentro está dentro de una función pues entonces, eh? Aplicas esta transformación que es equivalente si quieres calcular una derivada Por ejemplo, si quiero calcular la derivada de de y respecto a x cierto, entonces hago una transformación que es sacar la derivada de adentro y luego multiplicarlo por la derivada de fuera y y por la función que está dentro porque esto me pareció Genial porque cuando Ahorita estaba leyendo y recordando Cómo funcionaba el back propagation.
Justo lo que pasa, déjame colocó aquí otra vez el libro y colocamos una red aquí neuronal es justamente lo que pasa en una red y ahí fue donde me pareció genial, Qué pasa No no no no, no, no, solamente es explicarlo, No señor, solamente es es dar introducción de cómo funciona Julián no no, no es Es solamente introducción porque estoy explicando todo esto ya van a ver porque cuando estemos utilizando python, se van a dar cuenta, Qué pasa esta esta capa de acá es una función que está dentro de esta capa de acá, que es otra función. Entonces tengo una función dentro de una función y ahí se aplica la regla de la cadena cuando se está haciendo Back propagation, como yo estoy calculando la derivada, que es el cambio entre la entrada y la salida la la digo Perdón la entrada predicha, la entrada la salida pericial la salida real cálculo el error.
Cálculo un gradiente el gradiente se aplica la primera función y la forma en la que se propaga es con la con la sheinfeld, calculo El garante de acá, cómo está? Es una función que está dentro de la siguiente capa pues propagando de esa manera eso es cómo funciona y ya era lo que les quería explicar ya como para asentar las bases matemáticas porque las sesiones anteriores será como un poco al aire un poco explicarlo, pero creo que ya con esta descripción podemos entender cómo es que funciona ese background y por qué puede hacer un ajuste de esa web Ahora sí entrando en materia voy para el código y le echamos uno entonces hay dos hay dos elementos nuevos ahora el primer elemento es el que hemos visto la la la sesión anterior que era, eh? Tensorflow. Habíamos entendido Cómo funcionaba crearlo era muy High Eleven era muy fácil de entender se han iniciado el modelo con seconds se inicializa las diferentes capas en este caso la capa uno
Era el input, luego teníamos capaz, densamente conectadas, que es, básicamente lo que acabamos de ver en el libro con un número de opinión neuronas y un una función de activación que es un hiper parámetro la razón por la que está está Reno Ronald va bajando progresivamente hasta llegar a una neurona es porque queremos capturar los patrones que nos pueden decir a partir de las entradas de los carros ir bajando hasta cuál cuáles son esas relaciones que me dice Cuál es el precio al final adicionalmente la razón por la que no aparece aquí una un una activación una función de activación es porque deseamos que la función de activación en este caso. Sí sea lineal, que no esté constreñida a a que tenga curvas, sino que ya o a un Rango específico, sino que ya pueda tener un Rango amplio y continuo, esa es la razón por la que esta última capa no tiene función de activación y funciona en este.
Regresivo si están haciendo clasificación y demás aquí sí que se usa una función de activación diferente que quizás trabaja con probabilidades o o con la métrica que ustedes estén utilizando, ahí tengo que hacer un poco de investigación adicionalmente habíamos visto Cuál es la métrica que vamos a usar para calcular que esté, básicamente es el MC era lo que decíamos, eh? Silva eleva al cuadrado, Sí calculó el absoluto, si alguna combinación hay muchas funciones de activación que tenemos aquí y adicionalmente definimos el optimizador el optimizador aquí otra vez viendo el gif, este gif es muy chévere, Eh optimizer, hay diferentes, eh? Activadores que Superman había un Este es el gif que nos gusta ver entonces hay algunos.
De hecho Alejandro El el en la sesión anterior nos dejó un video Lomas de bacano en lo en dónde entendíamos Cómo funcionaba esa ese optimizador que es la forma en la que podemos bajar ese landscape y encontrar el el mínimo error funcionan más rápido según los que otros funcionan incluso con inercia, con eh, momentum. Digamos que ya vas en cierta dirección y no no es que solo calcule si tengas consideración y gradiente en la posición actual en la que estás sino también tienes en consideración el gradiente anterior y el impulso digamos que tenías desde atrás y de esa manera, puedes bajar más rápidamente o incluso también puedes Modificar el el learning rate que es como básicamente la longitud de los pasos que estás dando Entonces si ya estoy muy en picado, pues doy pasos más largos y así mi entrenamiento Mejora y va más rápido ese tipo de cosas se utiliza con este optimizador también es un hiper parámetro en algunas cosas funciona uno mejor que otro Hay unos que ya son como estándar y funcionan bastante bien. Este es uno de ellos ahora cómo se entrenaba que
Aquí ya empezamos a ver un poco más, eh? El detalle del entrenamiento el método que estamos usando se llama fit, vale? Tiene como entradas los fichos que son las características que tiene mi modelo el target esto es justamente exactamente como lo hemos visto Incluso en catbus en los demás algoritmos Cuando hacemos un clic acá tenemos el, eh, los features y aquí tenemos el dato misma idea es exactamente la misma idea cómo funciona tenemos el número de ports, el número de pots es como el número de iteraciones, el número de de de de cálculos que yo voy haciendo para poder ajustar, entonces llegó cálculo ajusto. Vuelvo a calcular ajusto. Eso es un deporte 12 * 8 13 * 8 y eso también se se se regula para que evitemos también el overfield, que ya vamos a ver por qué hacemos también, eh? El el ingreso de otros argumentos tenemos el big box, que esto es básicamente informativo Esto me va imprimiendo en consola. Básicamente con detalle Que tanto, se está entrenando y adicionalmente aquí lo vamos a lavar la
Tenemos el tamaño del batch, el tamaño del batch me indica básicamente Cuántos cuántos en vez de usar todo el dataset uso 32 en este caso 32 elementos dentro de ese ejemplar grande que tengo y sobre esos cálculo el gradiente listo, eso es una muestra que a pesar de que no vaya a tener la precisión exacta exacta en la dirección. Voy a ganar velocidad. Hay incluso habíamos visto en el libro también, eh? Cómo se comportaban, eh? Hay uno que se llama estocasticos que andaba por todas partes, pero nunca muy difícilmente llegadas rápido al punto mínimo Si usas, una muestra que es el verde, Pues el camino Era también un poco borracho Pero tenías una dirección un poco más más definida.
Y su southbox que es usar el el dataset completo, pues sabías Exactamente para dónde tenías que ir, pero para calcularlo, te demoras 10 veces más el triple entonces, eh? Es la misma idea que se maneja acá el batch también es un hiper parámetro dependiendo de qué tanto sea tu grata sed y y la complejidad del problema Pues utilizas un batch más grande o más pequeño em y adicionalmente aquí agregamos algo que era un callback, también lo hemos visto, eh? La sesión anterior que es para evitar que haya feeling Entonces cómo yo puedo encontrar la diferencia entre el Data set use de validación y de entrenamiento Si ya se empiezan a desviar el de entrenamiento, sigue bajando, pero el de validación empieza a fallar, que en términos prácticos Eso significa estoy empezando a aprender demasiado a manejar estos carros, pero los carros nuevos Les estoy dando, vale Ya estoy empezando a fallar porque me estoy especializando mucho en estos casos antes de que me especialice demasiado hago una pausa y digo, ya creo que ya entrené lo suficientemente.
Está bien antes de tirarme los demás, Pues prefiero para darle el entrenamiento aquí es exactamente la misma idea aquí, Tú puedes definir un parámetro que puede ser la paciencia o también, eh? Patience o también hay otro que es, eh? La divergencia o la diferencia que hay entre el training y el el de validación, si esa diferencia empieza a creer crecer en cierto porcentaje, Pues también lo puedes parar en vez de en vez de esperar que falle, eh? En cada epoch diga como que Ah fallaste una vez y por acá, yo le puedo decir Juan Sí ya fallas 10 veces sí, ya la diferencia entre los epoch, eh? Es de 10 veces, pues hay patas, pero también puedo definir un porcentaje en la diferencia. Eso podría pasar también y adicionalmente puedo también definir Qué tan grande, va a ser el dataset de validación con respecto al de entrenamiento usualmente se maneja un 20 o 15 o un 10% dependiendo de tu tamaño del dataset, eh? Acá también nos comunica el parámetros.
Simplemente se usa 20% o 15% casi se dan cuenta, se está usando 20% finalmente se evalúa el modelo para saber que está funcionando bien. Sacamos las métricas que ya conocíamos desde modelos anteriores r. Cuadrado, eh? El mini Square error, el mina absoluta, error, que básicamente nos dicen un poco que también lo está haciendo el modelo aquí adicionalmente estoy haciendo un un plot de El histograma, que es cómo va bajando a partir de los spots Cómo va bajando ese Rock vale Y si todavía hay oportunidad de que baje más y, eh, De hecho déjenmelo. Lo voy corriendo porque se acaba la explicación y después no hemos dado mientras mientras que esto sucede les termina la explicación, voy corriendo, eh? Adicionalmente aquí se está colocando em se está aplicando la predicción para el dataset que se usó de de entrenamiento, simplemente en términos informativos para poder yo.
Pero yo era esto en un Excel de pronto, los números me sirven pero el Excel también podría informar un poco que también lo está haciendo si yo comparo el valor real con el valor prediche, digo Ah lo están matando mucho, etcétera, etcétera, eso también puede funcionar y finalmente guardo resultados y guardo el mueble es así de sencillo, vale? Mientras Se entrena datos a Resaltar De hecho aquí ya tengo un plot, que es el que me gustaría mostrarles este plot is the el defensor Flow Entonces tenemos que colocarle título y demás Pero básicamente el eje y es el error y este eje de acá, que es el, eh. El X viene siendo el número de Porsche Entonces se dice acá Yo alcancé a hacer 10 y pues no estoy seguro. Si está escala está alto. Sí te quería mirar de pronto los números También me están fallando, tengo que ponerle números en el eje, porque ah, no Mientras Sí está bien porque mira aquí está elevado a la 8 entonces.
Sí sí, está bien el Derrotado ya es muy grande, pero si se dan cuenta todavía hay una tendencia bajista que significa que yo todavía podrían tenerlo más y podría funcionar más y aquí alcanzamos a ver una de las desventajas comparativas para este tipo de problemas que tiene la red neuronal, la red neuronal se demora mucho más en entrenarse que un que un algoritmo, por ejemplo el cactus o el Random Forest se demora muchísimo más en entrenarse Entonces eso podría ser un problema, si el problema se puede solucionar pues de una forma más sencilla, como les decía coges una basura y matas una mosca. Quizás no quizás quieres hacerlo con otra cosa que funcione igual de bien incluso mejor y sea más barato, Qué sucede, qué está pasando aquí en la parte inferior lo que está pasando? Es que por cada uno de los spots. Él está calculando gradiente bajo paga a ajustar los pesos, Gracias por no ajustar los pesos y aquí te tira el error en los que va generando por
El el Data set de trenes y este es el dato se te validation entonces aquí podríamos tener nosotros, eh? Ahorita estoy bloqueando una curva, pero se podría plotear las dos para ver si está generando perfil sí, ambas están bajando significa que todavía tengo oportunidad de seguir descendiendo si la devaluación empieza a subir y está sigue bajando ahí empezó a tener perfil Pero eso Posiblemente no pase Porque sigo teniendo aquí en el Stop early, el talkback que me permite poner ese freno de mano digamos un poco y acá lo que nosotros podríamos hacer es coger calculadora muy sencillo y está escondiendo aquí atrás aquí lo que yo podría hacer es coger este dato y Recuerden que si yo le sacó el cuadrado al MC yo tengo otra métrica, que se llama rmc, esa Porque es importante porque
La puede entender a alguien de negocio significa que yo le sacó el cuadrado de esto y llegó, hay una varianza en los precios que está apareciendo, hay una varianza de 7927 para arriba y para abajo, eso todavía es un error muy grande, porque el mejor resultado que tuvimos era como 2500 dólares por lo que tú habilidad Seguramente se puede bajar más acá, por ejemplo, si copio esto lo pego acá y vuelvo a sacar el mismo cuadrado, ya está bajando 7000 Y así va bajando una idea que yo vaya Ajustando pues va bajando va bajando. Va bajando hasta que encuentra como esa convergencia es el nombre Eh va bajando bajando bajando y adicionalmente hay algo que querías Mostrarme y es esto el como él el costo computacional que está teniendo esto aquí, por ejemplo, utilizando python Y utilizando tensorflow, digamos que esto está usando cuando tú 20% quizás si sumamos este y el de Pay charm tengamos 30 estando.
Tenemos muy, eh, Muy optimistas, Entonces es eso es algo a tener en cuenta también hay algo adicional que también me gustaría comentarles y es Había algo, déjame les muestro acá en la documentación algo particular que tienen este tipo de algoritmo. Es que no solo son dependientes de las entradas y de las salidas, sino que incluso también son dependientes de el estado inicial es algo particular que tienen las redes neuronales y por eso se usaba un algoritmo de inicialización de pesos que se llamaba como murloc O algo parecido. Déjame ver si si está por aquí en valores perfectos o no sé si eran el módulo donde recibe.
Creo que son perdí exactamente cuando se hacen sin fin hablar de ese tema de una forma de de mejorar el performance de los modelos es a partir de la inicialización de los pesos, pero si algo se los quedó bien, va a estar cloro Angie Sensation eso es simplemente quizás un tiempo el otro nombre, pero eso es un valor, que ya viene por defecto que tienen las redes neuronales de tensorflow, es una es un valor que viene por defecto, pero es una es un algoritmo que se inventó un Man Hace mucho tiempo que busca los mejores valores básicos que sean pequeños en los pesos.
Para que cuando yo esté entrenando mi modelo pueda llegar a la convergencia más fácilmente esa es una cosa particular que tiene este tipo de algoritmos, pero para tensorflow, pues no llega a ser un problema Entonces Mira acá por ejemplo, eh, Ya habrá terminado el entrenamiento acá quizás se podría decir que podría yo llegar un poco más abajo, si yo cojo este valor que es lms y le sacó aquí la raíz cuadrada, llegamos a 6700 y pico no está mal Quizás podríamos bajar mucho más con más spots, Por ejemplo, si le agregó más spots porque no en ningún momento hubo como ese Ernesto stop, eh? Funcionó bastante bien algo adicional que también, eh? Sucede acá es en el optimizador estos también Vienen con un valor por defecto de del rate Entonces eso también lo podríamos haber incluso para que de pronto la convergencia llegue más rápido porque si le colocó de pronto un Let me Race más grande más pequeño, eso podría mejorarlo, eh? Pero
Nada más tengo 0.91 r. Cuadrado O sea 91% no está tan lejos de lo que de lo que hemos tenido y recuerda que son datos reales Esto es lo que más se ve poner más contento se puede mejorar. Sí se puede mejorar, se puede hacer fine toning con diferentes tipos de estrategias, si no, no habría problema de hacerlo todavía hay oportunidad de crecimiento con esto y hasta ahí quedaría tensor se entiende, creo que la idea y y funciona bastante bien Ahora quisiera saltar a la parte de python, que es aquí donde empieza a cambiar un poco la cosa Se sigue un poco, se sigue exactamente la misma idea los principios son los mismos, tengo la red neuronal, densamente. Tengo funciones de activación. Tengo el mismo cost funcion. Tengo un optimizador. Tengo un background todo. Eso se hace, tengo eps, pero hay diferencias en la forma en la que se escribe se escribe con más.
Detalle se llegan a colocar elementos, eh? Incluso el epoch no funciona de forma automática, sino que hacer hay que hacer un Ford por ejemplo, pero se puede customizar mucho más y se puede incluso hacer un Debut más personalizado y superformance crece que ya vamos a ver, eh? Justo eso cómo funcionaría Entonces acá mi misma idea, eh? Lo que se está haciendo aquí en esta línea 25 es el mismo split que se hace aquí en tensorflow cuando yo le colocó la división de El 20% es exactamente la misma idea que se está haciendo acá, simplemente que digo que el training el training size va a ser del 80% porque en este caso estoy definiendo el tamaño de 30 Entonces el 20% restante vendría siendo para el de validación. Se transforman que es aquí donde se empiezan a ver las mejoras en performance se transforman en algo que se llama tenso, eh? Que python se entiende y que tiene un tipo de dato que es flat 32 porque
Es porque eso hace que el el Speed del entrenamiento sea más rápido a estos términos de cpu a términos de gpu, sí lo están usando ese tipo de definir Cuál es el dato que me estás tirando Y qué tan qué tan largo es ese dato que me lo digas de antemano Eso hace que yo pueda meter el acelerador más duro porque ya sé lo que va a esperar entonces, por eso se hace este Rut y se hace como ese esa transformación a a que sea un tenso, ahora está la misma idea está aquí en esta línea, eh, 35 que es donde estamos construyendo la red neuronal es exactamente lo mismo que estamos haciendo acá, que es le metemos una entrada, le metemos una densamente conectada con este número de unidades que son las neuronas que hay por cada una de las capas. Aquí funciona Exactamente igual Solo que a más detalle Entonces por ejemplo acá yo definiendo esta línea vendría siendo input, esa input va a tener 50 unidades, tengo luego.
La función de activación si se dan cuenta ya no está dentro de la misma capa línea, sino que funciona como una capa adicional en el término de Cómo se escribe luego creo otra capa densamente, eh? Conectada eso sería el equivalente a dence acá solo que se Dance se podría traducir en estas dos, porque aquí en la tensión le puedo tirar el el la función de activación y así voy bajando si se dan cuenta tengo que Definir la entrada y tengo que defender la salida este tipo de cosas permiten que tú ya hagas arquitecturas más complejas que Que pegues por acá, por ejemplo, yo yo me atrevería a decir que por acá tiraron los Transformers es eso que se utiliza ahorita para para chachi pity incluso los algoritmos de atención listo unit que es el algoritmo de atención que se hace para los lms para que pueda tener esta consistencia con con la información pasada ese tipo de cosas tan detalladas y complejas creo que
Te las permite hacer python, ese sería un poco la diferencia y en performance al parecer, Suele suceder. Sí sí sí sí, mira qué mira que justamente justamente en mi investigación que yo hice de, eh? De quién usaba python's lo usa Google y yo dije Bueno en qué lo habrán usado, pues hasta ese detalle, No sé no trabajo Google pero sí sé que leí que los hago sí, eso sí, sí, sí, sí seguro que lo leí yo dije Bueno raro, que teniendo su propio framework, pues usarán python, pero como te digo es porque creo que python ya desde la base, lo construye, no para que sea un poco más hyperformance lo usan los investigadores, eso ya te hace decidir Eh bueno alto rendimiento es y y te va a funcionar muy bien acá el mismo comentario que les comentaba en la última en la última capa Esta última capa. Si se dan cuenta no tiene un relé.
Adicional es porque queremos que tenga esa salida continua porque tenemos que hacer una regresión que no esté limitada en un Rango y que dependa básicamente de los últimos pesos y las entradas que tiene la última capa es decir que tiene esta capa a partir de todo lo que ya se sintetizó desde atrás sí, entonces, eh? Vendría por ese lado acá, Qué significa defino la función la función, eh? De construcción la función de de costo de error, que yo estoy utilizando en este caso estamos utilizando Exactamente La misma que es mini Square se escribe un poco diferente Pero la misma es la misma idea se define un optimizador aquí, lo utilizamos como un texto porque eh, tensorflow te permite ya tirar el nombre y él tiene unos valores por defecto que te asigna Pero tú podrías como yo les comentaba la sesión anterior crear un objeto, por ejemplo que se llame que se llama así y esto tendrá, eh?
Tensorflow un objeto definido al cual yo le puedo mandar el link radio los parámetros que yo quiera y eso pues le va a funcionará como yo quiera en este caso, por ejemplo, si se dan cuenta yo le tiró, estos son los parámetros Así es como funciona la sintaxis Esto sí es un poco más de Cómo funciona la sintaxis de de pyt. Yo le tiró los parámetros que básicamente vendrían siendo todos los pesos de la red Sí y le digo este va a ser tu landing rate que este va a ser qué tanto pues va a ir caminando y bajando slam Entonces tienes el optimizador ahora qué haces defines los post defines El bac size en este caso lo vamos a tirar un poco más alto a 32 otra vez este ya me sobra Perdón esto no Debería ser estar ahí y yo lo acabo de borrar y me lo puedo tirar dónde estás tú crees que hice un cambio de esos de último momento que me interesa esta canción Yo creo que no lo reemplace.
Ahora sí, funciona bien, eh? Tengo los spots en este caso tengo el bac size, es el tamaño otra vez tomo una muestra se hace esta transformación, eh? A un objeto que se llama tensor dataset para poder aplicar este Patch eso es lo que básicamente se hace acá, se se se une tanto el train, eh? Los switchers como los targets se unen como una tabla es lo que se puede interpretar y eh? Acá se cargan en un objeto que ya yo le puedo definir Ah listo, vas cogiendo esta tabla, pues muestras de 32 me lo va segmentando un poco y este shuffle es Vasco aleatoriamente que es donde también se viene ese principio de en en en en modelos de Inteligencia artificial de Machine learning, no son como los modelos convencionales que tú le picas al botón y siempre va a pasar lo mismo.
Va a llegar a abc abc a veces no, en este caso puede que por los datos que escoges tengas más error menos error por la idea es llegar a que no sea tan variante, sino que tenga ese punto de convergencia y por eso se remueven como los outliers, por eso se hace como ese tipo de limpieza en el precoces todo eso tiene esa trascendencia que de que como es también datos aleatorios, pues esperas que para todos pueda generalizar lo suficientemente bien siguiente paso, eh? Defines las métricas unas métricas, que ya vamos a ver que vendría siendo, eh? El mejor MS es decir, el mejor error que hayas podido lograr y los pesos que tendría el modelo que son los que tú quieres calcular los mejores pesos después de hacer todas las regresión Sí y este histori, básicamente es lo mismo que hemos intenso Flow que es hacer el history la línea que esto ya te lo entrega después del Fit pero es la línea de descendencia de calcular el Lost fonction.
Es lo que queremos hacer acá, era tan sencillo como definir los spots y ya acá suele ser un poco más complejo, pero creo que tiene sus beneficios porque por cada epoch tú podrías definir de pronto un callback más personalizado, podrías hacer algo más complejo algo más completo, eh? Dependiendo las necesidades Supongo que por eso es que lo prefieren los los investigadores, pero ya vamos a ver otra razón por la que yo también diría Es una muy buena razón para preferir este sobre el otro acá, qué se hace por cada uno de los spots se este método lo que hace es básicamente preparar el modelo porque vamos a hacerle entrenamiento porque sucede esto de que yo preparo el modelo para entrenamiento y por ejemplo que hay uno es para evaluación es porque cuando usas una una red neuronal algunas de las capas se utilizan, por ejemplo para solo entrenamiento para que no tenga overfield.
Que eh sea más experts, eh? Sea más distribuido el conocimiento que la que lo que yo les mostraba por ejemplo, eh? Por aquí hay una práctica que se usa como de Drop out, que es básicamente apagas y prendes unas neuronas, eso está por acá Aquí está ropet apagas y prendes unas neuronas en cada uno de los spots para que no para que el conocimiento se Esparza digamos entre todas las neuronas, sino que eso lo ha gustado eso de hecho funciona mucho para el, eh, la anomalía detección que yo les iba a comentar, que ya lo lo aplazamos Funciona muy bien para eso, porque ya lo veremos porque tiene sus beneficios Pero entonces, ese tipo de cosas quieres que solamente sucedan del entrenamiento cuando le evalúas esas capas se desactivan no deberían estar ahí, Vale entonces, por eso hacemos ese, está transformación de que listo listo vamos, es a entrenar en este momento ahora en este foro por cada uno de los baches Que yo habré escogido por cada uno de sus conjuntos, que ya han escogido.
Eh, aquí, simplemente en el optimizado se están colocando todos los gradientes en cero para que no tenga como un Eh sí como como que esté pegado con con las iteraciones anteriores, aquí se realiza este serial equivalente al fit que tenemos en en tensorflow, cuando ya aplicas el modelo y el modelo lo estiras Pues el batch de esos fichas. Esto te entrega la salida a partir de eso calculas, el los aplicas, el Back el Back propagation con el optimizador que tienes y te mueves el optimizador se muere en esa posición. Sí, calculas Cuál es el el el garante y te mueves en esa posición la optimizador empieza a mover y aplicar todo el dato para ver haces eso con todos los Bots que hayas definido acá por todo este coche que hayas defendido y luego, eh? Aquí este sería el equivalente a hacer la validación con el dataset de validación, que acá que hay acá Pero que acá simplemente lo tenemos como un argumento de entrada Aquí ahora le digo listo a partir.
De la justicia que existe arriba pueda evaluar, entonces, cómo te voy a evaluar, eh? Pásate a modo de evaluación aplicó la misma predicción aplico, eh? Calculo la diferencia calculo el error. Aquí simplemente Se imprime el error, pues para ver qué rayos sucede se agrega s eso al histórico, porque quiero Trazar la línea por cada uno de los spots y aquí hago una verificación. Esto es muy parecido al al stop r que aparece acá, sí, solo que aquí le colocamos paciencia de uno, si si no llegas a bajar Es decir si en la siguiente iteración pudiste mejorar sí Sino para aquí el page yo le podría decir si fallaste de pronto en dos o tres, sí, pero si ya fallaste más de tres para ahí, porque ya me estás empezando a generar ese overfield, en este caso para que ambos tuvieran el mismo comportamiento decidí Poner este valor en uno para que pudiéramos entender.
Cómo esta condición funcionado que era si el que tenías previamente que era el mejor no es mejor O sea no llega a ser mejor el el nuevo entonces para porque ya se supone que no podría mejorar, pero yo aquí podría colocarle más condicionales hacerlo un poco más elaborado, qué es lo que permite como python Entonces si llegas a ser el mejor pasan dos cosas reemplazo el mejor valor porque ya sé que el valor nuevo que tengo es mejor y cojo los pesos en esa configuración del modelo que funcionaron mejor y hay que hay que así de sencillo funciona Sí cuando Ya termina toda la iteración Pues yo ya sé que ya tengo un número unos números definidos tanto para el MC como para es para los pesos aquí imprimo Ah bueno inicializo el modelo con esos mejores pesos para poder remover con los pesos que son, eh? Imprimo los valores que son los los mejores errores que tuve
Aquí estoy haciendo el mismo plot exactamente el mismo plot que se hizo allí en en en en tensorflow para poder tener esa traza Sí y aquí adicionalmente pues estoy haciendo Exactamente lo mismo que también hicimos acá, que es crear como esta tabla informativa que me permite a mí ya coger esta tabla de coger los valores con el Data set de trading los valores que predijo con los valores reales y de esa manera un poco más informativo, Sí aquí lo último que se está haciendo es, eh? Se hace otra vez la evaluación con el dataset de de validación. Esto con el objetivo de construir este mismo este mismo objeto que está acá, que eso pues en realidad esto también lo podríamos coger incluso de acá no habría problema pero pues para que se mantenga la misma estructura lo lo copié dos veces aquí se agrega también la predicción se termina de completar el archivo de de predicción porque acá simplemente se inicializo para el price pero aquí Quiero la predicción del Price y aquí.
y salta frases completas para tener la tabla completa se guarda se guarda ahora por qué no lo ejecute ahorita porque no lo ejecuté ahorita que estábamos hablando porque sucede lo siguiente esta librería ya ahora sí, de verdad que bien optimizado para funcionar con cpu y con gpu ahorita que tiramos en el tensorflow capaz que nature entender un poco más, porque yo lo he intentado varias maneras, pero no he podido sacarle el 100% del potencial a mi gpu, pero en este caso sí se puede y muy fácilmente que es aquí donde uno dice si de verdad me o sea, puede que me demoré un poco más escribiendo el código pero que ya está en internet, la mayoría, pero me estás diciendo que en tiempo de entrenamiento me va a demorar la mitad Pues yo creo que me sirve Entonces mira aquí lo voy a tirar va a pasar lo que ya teníamos originalmente en el en el pipeline Escoge todo lo que es de preprocess Recuerden que en esta tarea de preprocess El único paso que nos estamos pasando y
Estamos utilizando el objeto guardado es el de iterative porque la imputation se demoraba como 2 horas en entrenarse Entonces en este caso, pues esa parte la estamos pasando que es esta de acá justo. Ahora está leyendo el archivo y solo leyendo el archivo, se demora Entonces ya sabemos que entrenando se demora no estoy mal. Era 2 horas 3 horas y utilizando el 100% de las células Pero entonces ahorita lo que vamos a ver, es que apenas pase a la etapa de entrenamiento de con python, vamos a ver cómo está cpu empieza a volar y empieza a echar candela y pues por eso no lo ejecuté ahorita porque capaz que se me congelaba la reunión y no les explicaba todo esto Ahí está aplicando toda la la el interés. Está removiendo todos los sliders, Ah dato, importante a mí se me ha olvidado. Está escalando ahorita. Lo vemos, está escalando porque la red neuronales sí son sensibles a las Diferentes escalas de los fichos ahorita miramos, Qué tipos de escalas hay y que eso es un eso es un método de normalización que tienen.
Entonces, mira Acá está acá está imprimiendo, pero mira, mira la cpu, Sí eso está. Está dándola toda dándola toda y de hecho este modelo está funcionando peor que el de tensorflow Creo yo que es por esa configuración inicial que yo les decía que se inicializan los pesos para que sea más, eh, más fácil para el modelo converger y ya tensorflow tiene por defecto, pero de pronto tengo que hacerle un ajuste al de python, no me dio el tiempo, eh? Pero espero en la próxima sesión, vamos a mostrarles. Voy a intentar sacar el cuadrado de esta cómo queda la raíz. Perdón, giramos el dato acá y mira, por ejemplo dan 14,000, pero todavía como en el no sé cuántos le puse. Creo que le puse 200 Sí pues mira, acá va bajando, va bajando y Y pues si se dan cuenta de hecho no se te va Cómo funciona mi computador Sito 10,000. Va bajando. Va bajando Ahí va bien.
Está por buen camino, pero si se dan cuenta, pues el computador la está dando toda la está dando toda la está dando toda hasta que termina no baja de 100 Entonces por ese lado uno puede decir mmm Quizás sí me compensa utilizar pyts Quizás sí sea más habiendo ya empieza a entender con datos reales como el argumento técnico que podrían tener estas personas que lo usan que dicen es algo. Quizás puedas llegar a ser más óptimos y más detallado y demás utilizando python's que con otra con otro proyecto que te demores más entonces miremos Cómo va a ver qué tal acá 9000 y así va bajando, va bajando bajando hasta que o pasan dos cosas o converge primero y ya empezamos a generar overfield o toca agregarle más epoch para que siga bajando ese landscape y siga generando un error menor y menor y menor cada vez y pues sí.
Cómo funciona ya últimos detalles esperar a ver si esto nos da así, yo creo que no está tan estallado Para que veamos el el proceso los chicos, hasta ahí sí, ven La mejora alguna duda de cómo funciona o ya se entiende Cómo es la vaina que más o menos la parte está Esta noche, va Yo la verdad no, no había utilizado Eh pyt, Pues digamos que las veces que entran a dos redes ha sido con con pues kilauren.
Eh, Perdón ligeras y sistema decía que que algo Me está faltando es más inclusive Pues creo que algún posiblemente te faltó de pronto algún parámetro por allá que que diga use, eh? All cpu o alguna vaina así pues pero no está, está muy bacano ya lo que sería es variar como la arquitectura de la Pues de la red, a ver si a mejor performance con más con más capas o cambiando la función de activación O él o él a la voz lo que sea, pero está muy bacán, súper bien el comentario, Qué chévere, que te haya servido de hecho yo también, pues de hecho lo entren en estos días porque yo también era tensor. Yo era como hagámoslo con tensorflow, porque así lo decía como los libros la mayoría que yo leía, pero ya investigando y leyendo papers y demás pythor ahorita digamos para hacer élite y para hacer un desarrollador. Bueno, en esto yo creo que tienes que tener python en mente.
Entonces sí Mira O sea la cpu va con toda con todas, no listo chicos, si no hay más dudas entonces pasaría a la parte ya de de de la escalabilidad que es una de las formas que se puede hacer normalización para que le entrenamiento sea más estable, sí, por último comentario un Random ahí. Pues creo que el único defecto que tiene python sobre tensorflow Es que la librería que es mucho más pesada, si no estoy mal, pues Bueno al menos en keras keras siento que es más liviana cambio, por ejemplo, yo he montado a dockers que utilizan torch Pues de de base y siento que esas vainas pesa como 2 gigas o sea, siento que es como como súper gorda Entonces el tensorflow Pues bueno no te sorprenderás siento que es como más liviana pero pues puede ser percepción mía, cuánto te pesa por ejemplo ahí en el en el ambiente has revisado no? No ni siquiera sé cómo se hace Acá no tienes ahí el punto b o o cómo tienes el
Mira por ejemplo, o sea el lip el lip Okay ahora el Live y ahí vas a ver las librerías que tienes instaladas búscate por ejemplo la de tensorflow comprarla con Lite Okay aquí no me tiró todo mal denle clic derecho al al Ah pues sí, sí sí tienes una carpeta Entonces igual que Torres bored, Dale clic derecho y miremos a ver clic derecho propiedades
1.191 y pirate ahí torch es interesante para que uno levanta después un docker y se da cuenta que que son gorditos. Bueno Oye sí y que la idea incluso era Ay entonces no sé si de pronto no uno de los modelos de pythor los podrá correr compilados o alguna vaina para que no no ocupen tanto espacio. Yo creo que sí incluso hay un hay un algoritmo que sacó un programa o algoritmo no me acuerdo que sacó Microsoft que lo que hace es limitar, por ejemplo los parámetros en vez de tener como 30,000 decimales como los pesos los va recortando hasta tal punto que que se vuelve mucho menos pesado, el el modelo. Yo creo que Peter se fue como torch no no aparece por la t o dos.
Ah sí, Mira eres claro, ahí está excelente aquí sale más más barato OK Google debe haber una forma de renderizar y yo creo que ya te baja el peso. Yo creo que sí era una forma de renderizar porque por ejemplo yo cuando lo he utilizado lo que hago es que tengo que instalar la librería y después cargar el modelo, pues cargar los pesos y ahí sí utilizarlo, no sé si habrá otra manera más sencilla, Pues no sé si vos conozcas, cómo dices que es la tuya Pero yo lo que hago es que cargo la librería, pues obviamente instalo torch o instalo, pues eh? Quieras cargo los pesos que pues están en un archivo.
Pico, lo está en archivo esos de k5 o no sé cómo se llama No sé cómo se llama el archivo de The pythor, Okay punto pth. Creo que es listo Entonces yo lo que hago es que le dicen instancia cargo los pesos y ahí sí le ingreso Pues el el valor que quiero pronosticar pero entonces claro, tengo que cargar la librería y ahí se me van de una una giga de en el docker Ya sé y eso hace que el docker después sea más lento para levantar esto Esa es la idea de este grupo que que probemos sí igual que alguno más de de de grupo sabe Ahí le dejo la inquietud experimentar. Es que es exactamente Esa es la idea falso experimentar probar molestar, Sí por eso también este código lo tienen ahí y que ah, que voy a meterle, eh? Fine tuning, pues para pulir más estos, hágale, Así es como se aprende Así es como se aprende.
Vais echando picas y no bacano que lo compartas porque de pronto estos detalles Yo no los estaba teniendo en cuenta hasta ahorita que tú lo mencionas porque pues estábamos enfocado en el entrenamiento que en el despliegue que ya eso lo vamos a ver en la En las siguientes, sesiones y y pues no ahí cómo se pueden dar cuenta, pues unas por otras quizás, las sintaxis más compleja performance al parecer mucho mayor customización, te puedo meterle cosas más más detalladas también va ganando ahí arquitecturas más avanzadas también va ganando ahí. Sí, de pronto la simplicidad que tiene este funcionado un poco en contra, aunque también leí que tenía un un como una capa más detallada tipo python, incluso python tiene otra que se llama python lighting o algo así y que es una capa parecida a la de tenso, pero usualmente se usan en estos dos formatos y pues nada, miren chicos, ahí sigue bajando. Sigue bajando y ya.
Estamos llegando a los 200 entonces quizás como no hubo un Eric stopping, podríamos bajar incluso más el mueble podría bajar incluso más en su entrenamiento para para llegar a espera 6000 y si se dan cuenta nos estamos acercando cada vez más a esos a esos 2,000 que tenía 2500 que tenían de kappas utilizando automed, pero esta era la idea que pudiéramos ver cómo funciona Cómo funcionaba. Nada Por qué no se ha detenido todavía porque mira que por ejemplo ya en fíjate contador de épocas ya en algunos está tirando como como una época, pues perdón, un MS más más alto luego uno más bajo luego uno más alto buena observación Y es de hecho, me hiciste acordar algo que les quería Mostrar Porque mira que aquí se hace la validación se hace esa evaluación contra el validador, pero Se entrena contra el
Mentiras, no creo que tienes toda la razón alguna Esto va a ir, no, Ah bueno ya yo creo que ya acabó sí ya acabó, pero no acabó por la razón, correcta. Sí puedo cambiar esas condiciones por ahí, un stop un break para que termine el signo. Sí, sí sí sí sí sí sí, sí, yo creería que sí sí, porque si no igual guarda los guarda los los pesos, pero vuelve y sigue en otra época Entonces sí, pero mira que no no no no, porque acá a pesar de que sigue tirando es lo que va a hacer es ignorar la actualización de los pesos, Sí me entiendes, puede que yo me haya desviado un poco, pero si puedo seguir bajando y encontrar otro peso no haría falta colocar un proyecto, Ah okay, igual, siguió terminó todas las épocas pero se quedó con los pesos más bajos, correcto, vale? Correcto, lo acabas de decir esto era justo también lo que yo le decía vamos a este tipo de customización. Quizás tendría que yo investigar más, pero no sé qué tan fácil sea hacerlo acá solo con él, entonces tú ya le puedes meter aquí algo más avanzado que funcionó un tipo de stopping lo que funciona con
condicionales que haga algo más parecido interesante por eso acá nunca funciona así entonces mira acá como resultado Pues que tenemos Este es el el mínimo valor que logró que capaz con más 6000 y pico de pronto con más de drogas o más neuronas diferentes distribuciones, pues tendríamos que probar hacer cambios pero yo creería que se puede mejorar Pero como veis me parece que bastante bien y por aquí tenemos el de train entonces mira acá se ve un poco la curva Sí otra vez 200 iteraciones y a pesar de que hay piquitos si se dan cuenta hay una tendencia a bajar todavía va a bajar por lo que podríamos llegar a más abajo así así seguro yo creería todavía habría cancha para para meterle más el acelerador y si lo dejamos toda la noche capaz, que encontramos algo más o menos bien y así y así muchachos entonces
Esto era la gran parte de lo que les quería Mostrar hoy ya ahorita terminamos está reu colocó comentarios en el commit y lo lo subo y pues nada, de verdad invitación a que le juegue yo, eh? Como les comentaba yo yo, pues el mindset, que que me gustaría que hubieran este en este grupo de estudio es ser buenos Sí para ser buenos hay que entrenar es como yo lo veo Hay que meterle hay que jugar con esto Hay que sacar ideas hay que tener un proyecto y parce, cómo lo hacemos. Me entiendes Eso es como la única forma que yo veo que uno puede ser bueno Y es el mindset, que yo les decía que ahí en en patinaje de hecho ayer nos metieron mero regañó, Pues porque no se está cumpliendo las las como las las metas que se tienen en mente y pues es parte del compromiso O sea si queremos ser buenos.
Metabólico metabolismo jugamos con esto así están empezando así no tengan mucha idea justamente así se aprende así se aprende metiendole el colmillos y pues nada, eh? Qué? Qué me queda Ah bueno este es el el dataset que ya tiene los los valores Mira acá, lo está este es el real Este es el french en algunos casos me imagino que estará cerca nosotros estará mal, pero mira, hay algunos bastante buenos se desvía un poco pero ya sabemos que en promedio se desvía como por 6000 se podría mejorar sí pero pues es un resultado decente para esta primera vez, eh? Siguientes pasos, pues se colocó la votación y la idea es utilizar todo esto que hemos hecho ya para hacer un despliegue, qué nos faltaría para completar el python? Ya tenemos la parte del preprocess ya tenemos la parte del entrenamiento lo que sigue aquí para allá ya es más sencillo.
Porque acá ya tendríamos una un siguiente paso que tendría que sería el test Sí y básicamente lo que se hace es se coge ese modelo entrenado que se que se buscó que no tuviera un scrim ni nada que funcionara suficiente bien suficientemente bien y aquí se hace una evaluación sencilla, por ejemplo de que compáralo con el modelo anterior, que yo tenía con las métricas del modelo, que ya está en producción, sí, Ese sí, esa métrica llega a ser, eh, Mejor la del nuevo modelo Pues despliega el modelo eso sería lo que sería acá sería el test y sería ya hacer la diferencia o el display, que que se podría digamos que han cogidos de la mano, el diploma, Qué es es ya colocarlo en una instancia donde donde, o sea le colocarlo ya en producción, eso vendría siendo un diploma y cuando ya sé se habla de inferencia, es cuando ya sé cuándo ese modelo ya vive en una instancia.
Dependiente y es usado Entonces cuando es usado Tiene ciertas particularidades también que es muy probable que cuando ustedes desplie las entradas que vaya a recibir ese modelo son las mismas que tienen originalmente en su dataset, es decir las mismas que recibimos aquí en el pre-proceso, el nombre en vez de tener Pues el vector del color tenemos Pues el nombre del color es muy probable que que que que que funciona así entonces acá lo que se hace es en esa inferencia hay como una capa antes de usar el modelo que aplica las transformaciones utiliza el modelo y entrega la respuesta esa sería la forma de funcionamiento La idea, es colocarlo en una Apple por ahí en fase x se utiliza y la tendríamos ya en un docker, eh? Hay dos formas, Pues hay dos librerías que se han sugerido una que se llama ML Flow aunque tengo la sos.
Que estás más de tracking, estás más de de seguirle el paso a cómo lo está haciendo el modelo? Quizás un poco de experimentación, pero no, no, en lo que he leído y en lo que yo lo he utilizado no está tan enfocado a a colocarlo en una instancia productiva ya ya completa, Sí sí, ahí tienes. Tienes la razón César sígueme el Flow es para hacer el Track a a los modelos que estás ejecutando, pues porque normalmente uno no entra en un solo modelo cierto, no entrena varios modelos y la idea de de de eso es compararlo cierto, al final Pues con mlflow, pues eh, te ayudas además de hacer el tracking sacar gráficas normalmente MLB lo conectas con asher o con Amazon o con Pues hay muchos sistemas para conectarlos, pero a veces todo el tema de experimentos y Entonces digamos que desde mlflow es relativamente sencillo escoger cuál modelo quieres sacar a producción, pero igual lo podemos hacer totalmente manual cierto, no hay necesidad, o sea también el Flow porque sí sí, ya como cierta.
Y probablemente nos toque conectarnos con no sé una distancia de esas gratuitas de Amazon alguna vaina Entonces si no queremos es cómo Cómo evitar la fatiga lo podemos sacar directamente manual elegir el modelo y hacerle un un despliegue digámoslo así y aquí está aquí es por eso que está la otra propuesta esto de hecho lo aprendí en el en el en la python, esto yo no lo sabía hasta ahorita se los juro ni siquiera la he usado, pero tuve estuve en este workshop donde este Man nos mostró. Esto se llama QR y él también nos dijo literalmente. Estoy diciendo que maldijo my Flow es como más para tracking y yo sí tiene razón y él nos mostró como esto ya podía ser con QR snaps con dockers literalmente crear una instancia de un docker con solamente lo que tendría la diferencia para utilizarlo y sacar porque esto podría ser útil porque incluso nota a pie de página el manejo, en dónde lo estamos usando.
Queremos quitar Él dijo así, porque requiere mucha vaina de permisos requiere cosas que ya a nivel productivo y de infraestructura son complejos Pero él dijo para aprender cómo funciona sin tener que tú pagar unasur sin tener que tú pagar una yws es perfecto, o sea, Ahí puedes hacer también experimentación, pues crear todo el pipeline de hecho, a ver si nos muestra a ver si vemos como un una imagen no he terminado de de explorar esto completamente a ver si en b2 nos tira una imagen, pero esto ocurriría en local, o dónde correría local local? Nosotros estamos haciendo local en el workshop, Sí todo esto ocurriría local Solo que lo que te digo como esos esas vainas de los permisos tengo que crear lo que ellos los tenía como mamados, porque todo eso lo tenía que hacer personalizado mientras suena y W después ya te lo hace Pero entonces él decía esa puede ser una muy buena opción para que tú tengas ese en tu ent es decir cuando
Entrenar el modelo ya tienes el país, vean totalmente desarrollado se Ejecutan todos los pasos incluso cada uno de los pasos se pueden ejecutar en un docker diferente el preprocess el Trade y el test. Simplemente hay que conectar las cajas de forma independiente por componentes. Sí sí sí sí, Y ya puedes hacer incluso cosas más complejas, no? Porque ya en producción, tú puedes tener algo acne, son tres modelos que son embebidos todos y entonces tengo que entrenarlos todos y luego evaluarlos en diferente manera y crearse esos pilotos complejos en diferentes componentes cada uno y el básico esto es muy bueno para aprender, pero en producción, ya lo estamos quitando y dije pues para aprender Sí pero producción, ya ha notado que al menos la herramienta no es robusta en cosillas para que para que funciona Ah mira, este es el dashboard, después tener los experimentos los runners, eh? Supongo que estos son escalas, no sé programas que ejecuten artics.
O sea, completo más o menos completo si es entonces este también lo podríamos explorar dicho igual, pero yo creo que la la Pues en el caso de que esté muy complejo la fácil, es que generamos, pues el el archivo con los pesos generamos un lápiz sencilla con con fase y Pía y hacemos carga Pues en un docker que carguemos los pesos dejamos, lo dejamos expuesto en un puerto y y subimos subimos el el lo que quedamos pronosticar y que nos devuelva el json con la salida, Sí yo creería que por ahí va a ser el camino porque porque yo no sé si yo haga esto en 8 días con curva, sino total, igual si se animan de una muchachas, sí, algunos animales. Háganlo. Hágale, Qué pasa es el espacio y nada, ya creo que esta sesión no les tenía, no les tenía mucho más, eh? Si les publicó el código y cuando ya esté la reunión grabada la reunión, hay algo más.
A nivel administrativo dentro de ocho días hay un hay un evento a esta misma hora el miércoles. El evento se llamaba tinkers Creo que creo que eh, Juan creo que te vas compartido el link cierto es un estimado en el chat de del disco Epa sino que yo mirando el calendario y gracias por eso yo mirando el calendario se cruza y yo pues estoy muy interesado en participar en esa entonces, posiblemente les dije también otra otra encuesta el jueves o el viernes mañana o el viernes para mover la sesión Al martes o el jueves ahí ya escogeremos porque esa a esa reunión Pues justo se se casa se clava con la con la de esta y ahí queda más difícil listo chicos, entonces.
Las dudas preguntas comentarios metal en el dientes está muy chévere, no parece, no todo se vuelve acá Muchas gracias Exacto acá, lo que les gustó para que no que les guste, eh? Ya he tenido próxima sesión despliegue de pronto. Hacemos algo un poco más sencillo incluso lo podemos combinar con lo de testing, el PayPal completo y saltamos ya sería a la siguiente semana a mirar los autores, que ese es el tema ese tema también está brutal créanme que les va a gustar. Eso es impresionante con todo. Eso se maneja el generativo se manejan, eh? Detección de anomalías, se manejaba reducción de ruido, se maneja, eh? Al otro que me gustó mucho Ah bueno, las gantz vienen de acá, que es, eh? Cuando tienes una generativa tienes una discriminadora viene de ahí.
Eh, usualmente lo usan para imágenes, pero yo tengo la absoluta seguridad que se puede usar para cosas más complejas como por ejemplo o bueno closets que no sean Tan tanto imágenes No digo que las imágenes sean complejas es que las imágenes son muy de humanos muy intuitivas, pero yo creo que esto se podría utilizar, por ejemplo generar un carro nuevo que sabemos que es el que más se vende Una vaina así características de carro que más se venden si lo combinamos es está marca con este motor cuando se queda Data o característica si tenemos más datos Supongo que característica del carro que es más rápido en estas características Data colocar un poco más complejo se podría hacer hasta estrategias Sí ya lo colocamos en en una línea de secuencias Cuáles son los pasos y las mejores estrategias para poder tener en un en un modelo reforzado ponle tú para poder ganar el juego, Qué estrategias tengo que seguir o incluso, eh? Sabes lo que yo estaba pensando eso.
También Sé que se usa para por ejemplo, generación de tipos de proteínas o materiales materiales químicos o materiales vainas así las combinó más o menos sé que tienen esas características, eso me permite a mí como inferir lo comparo, pero todo esto viene de red neuronales generativas que su base son los autores, eso está en el libro. Eso está en el libro entonces acá lo van a ver persona chimba y saben qué libro también es ese libro es muy tenso para entender las bases es una es una canalla es muy tenso, pero es muy muy tenso espérense los muestro la imagen porque ese tengo que actualizarlo tengo que actualizarlo en en resources este de acá Este de acá es el que habitamos esté leyendo como masa profundidad para hacer el Ender sunny, este es bueno este es el que está en el resources este es bueno pero este es más denso Uf
Este es más pesado, pero uno uno entiende cómo funciona, o sea, uno ya dice Esto es así, porque el mal mt matemática y un montón de principios y todo, pero para entenderlo parte brutal estos dos digamos está en la biblioteca, solamente no, no Exacto el que no está es perfecto. Si no lo conseguimos en el mentiras, no no, no, no, no no no, no mentiras yo lo tengo parce. Yo lo tengo yo se los paso yo lo tengo yo lo tengo yo lo tengo sí, sí, porque acá, o sea, lo estoy leyendo y sé que lo importe lo importe como PDF es este de acá y no este man es muy tenso, o sea, de hecho tiene pocas imágenes, si se dan cuenta fórmulas matemáticas, pero parce, o sea, uno Ya termina de comprender.
Cómo funciona es pasado? Me ha gustado mucho, se los paso listo chicos, no siendo más, no siendo más. Ya los dejo ya me voy para la casa preguntas dudas o todo bien, pero no muchas gracias, pinche. Verás que compartimos cosas enseña eso me enseña de grafos después porque yo no tengo ni idea, entonces ahí vamos, eso está ahí priorizado, pero ahí vamos con otras cosas primero, pero vamos para allá también, listo chicos, hasta dentro de martes o jueves de la próxima semana listo nos vemos Muchas gracias a los muchachos.