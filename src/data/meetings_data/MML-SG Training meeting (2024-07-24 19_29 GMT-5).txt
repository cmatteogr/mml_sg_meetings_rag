No pasa nada cosas, qué pasa Sí sí cosas que pasa Eh bueno en esta parte que se tiene tenemos ya el test Case que se enfoca a el preprocess tenemos una entrada que es la información de los carros cruda es decir, tengo el el snai que es como Lady que se tiene allá, eh? El año los colores si se dan cuenta aparecen texto porque es exactamente cruda como viene desde la página web, que es de dónde extraemos esta información, eh? Le tenemos un factor de de contaminación Pero hay más parámetros que yo les podría que yo le podría definir porque el perro se tiene más parámetros Solo que algunos vienen por defecto es como el treasure del precio mínimo, eh? La máxima frecuencia que necesito al menos de una marca para poder entrar dentro de ese Data set es decir si tengo Solamente 100 datos de bugattis, no es suficiente para poder encontrar un
Patrón de hacer una regresión por lo que se descartaba aquí estamos aplicando lo mismo parámetros de entrada las diferencias que ya no estamos separando el precio como un Target sino el precio hace parte de ese que viene con cargos y se hace todo lo que ya hemos visto anteriormente se borran los duplicados se limpian se hacen las transformaciones necesarias se aplica works vector o 1/4 en coreano o hashtag en los puntos que se necesite y al final cuando ya tengo todo numérico le hago una una imputación si hace falta que es por aquí Arribita donde se imputa el modelo en este caso Este modelo, eh? Hay una condición de sí quiero que se entren el imputado no porque es diputado se demora como unas 3 horas en entrenarse entonces en vez a veces de de entrenarlo otra vez para Mostrar cómo funciona pues uno lo que hace es utilizarlo no lo guardan memoria, lo utilizo que es básicamente la condición que aparece aquí, eh? Lo utilizo para
Rellenar esos Campos que están vacíos posteriormente aplicó, eh? Remover los slides si hace falta que esto Esta parte de acá es calor esta parte es importante porque estamos utilizando redes neuronales, entonces necesitamos que esté en un Rango de -1 y 1 o 0 y 1 escaló en un Rango de 0 y 1 para que no sea sensible a las a las Diferentes escalas de las de las variables que tengo y ya, eh? Guardo en memoria todos los modelos Recuerden que se guardan en memoria porque en un archivo porque en la inferencia se van a utilizar para hacer la transformación de lo que entré también, eh? Se guardan los Data sets que que han resultantes ayudan a hacer que se usa para entrenamiento hay otro dato acepte que se usa para para testeo aquí se maneja la misma idea entreno el el modelo con un dato Sep y luego lo evaluó con otro Data set para saber que está funcionando bien. Aquí se agregan dos adicionales que es el
El original la información original y lo guarda en un archivo que me entrega esa información y eso ya vamos a ver que se utiliza en la inferencia, dónde se guardan todos esos pads para que se puedan utilizar después en el futuro en un json hasta ahí el pre-proceso lo lo va a ejecutar mientras vamos explicando la siguiente parte como para ver cómo funciona voy a borrar de hecho los archivos que genera que son estos de acá para que veamos cómo nos genera y acá lo vamos a tirar Aquí vamos a ver que en la parte de la imputación voy a abrir un poco esto no sé porque no me gusta lo hacen Aquí está van a ver que preciso cuando llega al imputado esto se va a disparar como a 100 la cpu creo que es la que se disparó más siendo la memoria de hecho yo no espero que la la llamada se me caiga pero si se dan cuenta mira ahí estamos.
Imputación si me escuchan manos arriba o si no, esperemos un rato este tipo de cosas son las que podría pasar también cuando tienes más datos los los modelos tienden a ser más pesados y requerir más recursos entonces, por eso es que utilizamos gpus, por eso es que utilizamos, eh? Patrones distribuidos, que eso es un patrón de diseño que también lo podemos ver en el futuro de hecho después de que terminemos esta este proyecto, eh? Creo que vamos a tirar una encuesta en la que vamos a decir Bueno qué tema queremos seguir ahí se van a ver varias opciones entre esas patrones de diseño y Machine learning, eh? Pero si se dan cuenta miren estaba chupando memoria, que no puede y es porque ese modelo pesa bastante y de hecho me me siento animado a comprar un nuevo computador, pero ya veremos qué pasa después de un ratico más y el disco también va todo porque está leyendo el el archivo. Está leyendo el archivo que está guarda.
Acá quizás otros métodos serían listo, mientras esto termina, eh? Voy a saltar un momento al otro si el programa me lo deja si es que mira la máquina va lentísima por lo mismo que se está consumiendo todos los recursos. Eso va a ser solo un por un rato y pasamos al test Case de train y le empezamos a echar un ojo Ah Pero espera antes de tren entramos al sampling para que se entienda un poco Cómo funciona ese sampling también que es lo que acabamos de ver con cormo. Lo estamos aplicando acá, vale?
entonces
Hola Ya me escuchan de nuevo Sí ya pare la ejecución Sí ya ahí volvió no, no, entonces esperen hagámoslo a la antigüita porque si no ahí sí, necesito que tengan un poco de fe de que funciona lo pueden ejecutar en la casa. Ahí verán que funciona y me genera estos archivos que aparecen acá a mano izquierda, que básicamente uno de ellos uno de ellos tiene la voz de original, porque a veces uno necesita entender o relacionar el tren, que uno está haciendo, pues con los datos originales, eso me ha pasado entonces suelo crear dos archivos, uno que tiene la información original y el otro que ya tiene la información pre procesada que se está acá, sí que ya si se dan cuenta son puros números escalados y es exactamente lo que necesitamos para que funcione Entonces ahí. Sí, acto de fe muchachos me tocó pararlo Porque si se dan cuenta pues él.
Diputados y todo Necesito algo más potente evidentemente O sea ya me lo está pidiendo la vida, pero acá ya podemos ver que son 115, eh? Columnas tenemos 71.000 pilas bastante información, entonces sí, cuando eso es lo que estamos trabajando. Ahora entramos de pronto una parte a la parte del sample, el sample, Porque es importante Ya Lo acabamos de de comentar, porque puedo entrenar el modelo con la misma calidad, pero con menos datos Y eso me facilita a mí, pues el el uso de recursos acá de hecho lo que estamos haciendo es ejecutar, eh? Un modelo una función que simplemente me coge el dato frame, me saca un sampling. Me saca un una muestra en este caso por defecto tiene 1000, pero acá le estamos tirando 20,000 y hacemos la
De cosmogonías que tengo en ese Data set se alinean bastante bien con la distribución que ellos pudieran tener si el valor es cercano a uno significa que la distribución de ambos o se puede garantizar que la muestra representaba de representada Demasiado bien o muy bien el dato original Entonces vamos a tirar aquí el el caso solamente para ver cómo cómo funciona y qué datos me va tirando Recuerden que aquí le estamos tirando 20,000 y originalmente tenemos 71,000 Ay ahora no funciona creo que rompí algo que no tenía que romper esas en este pre-proceso. Cuando le dice control Z sí me lo sospechaba aquí, me lo sobrescribir. Bueno, tiremos le de pronto este de acá. Esperemos que el test tenga suficiente datos el nombre que 10,000.
Cosas de hacer pruebas acá preciso alcanzó a sobreescribir letra y tristemente Ah pero mira justo acá lo tenemos casualidades de la vida que lo copié acá reempla ahora sentiremos otra vez el test Case y ya debería estar vacío bien debería estar vacío Perfecto entonces Mira por cada una Qué pasó acá por cada una de las características se está evaluando se está haciendo el test de si representable suficientemente bien del Data original listo entonces uno puede colocar un trecho un valor umbral y uno dice Mira si estás por debajo del 0.9 no me sirve Necesito que consigas otra muestra o que no me borres tantos datos o uno Busca acá jugar con esta representación acá podría ser digamos cuando tú también y debería tener un score mejor de hecho a ver si con eso nos tira mejores datos.
Por encima de 0.9, que creo que sí, Mira 081 al menos en este tomar muestras está haciendo poco complejo Se está perdiendo un poco de información por ahora Dejémoslo así para que podamos ver los beneficios que tiene esto es esperado, que cuando coges una muestra pierdes un poco de información original Pero tú buscas un traidor un equilibrio en el cual puedo tener suficientes datos como para que se mejore la velocidad del entrenamiento, pero no pierde información de donde pierdo tanta información del deitac original entonces en el resto si dan cuenta está muy cercano a uno o es simplemente uno directamente representa bastante bien, eh? Mira acá, otro 0.6. Ya se va perdiendo más información ese tipo de cosas puede ir pasando, pero este tipo de cosas sirven para monitorear o si el ejemplar el, eh? La muestra que estoy tomando representa bastante bien el dato original ahora esto Cómo se ve en el performance del entrenamiento.
Y ya vamos a vamos para allá Entonces tenemos aquí el el train de El del autoencoder y Aquí vamos a hacer, vamos a echarle una revisadita solamente para mostrar Cómo funciona. La vez pasada hemos hecho, simplemente un un modelo un poco burdo medio preparado de cómo se ve la estructura y dijimos que íbamos a bajar las las capas de las neuronas en un 30% es decir, la primera capa voy a tener el la entrada original. Luego sacamos el 30% de esa cantidad de neuronas que eran 115 les acabamos el 30% que ponle tú que era como 85 y ese era el tamaño de la siguiente capa así era el tamaño de la siguiente la siguiente capa y el modelo se veía justo así como aparece acá, que es, eh? El original Luego pasamos 84 de 84 pasamos a 58 y así empezamos a generar ese cuello de botella hasta llegar a 40.
Y luego a partir de ese 40 reconstruida con la misma distribución solamente que a la inversa el de cover Entonces tengo el incoder y tengo el Discover que aparece acá y, eh? En el Forward los uní para que quedaran con la estructura completa y esa es la estructura Pinal esta parte de earl stopping Ya lo vamos a ver en un momento que es una forma para evitar que tenga o perfil, que es que se entrene demasiado bien con los datos de entrenamiento, pero lo lo haga terriblemente mal con los datos de de validación que era el ejemplo que yo, por ejemplo, les decía la otra vez Es como si ustedes estuvieran entrenando manejando un carro, pero te especializas solamente en una marca de carro con las características específicas y apenas te tienen un carro que tiene, eh? La manija más a la derecha más a la izquierda te rompes porque te entrenaste demasiado bien Fue en un carro específico Entonces ya cuando generaliza, eh? Puedes trabajar con
Diferentes carros igual de bien esa es la idea que no te especialicé sino que puedas generalizar a todos los datos y lo hagas lo suficientemente bien para la mayoría de ellos ahora en el entrenamiento, cómo funciona Este es el modelo en el entrenamiento estamos llamando el dataset que viene con el enviamos el PayPal leemos el dataset como un Data frame, aquí Hacemos unas validaciones de eh, verificar que el tamaño de de cómo hacemos Eh un split, creo que es de validación y entrenamiento aquí se hace una validación de que no superé el 70% pues para que no haya problema Recuerden que el Data set de validación es diferente al dato hacer dentro de de Test porque el de testing se usa al final solo para hacer pruebas el de validación se utiliza para validar el modelo lo está haciendo bien en el entrenamiento. Entonces aquí se hace también una división de ese dato, eh? Por ejemplo Aquí también se evalúa que el
Que ya lo vamos a ver qué es lo que acabamos de ver del borracho. Que coges un que coge solamente una unidad o una muestra o el datación completo, eh? Tenga unos valores definidos Aquí también hacemos una regla de eso en esta parte transformamos la entrada en un tensor que es cómo funciona pyts básicamente transforma esto en números que están en la escala de float 32, la razón que lo porque lo hace es porque se puede sacar provecho a la cpu, Con todo O a la gpu y es como mejor funciona, o sea, sí, esa transformación solo por esa razón, que si lo dejas con el dataset original de pandas que a pesar de que es eficiente no es tan eficiente en algunos casos, entonces acá lo que hace es una transformación para que funcione mejor el entrenamiento de nuestro modelo aquí se hace el Split que les comentaba de que cojo el el Data set que ya tengo acá transformado le hago un split para poder sacar un 10 20%
30% máximo y poder evaluar sobre el entrenamiento que también lo está haciendo mi modelo Entonces de esa manera, puedo evitar que también tenga un overfield, sea más fácil aquí se hace el batch esto lo vamos a ver ahorita en un momento que es aquí es donde yo defino que muestra quiero coger para calcular el gradiente. Sí quiero coger el Data set completo o quiero coger una muestra Y qué tamaño va a tener esa muestra y ahorita vamos a ver Qué diferencia, hay cuando ya le tenemos el entrenamiento que le cambia el número y Ah mira, se demora menos se demora más, eh? Vamos a ver eso también número de Potts que es básicamente el número de ajustes que quiero que vaya haciendo el el modelo es decir, entra a la entrada del carro reconstruye y envía el error reconstruye, eh? Envía el error con el gradiente ajusta todos los pesos, eso es un depósito o una época se llama también quiero que lo haga 300 pesos, entonces le vuelvo a enviar un dato, eh? Lo vuelve a reconstruir calcula el error. Envía gradiente.
1 2 3 4 5 300 veces quiero que lo haga para eso es este número que aparece acá, eh? También está este Dr stopping, que es Sí el Data set el dataset que yo estoy usando depende Aquí les muestro un poco Cómo funciona esto de early stopping, que aquí se ve mejor en una imagen en esta capa a partir de los entrenamientos que yo voy haciendo con los spots que voy envíe datos reconstruyo ajusto con el garante ajusto con el gradiente el error que yo voy calculando va disminuyendo porque ya sobre el tiempo del modelo va aprendiendo los patrones y va reconstruyendo en este caso mejor la salida o en el caso del precio va entregando mejores precios, Qué pasa que en cierto punto si yo le sigo diciendo que se siga entrenando él va a empezar a tener overfield, que es para el dataset de entrenamiento.
Que es el que aparece aquí training training set el error va a seguir disminuyendo, pero para el Data set de validación El error en cierto punto empieza a incrementar Y eso es porque la misma idea que les decía del carro, te especializa tanto en manejar un carro que apenas te mueve la manija. Se rompe se rompe tu cerebro porque no sabes cómo manejar ese movimiento de la manija o que el pedal sea diferente o que sea automático y el otro manual ese tipo de cosas se puede interpretar de esta misma manera, Por eso se usan datos siete validación para que apenas el de validación empieza a fallar, ahí tengo que hacer el Stop tengo que decirle ahí ya deja de entrenarte porque vas a empezar esa tener un mal comportamiento y no estás generalizando lo suficientemente bien. Entonces para eso se utiliza, eh? Este patio es básicamente decirle Mira 100 de estos 300 100 15. Ya la estás embarrando para sí, Ya en 15 seguías la Estás embarrando mal, no no?
En dónde más sí acá uno Incluso le podría poner uno que simplemente si la embarras en una para pero recuerden que en este landscape en este plot en este en esta reconstrucción del error podemos tener algo que se llama globales y eh? Globales y locales mínimos entonces ahí algunos puntos Déjenme de pronto, acá lo explico mejor con una imagen Ah pero no quiero borrar global mínima global and local en el error yo podría tener puntos que por ejemplo como este verde que aparece acá que puede que sea el punto más bajo en esta parte del error de los parámetros que yo escojo para reconstruir Pero hay otra configuración de parámetros en esta red neuronal que me puede entregar un error incluso menor nosotros queremos encontrar ese error.
Menor y lo que pasa es que si yo le colocó una paciencia muy corta apenas él detecte que aquí ya está haciendo un fallo Pon tú, que sube aquí un poco en la dirección que aparece acá move el para Pero si yo le permito que siga explorando al menos por 15 pasos él podría encontrar de ese global mínimo esa es la razón porque el patience o la paciencia que sería la traducción no Pues en lo personal en este caso específico no me gusta ponerlo en uno porque podría podría explorar aún más diferentes configuraciones y encontrar una que se comporta incluso mejor que la que tengo actualmente para eso se hace esto y para eso funciona el el Eric stopping que vemos acá este código básicamente hace lo que les acabo de decir tienen memoria, como Cuántas veces las embarrado Cuál es la diferencia de esa de esa y se desamparada que acabas de hacer Y a partir de eso, pues irte para ya no te entrenes más.
Pues lo que vamos a ver aquí en el código también ahora Aquí hay una diferencia con respecto a lo que vimos de pronto la sesión anterior, pero que ya la hemos visto en la parte de presidir el precio Recuerden que hay algo que se llama find Unity en en Machine Leal que es básicamente encontrar las configuraciones del modelo encontrar esos hiper parámetros que los hiperparametros son datos que tienes que enviar antes de enterrar el modelo, No los puedes descubrir dentro del entrenamiento Si no tienes que predefinir los por ejemplo un hiper parámetro, eh? En esta solución es número de capas que estamos manejando en el autor que es Cuántas capas que es utilizar en este caso estamos utilizando tres tres profundas, em o también podríamos decir Cuántas neuronas, quieres que tenga eso es un hiper parámetro hay otra, que por ejemplo es el el learning rate que es el básicamente.
bajando ese error, qué tan largo quieres que sea el paso que Recuerden que siempre queremos bajar como esa montaña de error hasta el punto más bajo Entonces si doy paso es muy largos capas que me pierdo el global, pero si de paso es muy cortos, me demoro toda la vida en bajar hasta el final de de ese landscape de rock para encontrar el error mío, entonces esto es otro hipermetropía colocarlo aquí algo que se llama obtuve, que es el equivalente a por ejemplo, lo que nosotros alcanzamos a ver en skiller de Creed se acuerdan que habíamos visto algo que era y había una cosa que se llama greet Search y también había Random Search y había que básicamente él encuentra diferentes configuraciones de
Parámetros por ejemplo, aquí se ve diferentes configuraciones de los hiper parámetros el testea con cada una de esas y te entrega Cuál es la mejor la mejor que se comportó hay otra que es Random que literalmente Escoge y para parámetros al azar y Espera que encuentres mejor hay otro que hemos visto que era con Valle no que a partir de métodos probabilísticos él podía definir Cuál era el posible futuro mejor valor de ese parámetro para que el error fuera menor Entonces cómo utilizar el mismo truco que habíamos hecho de es como que Machine learning se ayuda a Machine learning a mejorarse y aquí sería la misma idea obtuvo una es una librería que me permite a mí hacer fine toning de una red neuronal Entonces en este caso. Yo le estoy diciendo Mira Necesito que le hagas fine Junior al learning rate es la única que estamos utilizando El Paso recuerdan, qué tan largo quiero que sean esos pasos en este Rango quiero que sea de este a este valor y quiero que los pasos que es sean de forma logarítmica.
Qué significa eso que vas a saltarte, por ejemplo de del learning rate 10 pasas a 100 pasas a 1000 pasas de 10,000, eso es logaritmo Sí en vez de que sea lineal que lineal sería por ejemplo, 10 20 30 40 50 que es una línea sí, acá puedes definirlo y empezar a entregar diferentes distribuciones para que él empieza a estallar tu cpu con toda ra y encuentra el mejor resultado, eso es lo que queremos que aquí le definimos Pues el modelo este learning red Select definimos acá en el optimizador Recuerden que el optimizador es este algoritmo que me permite a mí Espera que ese ese kit siempre nos sirve Dónde está siempre.
Por ahí de vez en cuando que básicamente es ese ese algoritmo Ay No mira aquí está solo que sé que hay uno que es más chido este algoritmo que me permite a mí bajar y extender ese gradiente encontrar ese gradiente con esa inercia, para bajar lo más rápido posible ese lapse y encontrar el error Mínimo yo sé que en git que es más chévere creo que es esta de acá, sí que mira que acá se pueden ver los diferentes tipos de optimizadores algunos funcionan mejores que otros y algunos bajan más rápido que otros entonces, por eso queremos hacerlo para bajar lo más rápido posible y consumir menos recursos y aquí le estamos diciendo es ser el el orden Rec el learning red queremos que use y el optimizador que estamos usando es Ah pero este podría ser otro optimizador y podría ser otro hiper parámetro que yo defino aquí arriba, eso también podría pasar. Yo le puedo definir cualquier que use o cualquier que cambie o lo que sea, eso es una opción inicializamos el leerlo stopping, que es el que como les comentaba es el que me va a permitir a mí frenar frenar antes de que de que ya te me
acá súper feliz y te rompas y aquí es donde empezó a ejecutar los spots que es por cada una de estas iteraciones que quiero que hagas que son 300 de estos ajustes que quiero que hagas Pues básicamente quiero que calcules el el error en este caso hay otro foro interno porque aquí es donde sucede la parte del Bach que no quiero que lo hagas con el Data set completo, sino con una muestra que cojas una muestra Pero al final quiero que calcules me encuentres el error que se hace a partir del criterio el criterio que escogimos fue MS que ya vamos a ver por qué escogemos MC ahorita y vamos a ver por qué escogemos en el futuro otro que va a mejorar este autor, pero básicamente es compárame la reconstrucción que me hiciste con el dato original y MS significa restarlos y saca el cuadrado en ese caso Buscamos que el error sea lo más pequeño posible la razón por la que sea el cuadrado ya habíamos visto porque
Si el error llega a ser muy grande significa que el valor al cuadrado va a ser mucho más grande aún por lo que el modelo Tiene que ajustar más duro y vamos a empezar a tener una precisión más más grande, por eso se usa MC en algunos casos. Funciona mae, que es con el valor absoluto ya no es tan drástico a los errores grandes y demás y hay otros más hoy, vamos a ver uno que va a estar bastante interesante entonces cálculo ese error que hay acá, eh? Sacó el el promedio de todos los de todos los buts que calcule y luego evaluó, pero si se dan cuenta la evaluación, lo hago es con el dataset de de validación de esa manera, puedo decir de entregaste bien Te entrenaste con el con el de training, pero valió con el de validación valga la redundancia Y de esa manera, puedo generar una Independencia y encontrar si tiene su perfil o no? Vale aquí al final se calcula el valor total de esa validación que es exactamente lo mismo que
Acá acá Se imprime, simplemente en consola, eh? Se calcula el error de construcción que es básicamente el mismo rc ese mismo, eh? Vale Lost era lo que yo les decía que por cada una de las características que yo tengo comparo que también lo estás haciendo encuentra Esa diferencia y ese va a ser mi valor de reconstrucción, si el valor de reconstrucción es muy alto el error, perdón, el error de reconstrucción es muy alto significa que el autor no está funcionando todavía lo suficientemente bien por lo que tengo que volver a entrenar Esa es la idea listo Entonces se hace todo, eso se hace todo esto aquí hay unas partes de código que simplemente toman tiempo. Ya vamos a ver cómo se ve en la ejecución Eh Al final también se ejecuta el Airlines stopping, que es, básicamente le digo Hey Mírame Sí con este error de validación, que acabo de calcular este Man ya tiene overfield, Así que es cuando ya empezamos a ver que hubo una diferencia entonces ahí lo cálculo, acá y si
Ver o perfil le digo para ya no me no me no me entrenes más y rompe el Loop de de le puse Es decir para el entrenamiento por ahora Para que no haga otro experimento y luego retorno el error aquí se hace ya el la inicialización con nocturna, por eso es que se necesitaba la función porque obtuvo una funciona creando una función que tiene un argumento llamado Trail y que aquí le puedes tirar Pues todos los datos que necesitas es sin meterle más cuenta del asunto es un framework Y de esa manera sirve no tenemos que entender a profundidad, cómo lo terminó de construir, sino simplemente lees la documentación te das cuenta que necesitas crear una función y le tiras el caso de estudio le dices quiero minimizar este dato que ese dato es el error de reconstrucción quiero que el error de reconstrucción sea lo más pequeño posible y qué pasa él por dentro con este optimizador y con la función si se encuentra este Trade model es la misma función que tengo acá.
Es lo que hace es con estadística y un montón de cosas empieza a mirar Cuál es el el el hiper parámetro que mejor te funciona y empieza a mirar la configuración y empieza a ajustar ajustar ajustar automáticamente Y eso te lo simplifica Eh Al final lo que te hace es entregarte los mejores parámetros, qué es lo que hace acá Apenas terminé de ejecutarse esto él te va a tirar. Mira, estos son los mejores parámetros de la red neuronal que sirve para que pues para que entre en su modelo y aquí se pueden imprimir los mejores parámetros que son recuerden los pesos de esa red neuronal, que son los cablecitos que están conectando todas mis neuronas aquí en la siguiente parte del código, lo que pasa es que se vuelve a inicializar el mismo entrenamiento, pero ya con los parámetros que yo sé que van a funcionar con los mejores parámetros y ya no necesito hacer un fighting, si no ejecutó el epoch y ejecutó toda esta secuencia de entrenamiento con los mejores parámetros y voy a tener Pues el mejor modelo posible.
Construido y eso es lo que me va a retornar aquí al final lo que hago es guardar resultados guardar el modelo y ya está, sé que está fue larga y un poco tediosa, pero ya va bien hasta ese o tiene alguna duda muchachos Recuerden que pueden interrumpir en cualquier momento está De pronto, es un poco más difícil de masticar Recuerden que silencio estoy Sí sí están ahí César sí, señor Una pregunta, cuál es la principal diferencia con este preprocesamiento que que que hiciste para la red neuronales a los que hiciste con los modelos clásicos de Machine learning, la normalización o qué? O algo más La normalización es es la principal diferencia, pero el resto te diría que es
Exactamente igual no se le hizo nada más diferencia acá adicional pero que es por el tipo de problema que estamos resolviendo es que como les comentaba se quitó el target y se hizo parte de El Data ser original es decir el precio ya no es una variable que quiero predecir si no hace parte del datación que que quiero reconstruir esa sería la diferencia listo perfecto muchachos. Ahora empecemos a ver esto cómo funciona metemos la candela Entonces vamos a ver por ejemplo una pequeña diferencia que sería vamos a colocarle aquí primero 32 el bac size, el Back size, lo suele colocar muy relacionado a las a las a las potencias cuando manejas, eh, binario que es, eh? 8:16 32 creo que sigue 64 128 258.
La razón por la que hacen esto es porque los sistemas en los que va a funcionar este entrenamiento son binarios y para que pueda manejar de forma distribuida y más o menos maneje toda la capacidad, pues se le tiran estas distribuciones binarias también esas como el argumento que hay detrás, pero esto también al final es un hiper parámetro aquí le puedes meter un 10 que no hace potencia o hace parte de esa escala binaria y te funciona igual de bien, eh? Es más como una regla, no escrita que hay entonces aquí vamos a ver un poco cómo funciona con el dataset completo sin aplicarle sample que Recuerden que el sample ahorita lo lo aplicamos y nos generó, pues este que es el el una muestra de de ese Data set, no que más o menos se ajusta al a la distribución real, entonces acá lo va a tirar la ejecución también vamos a ver que esto va a empezar a consumir cpu en este caso, si tuviera gpu, que espero pronto tener.
Veríamos como tirar la gpu para arriba, pero entonces acá lo que me interesa que veamos, por ejemplo es el tiempo tienen algún comentario ahí cuando hablaste de la gpu, Sí señor Ven a comprar una envidia o una media oscura en comentarios porque, eh, para para esos temas es más fácil manejarlos con con envidia cierto es bastante más complicado y aún así, eh? Envidia es un poquito complicado porque le toca instalar kuda Supongo que ya lo lo conoce un poquito nn esa esa vaina es complicada, vas a sufrir un un ratico Mira que hace poco escuché que lo habían simplificado que ya no sé si una librería o hicieron algo para que fuera más fácil sé que eso ya está más o menos arreglado. Sí no De hecho en en el en el repositorio o taller que que te dije yo utilizo pues.
Y gpu y ahí también hay algo para facilitar la instalación de de cuda entonces, pues también por si te sirve hay por ahí un tutorial de una mira cuando lo tenga que vamos a ver, te aviso y y me lo compartes y Súper feliz de utilizar, ahí te agradezco entonces listo chicos, retomando acá. Si se dan cuenta tiempo más o menos de ejecución 6 segundos por cada uno de los spots literalmente esto se me está demorando. Yo no lo he podido terminar de entrenar con con el Data set completo se demora calculando lo llevo 10:30 casi 2 días y con la red neuronal, que digamos que no es tan complejo porque bueno son varias neuronas, pero sí no, no me parece que sea la cosa más increíblemente compleja del mundo, no sé qué cosas más difíciles Entonces ahora, qué pasa si utilizamos ese sample le vamos a dejar el mismo número de batch.
Vamos a utilizar el sample que hace una representación como lo hablamos de esa muestra vamos a tirar otra vez el testing y vamos a empezar a ver unas diferencias en término de tiempo y de ejecución en performance ahí 3 segundos ya es un dato si es 3 segundos recortamos a la mitad al menos Entonces ya no son 2 días es uno, pero recuerden esto es un Trader estamos aquí ganando velocidad, pero Estamos perdiendo un poco de precisión los datos no representan fidedignamente el evento porque alcanzamos a perder algunos datos que con el con el test de conmovedor nos dimos cuenta que no alcanza, a hacer un fit, hay algunos que alcanzan a tener un drift alcanzó a tener un una desviación listo Entonces ahora va a parar, Qué pasa si habrá modificas el número de Bach que es lo que les comentaba ya no usas el el dataset completo.
Entonces de pronto algo más grande o una agrupación más grande, entonces cambiémosla volvamos a tirar ese mismo con él con un batch más grande y empezamos a ver también, qué tal lo hace un segundo y así empezamos a a trabajar Recuerden que acá también se Alcanzaría a perder un poco de precisión porque vamos un poco borrachos, ya no usamos el dataset completo y no somos la línea ver, eh? Azul Que aparecía aquí en en esta parte, que vamos directico al punto que es encontrando el valor porque utilizamos todo Data set completo para calcular, sino que ahora somos un poco más el verde, vamos un poco más borrachos, pero la velocidad aumenta, pero vamos un poco más borrachitos y quizás nos demoremos menos, pero con menos precisión al encontrar ese valor mínimo. Entonces aquí también pasaría lo mismo. Básicamente si lo invierto que dejó el Data es el original, le dejo los mismos números de baches.
Y vuelvo a tirar y aquí se reduce recuerdan que eran 6 segundos original. Ahora veamos cómo se ve con 6 segundos y 128 GB 2 segundos bien, sí que ahora sí utilizamos el dataset que es como fidedigno el completo, pero para hacer la regresión utilizamos una muestra y vamos perdiendo también un poco de posición, ahí son estrategias, no puedes ganar en ambas y tienes que jugar un poco con los recursos que tienes si tienes suficiente recursos gpu, cpu tiempo para entrenarlo página y venga, pero si la prioridad es que se entrene rápido o ya tienes tantos datos que necesitas Pues simplificarlo un poco. Ya hay una idea de cómo lo pueden hacer chicos de cómo se podría hacer ese tipo de de tareas ese tipo de muestras no sé si si entendieron si van bien ahí o como la sí dudas estará como una de las de las muestras.
Perfecto perfecto entonces, eh? Vamos a dejar este entrenando un poco porque me gustaría que al menos terminara uno de los uno de los spots uno de estos trails se llama de hecho uno de los trades para que pudiéramos ver cómo obtuvo una automáticamente vas cogiendo el mejor hiper parámetro por ti y ya no tienes que hacer como ese trabajo tedioso de tener que encontrarlo desde cero Entonces vamos a dejar esto corriendo como no es memoria creo que va a poder seguir hablando y haciendo cosas lo que está estallado es la la la cpu, la memoria va bien entonces entremos un poco más al siguiente tema que es, ya tenemos esto que que es el autoencoder no pero recuerden que el autor, eh? Con lo que habíamos visto la vez pasada tiene como está limitante de que el espacio la
Así que yo creo Cuando cuando llegó al cuello de botella ese espacio latente que está en la mitad es discreto, Qué significa eso que estoy reconstruyendo patrones base ahí, pero no, pero existen huecos eso sería como la descripción que se podría hacer existe en huecos en esos en esos patrones en los cuales Supongo que una marca ahí ciertas diferencias entre una marca y otra y en medio de ellas hay un espacio quizás matemáticamente no lo puedo describir tampoco tan bien quizás con el libro que les comentaba que lo vimos la vez pasada esa explicación funcione mejor que ya vamos a llegar a esta parte de este libro por el que quiero que veamos es este de acá porque acá lo veíamos un poco la sesión anterior Acá está imagen creo que ayuda bastante bien a entender que es
En el espacio latente, pues va a haber un punto que representa esa reconstrucción que yo estoy haciendo, pero es solamente un punto en ese espacio latente y eso me va a empezar a generar huecos Y eso impide que en algunos casos generalice lo suficientemente bien y si en algunos carros, por ejemplo, llegó a tener un poco menos de información Se podrían empezar a generar estos huecos que generan ruido y no me generan de esa generalización tan buena que yo quiero que tenga para que funcione mucho mejor el auto en correr al menos para esta tarea de detección de anomalías, podemos hacer que tenga una representación distribuida que sería como aparece aquí a la derecha, que ya no es un punto discreto en ese espacio latente, sino ya representa una distribución es decir, puedo extender un poco ese esa representación de esa instancia no sé si se entiende Lo que trato de decir ponle tú es como una bombilla.
El láser el láser es es un punto discreto tiras un láser lo tienes solamente en un punto la bombilla, pues alcanza a extender esa luz y puedes un poco generalizar cerca de donde que hace foco de la luz y pues encontrar esos esas relaciones que hay en medio de ese espacio latente para que no se pierda como esta información, sino que pueda combinar se pueda tener una mejor representación de la realidad del evento, eso es más o menos lo que lo que queremos lograr Cómo se logra eso, eh? Hay una transformación o una derivada de El auto incorrect que se llama Spears autor penal. Aquí quiero hacer una diferencia porque hay una cosa que se llama esfuerzo auto Encore qué es esto que aparece aquí en pantalla, que es que en vez de hacer un cuello de botella, lo que haces es ampliar, que eso se veía en el video de de Carlos de dotes, que es lo que decía era que
Hace la explicabilidad de las redes globales, pues se hacía un Spears que es agrego más más neuronas en el centro para poder distinguir cada uno de los conceptos Esa es la arquitectura, vale Pero hay otra cosa que se llama Spears autointer penalti, que es diferente a eso esto solamente para que se entienda el nombre es muy parecido, pero son diferentes. Esto es la arquitectura sperss, pero lo que nosotros queremos hacer es penalización que genera es persa, es como creo que traduce esparcimiento y ya vamos a ver más o menos A dónde va la ve eso se hace a través de un de métodos que se llaman de regularización, eso ya lo hemos visto también en sesiones anteriores, eh? Regularización también se utiliza para que no tengas overfield, que es evito que los parámetros del modelo se exploten es decir, sean sean muy variables Y de esa manera puedo hacer una generalización?
Entonces acá se veía un poco Cómo funcionaba. Él va a decir varias Este era una gráfica que nos explica también, cómo funciona es la línea verde. Es la que funciona bien. Si tengo Warfare es la línea roja que se va a empezar a pintar y es que ya lo estoy haciendo un poco locochón, de arriba abajo lo que quiero es que funcione lo suficientemente para que no old Aquí vamos a aplicar una cosa parecida, eh? Para que no suceda eso cómo hacíamos la regularización era a partir de unos métodos que se llamaba l1 y L2 que se llama lasso y Rich eh, que básicamente Cuando haces el el error de reconstrucción, es decir, cálculo la diferencia entre el generado y el original le agregó un factor adicional de penalización que depende del peso de la red neuronal de los pesos de la red neuronal, eso qué fuerza que el modelo encuentre el mejor la mejor configuración es de los pesos.
Pero que a la vez los pesos sean pequeños que no explote que no sea agresivamente grande ese esos valores y que pueda tener al mismo tiempo una reconstrucción lo suficientemente bien Es decir la combinación de esos pesos se van a mantener pequeños, pero a la vez van a funcionar lo suficientemente bien para que me reconstruyan bien la imagen o bien en este caso los datos de los carros, eso es, básicamente lo que lo que estamos agregando acá con estas l1 o L2 o también hay otra otro método que se llama Drop out, que esto sirve para redes neuronales, que es simplemente lo que haces es Apagar y prender algunas redes neuronales y de esta manera fuerzas, a que todas las redes neuronales tengan que aprender lo mismo o tenga que combinarse lo suficientemente bien para aprender el patrón que desean aprender de esta manera. Eso es un concepto que a mí me está Yo también la cabeza.
es como si pudieras distribuir el conocimiento en la red neuronal O sea no es como si algunas neuronas se priorizarán porque son más importantes que otras sino que fuerzas que todas las neuronas funcionen como si por salas que todo el cerebro funciona, o sea, fue muy chévere, cuando lo leí me pareció genial, esto se llama Drop out y es otro método de regularización que funciona en redes neuronales solo en redes neuronales porque por obvias razones, cómo funciona apagó y prende las neuronas es el único algoritmo que tiene ese tipo de de comportamiento, vale Y eso que eso que fuerza acá se mueve un poco al al libro que estábamos viendo esto ya es un poco avanzado si le suben esto aquí ya estamos tocando los límites un poco más avanzadilla a hacer, pero era la idea de llegar hasta las últimas instancias, quizás cuando volvamos a un proyecto nuevo empezamos desde cero, pero acá me parece interesante porque ya estamos tocando temas un poco más profundos en este libro que se llama Deep learning de Ian
Muy buen libro para entender perfectamente Cómo funcionan las cosas. Él habla del concepto de experto en Call y aquí es justamente lo que yo les decía este l hace representación a el error de reconstrucción que depende de FX que es básicamente el encoder GS el el el de cobre. Eso es representante de cobre, matemáticamente hablando el X es el dato original y esta sería la reconstrucción cálculo el error y le agregó un componente adicional, que es el el el el factor de regularización que puede ser un l1, un L2 puede ser un Drop out, algo parecido matemáticamente aquí no va a meter una profundidad porque esto Sí lo leí si están interesados ahí sí les dejo a su disposición del libro, pero hacer esto fuerza que pasemos de esa distribución discreta que teníamos hace un momento que era el punto en el espacio latente fuerza a que se genera una distribución.
Que ya se esparce y ya las redes neuronales, son capaces de generalizar lo suficientemente bien para forzar que haya algo parecido a una distribución En ese espacio latente y tener mejores datos a futuro mejores, eh? Más que datos es reconstrucciones porque ya generaliza y tapa estos huecos que existen en el espacio latente, porque ya no es un punto excreto es una distribución el man. Habla aquí de todo esto incluso utiliza la plaza cosa que yo decía en la universidad esto nunca me va a servir en chimba, Pues sí sirve es muy útil muy útil y aquí explican cómo funciona básicamente, eh, No me va a meter mucho en detalle, porque ya matemáticamente podría ser un poco tedioso, pero que entiendan que a partir de esa regularización forzamos que el espacio latente sea ya distribuido y no discreto ahora para pasar a la última parte de de la reunión de hoy cuando ya hacemos que funcione con
Atribuciones y no con valores discretos que con distribuciones ya estamos hablando un poco de probabilidades Estamos hablando Un poco de no hay huecos sí que básicamente si fueran discretos serían más bien punticos punticos aquí Rayados una distribución ya es una tendencia más generalizada podemos utilizar otro error de reconstrucción que no va a ser el mse porque el MC a mí me va a comparar básicamente, eh? El error el valor que me tira con el valor original y eso es un valor discreto yo lo que puedo hacer ahora es comparar distribuciones hacemos comparación de una distribución y no de un valor discreto y eso a mí me genera un resultado mejor en la generalización y por lo tanto en la detección de la anomalía porque ya sé que el valor que me va a entregar a mí no tiene no es susceptible a estos huecos a está de pronto.
Información de algunos casos o sobre información de algunos casos sino ya eh? Funciona de una forma mejor y utilizamos algo que se llama kiel divergent que es este Kill representa Cómo es el apellido de alguien, eh? Creo si lo llegan a ver con otro nombre OK Google esto es otra vaina matemática que lo que hace es comparar distribuciones este gif lo encontré fue el mejor que encontré para explicarlo creo que funcionó bastante bien estamos haciendo acá comparando dos distribuciones la azul y la verde en este caso de hecho si se dan cuenta puede tener estas diferentes formas puede dar dos picos en el azul pero apenas se se aproximan el error que es la barra que aparece en en en en naranja bajo si se dan cuenta entonces si me voy un poco a la izquierda estoy Ajustando con una, pero la otra todavía tiene mucho ruido por lo que todavía.
Justo lo suficientemente bien. Nosotros qué queremos hacer con esta transformación que estamos haciendo así reca del space autoencoder. Queremos meterle que las distribuciones que tengan cada una de las características que estamos generando sean muy parecidas Y de esa manera, vamos a tener una mejor muy bueno que va a funcionar para detección de anomalías y de ahí tenemos una calidad muy buena de de modelo listo por ahora, qué vamos a ver por ahora está todavía en en papel aquí aparece el código, pero todavía no está terminado de hecho, simplemente es una variación que se le hace en la reconstrucción, eh? Lo vamos a ver en la próxima sesión, pero es simplemente otro modelo evolucionado del autor, no es no es mucho más y acá, eh? Déjenme hago scroll, no sé si tienen dudas hasta ahí chicos.
o bien con el Spencer Laura señora, Sí tengo una duda, o sea, en la estructura de la red neuronal es así porque pues el
Cambio no está aquí en el modelo, como tal está en el momento que tú lo entrenas, que eso sería el cambio real existe aquí en el 30 sería que en vez de yo hacer aquí la comparación con el con el MC con el criterio, lo que yo hago es calcular este error, pero adicionalmente le agregó el error de de de regularización que es el l1 o el L2 o el Drop out, o lo que haga falta para forzar que el modelo se entrene con distribuciones para que sea equivalente a una distribución y aquí al momento de evaluar, eh? Ya no sería el MC sino sería un un cliente que sería el que acabamos de ver acá, Sí de hecho, la estructura podría ser la misma, Yo podría usar Incluso el mismo auto en correr el mismo Core y quizás lo maneje así porque podría generar confusión podría utilizar este exactamente este mismo que cambia al momento, que yo lo entreno utilizo un criterio diferente de error.
Le meto el error normal de reconstrucción, pero adicionalmente le le meto un error de regularización porque matemáticamente y por allá un agente se dio cuenta que si le metes ese error de regularización él va a forzar que el autoencoder Esparza ese conocimiento hablando las entre comillas y genere este tipo de distribuciones en vez de valores descritos tiene sentido Sí muchas gracias, listo, Dale entonces, listo chicos, no sé si hay más preguntas y ya saltaría de pronto aquí a esta parte que sería octubre como para ver ya cómo funciona acá, por ejemplo está el Trail cero. Él dice ese término con este valor Este es el hiper parámetro que que se escogió del learning rate Y pues el mejor que he utilizado, pues es el cero, por qué Pues porque es el único que ejecutaba básicamente si alcanzamos a tener tiempo Aunque aquí, yo creo que sí se demora un poco más, eh? Si llegaste a 300, posiblemente él ya compararía el
Pero que hizo con el Trail uno miraría cuál funciona mejor y automáticamente Pues él te va escogiendo y te va diciendo Ah mira, voy en el Trail 70, pero el que mejor funcionó fue el 50, entonces de momento. El mejor resultado es con el 50. Ah míralo acá, De hecho aquí fallo. Aquí está. Está buenísima, Porque mira acá la razón por la que no llego a 300 fue por el el erling stopping, porque se disparó el Airlines stopping, es decir, falló 15 veces en la reconstrucción y ahí le dijimos, ya estás teniendo O al menos, no estás teniendo un buen performance, paremos el entrenamiento, saltemos al siguiente, eso es lo que le estamos diciendo acá. Entonces por eso no llegó a 300, sino 95 y aquí ahora hace la comparación, te dice el mejor que tengo hasta ahora es el uno el uno tiene este error de reconstrucción y pues obtusas automáticamente eso es como la parte chévere, que no tienes que empezar a Pues a forzar algo, sino ya es un framework, no como
Como el que habíamos visto scrap y que también es un framework, que ya tiene cosas preconstruidas y simplemente lo usas te facilita la vida ya chicos próxima sesión, ya qué haríamos el de Space autor, los siguientes pasos suelen ser más sencillos que es evaluar Este modelo que simplemente es hacer el test ese también es muy fácil hacer el test Si pasas un umbral, pues lo desplegamos y no no lo despegamos el siguiente truco, que sigue acá es ya con el modelo entrenado. Tú le tiras los datos que quieres evaluar, en este caso los carros o en el ejemplo que yo les ponía las propiedades bienes raíces. Él va a generar la reconstrucción, que ya sabemos que la calidad va a ser mejor porque es un Spears autointer, pero yo empiezo a evaluar, error por error de cada una de las características que le hiciste bien en precio que lo hiciste bien en kilometraje, que lo hiciste viendo, no sé qué no sé qué.
Ahí todos deberían de hecho estar bastante cercanos a cero Eso Ese es el caso ideal la mayoría de casos deberían estar en el error de reconstrucción muy cercano a hacer, pero lo que nosotros vamos a querer Buscar es lo hiciste bien en toda en todas el color está bien el modelo está abierto, está bien, pero la falla está en precio Ahí es donde uno quiere decir Este es el carro que estoy buscando porque el error el precio que debería tener no es el precio que tiene y obviamente el error podría ir para arriba o para abajo, eso también lo vamos a ver podría estar sobrevaluado el carro o podría estar por debajo del precio que debería tener y ahí uno cogí dice Este es el carro que tengo que comprar y ya o está la casa que te compra o este es el lo que sea que quiere y por ahí existiendo eso todavía no está ahí Todo bien, todo bien.
Listo listo chicos, entonces, eh También de pronto para la próxima sesión, eh? Julián sabe mucho de racks, no está actuando con con este asistente que se está haciendo, pues para resumir todo lo que se ha hecho de las sesiones, eh? Vamos a hacer una sesión para mostrarles ese avance también que se tiene allí y compartir es la idea, eh? No sé creo que la gente por deadpool, o lo que sea no, no participó tanto, pero aún así la idea es que seamos constantes, es un trabajo de constancia de compartir de crecer de hacer proyectos, Si quieren crear uno adelante. Yo en lo personal lo disfruto mucho porque no estoy imaginando mucho que aprendo lo mucho que me tengo que ver forzado a a prepararme para responder todas las preguntas que puedan existir y eso me ha hecho crecer, increíblemente entonces otra vez la invitación está abierta si quieren.
Un tema o sea, aquí es gratis La verdad, no, no se va a pagar ni nada a lo mucho Les puedo prometer como una promoción en linkedin o algo así, pero seguro que eso sí se los puedo garantizar. Van a aprender seguro cuando lo enseño Entonces ya era lo que tenía para compartirles muchachos Muchas gracias, muchas gracias, chicos. Hasta luego.